name: Manual Upload (Sync to HF)

on:
  repository_dispatch:
    types: [manual-upload]
  workflow_dispatch:
    inputs:
      slug:
        description: 'Slug for folder name (e.g. "enem-2023")'
        required: true
      pdf_url:
        description: "Temporary URL for the PDF"
        required: true
      gabarito_url:
        description: "Temporary URL for the Answer Key"
        required: false
      title:
        description: "Exam Title"
        required: true
      year:
        description: "Exam Year"
        required: true
      institution:
        description: "Institution Name"
        required: true
      phase:
        description: "Phase"
        required: true
      source_url_prova:
        description: "Original Source URL for Proof"
        required: false
      source_url_gabarito:
        description: "Original Source URL for Answer Key"
        required: false

jobs:
  sync:
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
      - name: Checkout Repo
        uses: actions/checkout@v4

      - name: Setup Logger
        run: |
          cat << 'EOF' > logger.py
          import sys
          import time
          import json
          import hashlib
          import hmac
          import os
          import http.client
          import urllib.parse

          # --- CONFIG ---
          PUSHER_APP_ID = os.environ.get("PUSHER_APP_ID")
          PUSHER_KEY = os.environ.get("PUSHER_KEY")
          PUSHER_SECRET = os.environ.get("PUSHER_SECRET")
          PUSHER_CLUSTER = os.environ.get("PUSHER_CLUSTER")
          CHANNEL = os.environ.get("SLUG") # Using slug as channel

          def send_to_pusher(event_name, data_payload):
              if not all([PUSHER_APP_ID, PUSHER_KEY, PUSHER_SECRET, PUSHER_CLUSTER, CHANNEL]):
                  print(f"[Logger] Pusher not configured. Skipping {event_name}")
                  return

              timestamp = str(int(time.time()))
              body_data = json.dumps(data_payload)
              body = json.dumps({
                  "name": event_name,
                  "channels": [CHANNEL],
                  "data": body_data
              })
              
              body_md5 = hashlib.md5(body.encode('utf-8')).hexdigest()
              sign_string = f"POST\n/apps/{PUSHER_APP_ID}/events\nauth_key={PUSHER_KEY}&auth_timestamp={timestamp}&auth_version=1.0&body_md5={body_md5}"
              auth_signature = hmac.new(PUSHER_SECRET.encode('utf-8'), sign_string.encode('utf-8'), hashlib.sha256).hexdigest()
              
              params = urllib.parse.urlencode({
                  'auth_key': PUSHER_KEY,
                  'auth_timestamp': timestamp,
                  'auth_version': '1.0',
                  'body_md5': body_md5,
                  'auth_signature': auth_signature
              })
              
              try:
                  conn = http.client.HTTPSConnection(f"api-{PUSHER_CLUSTER}.pusher.com", timeout=3)
                  headers = {'Content-Type': 'application/json'}
                  conn.request("POST", f"/apps/{PUSHER_APP_ID}/events?{params}", body, headers)
                  resp = conn.getresponse()
                  resp.read()
                  conn.close()
              except Exception as e:
                  print(f"[Logger] Pusher Error: {e}")

          if __name__ == "__main__":
               if len(sys.argv) > 1:
                   msg = sys.argv[1]
                   send_to_pusher("log", {"message": msg})
          EOF

      - name: Start Sync
        env:
          PUSHER_APP_ID: ${{ secrets.PUSHER_APP_ID }}
          PUSHER_KEY: ${{ secrets.PUSHER_KEY }}
          PUSHER_SECRET: ${{ secrets.PUSHER_SECRET }}
          PUSHER_CLUSTER: ${{ secrets.PUSHER_CLUSTER }}
          SLUG: ${{ github.event.client_payload.slug || inputs.slug }}
        run: python3 logger.py "☁️ Request received. Syncing to standard dataset structure..."

      - name: Clone Hugging Face Repo
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          git config --global user.email "bot@maia.api"
          git config --global user.name "Maia Bot"
          git clone --depth 1 https://user:$HF_TOKEN@huggingface.co/datasets/toquereflexo/maia-deep-search hf_repo

      - name: Download Files & Structure (Smart Rename)
        id: download
        env:
          SLUG: ${{ github.event.client_payload.slug || inputs.slug }}
          PDF_URL: ${{ github.event.client_payload.pdf_url || inputs.pdf_url }}
          GAB_URL: ${{ github.event.client_payload.gabarito_url || inputs.gabarito_url }}
          # Use provided filename or fallback, but allow renaming
          PDF_FILENAME: ${{ github.event.client_payload.metadata.pdf_filename || 'prova.pdf' }}
          GAB_FILENAME: ${{ github.event.client_payload.metadata.gabarito_filename || 'gabarito.pdf' }}
        run: |
          cd hf_repo
          mkdir -p "output/$SLUG/files"

          # Function to check, rename if needed, and download
          # Exports variable: final_pdf_name, final_gab_name
          download_smart() {
            local url="$1"
            local fname="$2"
            local type="$3" # "pdf" or "gab"
            
            local final_name="$fname"
            
            if [ -n "$url" ] && [ "$url" != "null" ]; then
                # SMART RENAMING LOOP
                # If file exists, try file-1.pdf, file-2.pdf, etc.
                local base="${fname%.*}"
                local ext="${fname##*.}"
                local counter=1
                
                while [ -f "output/$SLUG/files/$final_name" ]; do
                    # Collision detected!
                    # Check if it's visually identical? (Optional optimization, but user asked to keep it if different)
                    # For now, simplistic approach: Collision = Rename. 
                    # We assume visual hash check happened BEFORE in frontend/worker if we wanted to avoid upload.
                    
                    counter=$((counter + 1))
                    final_name="${base}-${counter}.${ext}"
                done
                
                if [ "$fname" != "$final_name" ]; then
                     echo "::notice::Collision resolved: Renaming '$fname' -> '$final_name'"
                fi

                echo "Downloading $final_name from $url..."
                
                if ! curl -sL --fail -o "output/$SLUG/files/$final_name" "$url"; then
                     echo "::error::Download failed (HTTP Error) for $url"
                     exit 1
                fi
                
                # MIME CHECK
                mime_type=$(file --mime-type -b "output/$SLUG/files/$final_name")
                if [[ "$mime_type" != "application/pdf" ]]; then
                    echo "::error::Invalid file type! Expected pdf, got $mime_type"
                    rm "output/$SLUG/files/$final_name"
                    exit 1
                fi
            else
                echo "Skipping $type (No URL provided)."
                final_name="" 
            fi
            
            # Export to GitHub Output
            echo "final_${type}_name=$final_name" >> $GITHUB_OUTPUT
            echo "::notice::Final $type Name: $final_name"
          }

          download_smart "$PDF_URL" "$PDF_FILENAME" "pdf"
          download_smart "$GAB_URL" "$GAB_FILENAME" "gab"

          ls -R "output/$SLUG"

      - name: Update Manifest (Merge Strategy)
        env:
          # Access nested metadata or fallbacks
          SLUG: ${{ github.event.client_payload.slug || inputs.slug }}
          TITLE: ${{ github.event.client_payload.title || inputs.title }}
          YEAR: ${{ github.event.client_payload.metadata.year || inputs.year }}
          INSTITUTION: ${{ github.event.client_payload.metadata.institution || inputs.institution }}
          PHASE: ${{ github.event.client_payload.metadata.phase || inputs.phase }}
          # CRITICAL: Use the FINAL RENAMED FILENAMES from the download step
          PDF_FILENAME: ${{ steps.download.outputs.final_pdf_name }}
          GAB_FILENAME: ${{ steps.download.outputs.final_gab_name }}
          VISUAL_HASH: ${{ github.event.client_payload.metadata.visual_hash }}
          VISUAL_HASH_GABARITO: ${{ github.event.client_payload.metadata.visual_hash_gabarito }}

          # Only add entries for what we actually downloaded (if filename is not empty)
          HAS_PDF: ${{ steps.download.outputs.final_pdf_name != '' }}
          HAS_GAB: ${{ steps.download.outputs.final_gab_name != '' }}
        run: |
          cd hf_repo/output/$SLUG

          cat << 'EOF' > update_manifest.py
          import json
          import os
          import sys

          slug = os.environ.get("SLUG")
          title = os.environ.get("TITLE")
          year = os.environ.get("YEAR")
          inst = os.environ.get("INSTITUTION")
          phase = os.environ.get("PHASE")
          pdf_fname = os.environ.get("PDF_FILENAME")
          gab_fname = os.environ.get("GAB_FILENAME")
          v_hash = os.environ.get("VISUAL_HASH")
          v_hash_gab = os.environ.get("VISUAL_HASH_GABARITO")

          has_pdf = os.environ.get("HAS_PDF") == 'true'
          has_gab = os.environ.get("HAS_GAB") == 'true'

          manifest_path = "manifest.json"
          items = []

          # 1. Load Existing (Fail Safe)
          if os.path.exists(manifest_path):
              try:
                  with open(manifest_path, 'r', encoding='utf-8') as f:
                      content = f.read().strip()
                      if content:
                          items = json.loads(content)
                          if isinstance(items, dict) and 'results' in items:
                              items = items['results']
              except Exception as e:
                  print(f"::error::Failed to read existing manifest: {e}")
                  sys.exit(1)

          # 2. Build Dictionary for Upsert (Key = filename)
          items_map = {item.get('filename'): item for item in items if item.get('filename')}

          # 3. Prepare Payloads (Using FINAL filenames)
          if has_pdf and pdf_fname:
               items_map[pdf_fname] = {
                  "nome": title,
                  "tipo": "Prova",
                  "ano": year,
                  "instituicao": inst,
                  "fase": phase,
                  "link_origem": "manual-upload",
                  "status": "downloaded",
                  "filename": pdf_fname,
                  "path": f"output/{slug}/files/{pdf_fname}",
                  "visual_hash": v_hash
              }

          if has_gab and gab_fname:
               items_map[gab_fname] = {
                  "nome": f"{title} (Gabarito)",
                  "tipo": "Gabarito",
                  "ano": year,
                  "instituicao": inst,
                  "fase": phase,
                  "link_origem": "manual-upload",
                  "status": "downloaded",
                  "filename": gab_fname,
                  "path": f"output/{slug}/files/{gab_fname}",
                  "visual_hash": v_hash_gab
              }

          # 4. Reconstruct List
          final_items = list(items_map.values())

          # 5. Write Back
          with open(manifest_path, "w", encoding="utf-8") as f:
              json.dump(final_items, f, indent=2, ensure_ascii=False)
              
          print(f"Manifest updated. Total items: {len(final_items)}")
          EOF

          python3 update_manifest.py
          cat manifest.json

      - name: Compute Visual Hashes
        env:
          SLUG: ${{ github.event.client_payload.slug || inputs.slug }}
        run: |
          (
            echo "Installing dependencies for hashing..."
            sudo apt-get update && sudo apt-get install -y build-essential libcairo2-dev libpango1.0-dev
            npm install pdfjs-dist@3.11.174 canvas@2.11.2
            
            echo "Computing visual hashes..."
            # Check if directory exists first
            if [ -d "hf_repo/output/$SLUG" ]; then
                node scripts/compute-hash.js "hf_repo/output/$SLUG"
            else
                echo "::error::Directory hf_repo/output/$SLUG not found!"
                exit 1
            fi
          ) 2>&1 | python3 logger.py

      - name: Generate Thumbnails
        env:
          SLUG: ${{ github.event.client_payload.slug || inputs.slug }}
        run: |
          cat << 'EOF' > generate_thumbnails.py
          import os
          import json
          from pdf2image import convert_from_path
          from pathlib import Path

          slug = os.environ.get("SLUG")
          if not slug:
              print("[Thumb] SLUG env var missing.")
              exit(1)

          base_dir = Path(f"hf_repo/output/{slug}")
          thumbs_dir = base_dir / "thumbnails"
          thumbs_dir.mkdir(exist_ok=True)
          manifest_path = base_dir / "manifest.json"

          if manifest_path.exists():
              try:
                  with open(manifest_path, 'r', encoding='utf-8') as f:
                      data = json.load(f)
                  
                  items = data if isinstance(data, list) else data.get('results', [])

                  updated_count = 0
                  for item in items:
                      fname = item.get('filename')
                      if fname and fname.lower().endswith('.pdf'):
                          pdf_path = base_dir / "files" / fname
                          
                          if pdf_path.exists():
                              try:
                                  # Convert first page
                                  images = convert_from_path(str(pdf_path), first_page=1, last_page=1, dpi=150)
                                  if images:
                                      thumb_name = f"{Path(fname).stem}.jpg"
                                      thumb_path = thumbs_dir / thumb_name
                                      images[0].save(thumb_path, 'JPEG', quality=80)
                                      
                                      item['thumbnail'] = f"output/{slug}/thumbnails/{thumb_name}"
                                      updated_count += 1
                                      print(f"[Thumb] Generated: {thumb_name}")
                              except Exception as e:
                                  print(f"[Thumb] Failed for {fname}: {e}")
                  
                  if updated_count > 0:
                      with open(manifest_path, 'w', encoding='utf-8') as f:
                          json.dump(items, f, indent=2, ensure_ascii=False)
                      print(f"[Thumb] Success. Generated {updated_count} thumbnails.")
                  else:
                      print("[Thumb] No thumbnails generated.")

              except Exception as e:
                  print(f"[Thumb] Critical Error: {e}")
          else:
              print(f"[Thumb] Manifest not found at {manifest_path}")
          EOF

          (
            echo "Installing dependencies..."
            sudo apt-get update && sudo apt-get install -y poppler-utils
            pip install pdf2image

            echo "Generating thumbnails..."
            python3 generate_thumbnails.py
          )

      - name: Generate Index (Markdown)
        env:
          SLUG: ${{ github.event.client_payload.slug || inputs.slug }}
          TITLE: ${{ github.event.client_payload.title || inputs.title }}
          PDF_FILENAME: ${{ steps.download.outputs.final_pdf_name }}
          GAB_FILENAME: ${{ steps.download.outputs.final_gab_name }}
        run: |
          cd hf_repo/output/$SLUG
          echo "# $TITLE" > index.md
          echo "" >> index.md

          # Fix: Use dynamic FINAL filenames
          if [ -n "$PDF_FILENAME" ] && [ -f "files/$PDF_FILENAME" ]; then
              echo "- [Prova](https://huggingface.co/datasets/toquereflexo/maia-deep-search/resolve/main/output/$SLUG/files/$PDF_FILENAME)" >> index.md
          fi

          if [ -n "$GAB_FILENAME" ] && [ -f "files/$GAB_FILENAME" ]; then
             echo "- [Gabarito](https://huggingface.co/datasets/toquereflexo/maia-deep-search/resolve/main/output/$SLUG/files/$GAB_FILENAME)" >> index.md
          fi

      - name: Push to Hugging Face
        env:
          SLUG: ${{ github.event.client_payload.slug || inputs.slug }}
          PUSHER_APP_ID: ${{ secrets.PUSHER_APP_ID }}
          PUSHER_KEY: ${{ secrets.PUSHER_KEY }}
          PUSHER_SECRET: ${{ secrets.PUSHER_SECRET }}
          PUSHER_CLUSTER: ${{ secrets.PUSHER_CLUSTER }}
        run: |
          cd hf_repo

          # LFS Optimization for robustness
          git config --global http.postBuffer 524288000
          git config --global http.maxRequestBuffer 104857600
          git config --global lfs.activitytimeout 300
          git config --global lfs.dialtimeout 300

          git lfs install
          git lfs track "*.pdf" "*.jpg"
          git add .
          git commit -m "Manual upload (Rich Metadata) for $SLUG" || echo "Nothing to commit"
          git push

          # Notify Frontend
          python3 ../logger.py "✅ Cloud sync complete! Files and Thumbnails are live."
