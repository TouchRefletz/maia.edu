name: Manual Upload (Sync to HF)

on:
  repository_dispatch:
    types: [manual-upload]
  workflow_dispatch:
    inputs:
      slug:
        description: 'Slug for folder name (e.g. "enem-2023")'
        required: true
      pdf_url:
        description: "Ignored (Legacy)"
        required: false
      gabarito_url:
        description: "Ignored (Legacy)"
        required: false
      title:
        description: "Exam Title"
        required: true
      year:
        description: "Exam Year"
        required: true
      institution:
        description: "Institution Name"
        required: true
      phase:
        description: "Phase"
        required: true
      source_url_prova:
        description: "Original Source URL for Proof"
        required: false
      source_url_gabarito:
        description: "Original Source URL for Answer Key"
        required: false

jobs:
  sync:
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
      - name: Checkout Repo
        uses: actions/checkout@v4

      - name: Setup Logger
        run: |
          cat << 'EOF' > logger.py
          import sys
          import time
          import json
          import hashlib
          import hmac
          import os
          import http.client
          import urllib.parse

          # --- CONFIG ---
          PUSHER_APP_ID = os.environ.get("PUSHER_APP_ID")
          PUSHER_KEY = os.environ.get("PUSHER_KEY")
          PUSHER_SECRET = os.environ.get("PUSHER_SECRET")
          PUSHER_CLUSTER = os.environ.get("PUSHER_CLUSTER")
          CHANNEL = os.environ.get("SLUG") # Using slug as channel

          def send_to_pusher(event_name, data_payload):
              if not all([PUSHER_APP_ID, PUSHER_KEY, PUSHER_SECRET, PUSHER_CLUSTER, CHANNEL]):
                  print(f"[Logger] Pusher not configured. Skipping {event_name}")
                  return

              timestamp = str(int(time.time()))
              body_data = json.dumps(data_payload)
              body = json.dumps({
                  "name": event_name,
                  "channels": [CHANNEL],
                  "data": body_data
              })
              
              body_md5 = hashlib.md5(body.encode('utf-8')).hexdigest()
              sign_string = f"POST\n/apps/{PUSHER_APP_ID}/events\nauth_key={PUSHER_KEY}&auth_timestamp={timestamp}&auth_version=1.0&body_md5={body_md5}"
              auth_signature = hmac.new(PUSHER_SECRET.encode('utf-8'), sign_string.encode('utf-8'), hashlib.sha256).hexdigest()
              
              params = urllib.parse.urlencode({
                  'auth_key': PUSHER_KEY,
                  'auth_timestamp': timestamp,
                  'auth_version': '1.0',
                  'body_md5': body_md5,
                  'auth_signature': auth_signature
              })
              
              try:
                  conn = http.client.HTTPSConnection(f"api-{PUSHER_CLUSTER}.pusher.com", timeout=3)
                  headers = {'Content-Type': 'application/json'}
                  conn.request("POST", f"/apps/{PUSHER_APP_ID}/events?{params}", body, headers)
                  resp = conn.getresponse()
                  resp.read()
                  conn.close()
              except Exception as e:
                  print(f"[Logger] Pusher Error: {e}")

          if __name__ == "__main__":
               if len(sys.argv) > 1:
                   msg = sys.argv[1]
                   send_to_pusher("log", {"message": msg})
          EOF

      - name: Start Sync
        env:
          PUSHER_APP_ID: ${{ secrets.PUSHER_APP_ID }}
          PUSHER_KEY: ${{ secrets.PUSHER_KEY }}
          PUSHER_SECRET: ${{ secrets.PUSHER_SECRET }}
          PUSHER_CLUSTER: ${{ secrets.PUSHER_CLUSTER }}
          SLUG: ${{ github.event.client_payload.slug || inputs.slug }}
        run: python3 logger.py "☁️ Request received. Syncing manifest only (Link Mode)..."

      - name: Clone Hugging Face Repo
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          git config --global user.email "bot@maia.api"
          git config --global user.name "Maia Bot"
          git clone --depth 1 https://user:$HF_TOKEN@huggingface.co/datasets/toquereflexo/maia-deep-search hf_repo

      - name: Update Manifest (Link Only Strategy)
        env:
          # Access nested metadata or fallbacks
          SLUG: ${{ github.event.client_payload.slug || inputs.slug }}
          TITLE: ${{ github.event.client_payload.title || inputs.title }}
          YEAR: ${{ github.event.client_payload.metadata.year || inputs.year }}
          INSTITUTION: ${{ github.event.client_payload.metadata.institution || inputs.institution }}
          PHASE: ${{ github.event.client_payload.metadata.phase || inputs.phase }}

          # Display Names
          PDF_DISPLAY_NAME: ${{ github.event.client_payload.metadata.pdf_display_name }}
          GAB_DISPLAY_NAME: ${{ github.event.client_payload.metadata.gabarito_display_name }}

          # Original Source URLs
          PDF_SOURCE: ${{ github.event.client_payload.source_url_prova || inputs.source_url_prova }}
          GAB_SOURCE: ${{ github.event.client_payload.source_url_gabarito || inputs.source_url_gabarito }}

          # Visual Hashes (Passed from frontend)
          VISUAL_HASH: ${{ github.event.client_payload.visual_hash }}
          VISUAL_HASH_GABARITO: ${{ github.event.client_payload.visual_hash_gabarito }}

          # Filenames (Sanitized from Frontend/Worker)
          PDF_FILENAME: ${{ github.event.client_payload.metadata.pdf_filename }}
          GAB_FILENAME: ${{ github.event.client_payload.metadata.gabarito_filename }}

          HAS_PDF: ${{ github.event.client_payload.source_url_prova != '' }}
          HAS_GAB: ${{ github.event.client_payload.source_url_gabarito != '' }}

        run: |
          # Create script in root (OUTSIDE hf_repo to avoid commit)
          cat << 'EOF' > update_manifest.py
          import json
          import os
          import sys
          from pathlib import Path

          slug = os.environ.get("SLUG")
          title = os.environ.get("TITLE")
          year = os.environ.get("YEAR")
          inst = os.environ.get("INSTITUTION")
          phase = os.environ.get("PHASE")

          # PDF DATA
          pdf_fname = os.environ.get("PDF_FILENAME")
          pdf_display = os.environ.get("PDF_DISPLAY_NAME") or title
          pdf_source = os.environ.get("PDF_SOURCE")
          v_hash = os.environ.get("VISUAL_HASH")

          # GAB DATA
          gab_fname = os.environ.get("GAB_FILENAME")
          gab_display = os.environ.get("GAB_DISPLAY_NAME") or f"{title} (Gabarito)"
          gab_source = os.environ.get("GAB_SOURCE")
          v_hash_gab = os.environ.get("VISUAL_HASH_GABARITO")

          has_pdf = os.environ.get("HAS_PDF") == 'true'
          has_gab = os.environ.get("HAS_GAB") == 'true'

          # Ensure the folder exists even if no physical files (Repo structure)
          output_dir = Path(f"hf_repo/output/{slug}")
          output_dir.mkdir(parents=True, exist_ok=True)
          manifest_path = output_dir / "manifest.json"

          items = []

          # 1. Load Existing (Fail Safe)
          if manifest_path.exists():
              try:
                  with open(manifest_path, 'r', encoding='utf-8') as f:
                      content = f.read().strip()
                      if content:
                          items = json.loads(content)
                          if isinstance(items, dict) and 'results' in items:
                              items = items['results']
              except Exception as e:
                  print(f"::error::Failed to read existing manifest: {e}")
                  sys.exit(1)

          # 2. Build Dictionary for Upsert (Key = filename)
          items_map = {item.get('filename'): item for item in items if item.get('filename')}

          # 3. Prepare Payloads
          try:
              year_int = int(year)
          except:
              year_int = year

          # Entry 1: Prova
          if has_pdf and pdf_fname:
               items_map[pdf_fname] = {
                  "nome": pdf_display,
                  "tipo": "prova",
                  "ano": year_int,
                  "instituicao": inst,
                  "fase": phase,
                  "link_origem": pdf_source,
                  "status": "verified",
                  "filename": pdf_fname,
                  "visual_hash": v_hash
              }

          # Entry 2: Gabarito
          if has_gab and gab_fname:
               items_map[gab_fname] = {
                  "nome": gab_display,
                  "tipo": "gabarito",
                  "ano": year_int,
                  "instituicao": inst,
                  "fase": phase,
                  "link_origem": gab_source,
                  "status": "verified",
                  "filename": gab_fname,
                  "visual_hash": v_hash_gab
              }

          # 4. Reconstruct List
          final_items = list(items_map.values())

          # 5. Write Back
          with open(manifest_path, "w", encoding="utf-8") as f:
              json.dump(final_items, f, indent=2, ensure_ascii=False)
              
          print(f"Manifest updated at {manifest_path}. Total items: {len(final_items)}")
          EOF

          python3 update_manifest.py
          cat "hf_repo/output/$SLUG/manifest.json"

      - name: Push to Hugging Face
        env:
          SLUG: ${{ github.event.client_payload.slug || inputs.slug }}
          PUSHER_APP_ID: ${{ secrets.PUSHER_APP_ID }}
          PUSHER_KEY: ${{ secrets.PUSHER_KEY }}
          PUSHER_SECRET: ${{ secrets.PUSHER_SECRET }}
          PUSHER_CLUSTER: ${{ secrets.PUSHER_CLUSTER }}
        run: |
          cd hf_repo

          # LFS Optimization for robustness
          git config --global http.postBuffer 524288000
          git config --global http.maxRequestBuffer 104857600
          git config --global lfs.activitytimeout 300
          git config --global lfs.dialtimeout 300

          git lfs install
          # No file tracking needed since we are not adding PDFs

          git add .
          python3 ../logger.py "Commitando alterações no manifesto..."
          git commit -m "Manual upload (Manifest Link) for $SLUG" || echo "Nothing to commit"
          git push

          # Notify Frontend
          python3 ../logger.py "✅ Cloud sync complete! Manifest updated."

      - name: Update Semantic Cache
        if: success()
        env:
          WORKER_URL: https://maia-api-worker.willian-campos-ismart.workers.dev
          GH_PAT: ${{ secrets.GH_PAT }}
          # Inputs for manual upload
          SLUG: ${{ github.event.client_payload.slug || inputs.slug }}
          TITLE: ${{ github.event.client_payload.title || inputs.title }}
        run: |
          (
            echo "Updating Semantic Cache at $WORKER_URL..."

            MANIFEST_PATH="hf_repo/output/$SLUG/manifest.json"

            # Generate Clean Query from Title/Slug
            CLEAN_QUERY="${SLUG//-/ }"
            echo "Using Clean Query for Embedding: $CLEAN_QUERY"

            if [ -f "$MANIFEST_PATH" ]; then
              
              # Using jq to create a safe JSON payload
              # Note: For manual upload, original_query is the TITLE provided by user
              PAYLOAD=$(jq -n \
                      --arg query "$CLEAN_QUERY" \
                      --arg original_query "$TITLE" \
                      --arg slug "$SLUG" \
                      --slurpfile manifest "$MANIFEST_PATH" \
                      '{
                        query: $query, 
                        slug: $slug, 
                        metadata: {
                          source: "manual-upload",
                          original_query: $original_query,
                          file_count: ($manifest[0] | length),
                          institution: ($manifest[0][0].instituicao // $manifest[0][0].institution // "unknown"),
                          year: ($manifest[0][0].ano // $manifest[0][0].year // "unknown"),
                          type: "manual-upload-result"
                        }
                      }')
              
              
              # Curl Request
              RESPONSE=$(curl -s -w "\nHTTP_CODE:%{http_code}" -X POST "$WORKER_URL/update-deep-search-cache" \
                   -H "Content-Type: application/json" \
                   -H "Authorization: Bearer $GH_PAT" \
                   -d "$PAYLOAD")
              
              HTTP_CODE=$(echo "$RESPONSE" | tail -n1 | cut -d: -f2)
              BODY=$(echo "$RESPONSE" | sed '$d')

              echo "Worker Response Body: $BODY"
              echo "Worker HTTP Code: $HTTP_CODE"

              if [ "$HTTP_CODE" -ne 200 ]; then
                echo "::warning::Failed to update cache. HTTP $HTTP_CODE (Non-blocking)"
              fi
            else
              echo "Manifest not found at $MANIFEST_PATH. Skipping cache update."
            fi
          ) 2>&1 | python3 logger.py
