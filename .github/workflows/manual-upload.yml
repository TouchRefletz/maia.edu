name: Manual Upload (Sync to HF)

on:
  repository_dispatch:
    types: [manual-upload]
  workflow_dispatch:
    inputs:
      slug:
        description: 'Slug for folder name (e.g. "enem-2023")'
        required: true
      pdf_url:
        description: "Temporary URL for the PDF"
        required: true
      gabarito_url:
        description: "Temporary URL for the Answer Key"
        required: false
      title:
        description: "Exam Title"
        required: true
      year:
        description: "Exam Year"
        required: true
      institution:
        description: "Institution Name"
        required: true
      phase:
        description: "Phase"
        required: true
      source_url_prova:
        description: "Original Source URL for Proof"
        required: false
      source_url_gabarito:
        description: "Original Source URL for Answer Key"
        required: false

jobs:
  sync:
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
      - name: Checkout Repo
        uses: actions/checkout@v4

      - name: Setup Logger
        run: |
          cat << 'EOF' > logger.py
          import sys
          import time
          import json
          import hashlib
          import hmac
          import os
          import http.client
          import urllib.parse

          # --- CONFIG ---
          PUSHER_APP_ID = os.environ.get("PUSHER_APP_ID")
          PUSHER_KEY = os.environ.get("PUSHER_KEY")
          PUSHER_SECRET = os.environ.get("PUSHER_SECRET")
          PUSHER_CLUSTER = os.environ.get("PUSHER_CLUSTER")
          CHANNEL = os.environ.get("SLUG") # Using slug as channel

          def send_to_pusher(event_name, data_payload):
              if not all([PUSHER_APP_ID, PUSHER_KEY, PUSHER_SECRET, PUSHER_CLUSTER, CHANNEL]):
                  print(f"[Logger] Pusher not configured. Skipping {event_name}")
                  return

              timestamp = str(int(time.time()))
              body_data = json.dumps(data_payload)
              body = json.dumps({
                  "name": event_name,
                  "channels": [CHANNEL],
                  "data": body_data
              })
              
              body_md5 = hashlib.md5(body.encode('utf-8')).hexdigest()
              sign_string = f"POST\n/apps/{PUSHER_APP_ID}/events\nauth_key={PUSHER_KEY}&auth_timestamp={timestamp}&auth_version=1.0&body_md5={body_md5}"
              auth_signature = hmac.new(PUSHER_SECRET.encode('utf-8'), sign_string.encode('utf-8'), hashlib.sha256).hexdigest()
              
              params = urllib.parse.urlencode({
                  'auth_key': PUSHER_KEY,
                  'auth_timestamp': timestamp,
                  'auth_version': '1.0',
                  'body_md5': body_md5,
                  'auth_signature': auth_signature
              })
              
              try:
                  conn = http.client.HTTPSConnection(f"api-{PUSHER_CLUSTER}.pusher.com", timeout=3)
                  headers = {'Content-Type': 'application/json'}
                  conn.request("POST", f"/apps/{PUSHER_APP_ID}/events?{params}", body, headers)
                  resp = conn.getresponse()
                  resp.read()
                  conn.close()
              except Exception as e:
                  print(f"[Logger] Pusher Error: {e}")

          if __name__ == "__main__":
               if len(sys.argv) > 1:
                   msg = sys.argv[1]
                   send_to_pusher("log", {"message": msg})
          EOF

      - name: Start Sync
        env:
          PUSHER_APP_ID: ${{ secrets.PUSHER_APP_ID }}
          PUSHER_KEY: ${{ secrets.PUSHER_KEY }}
          PUSHER_SECRET: ${{ secrets.PUSHER_SECRET }}
          PUSHER_CLUSTER: ${{ secrets.PUSHER_CLUSTER }}
          SLUG: ${{ github.event.client_payload.slug || inputs.slug }}
        run: python3 logger.py "☁️ Request received. Syncing to standard dataset structure..."

      - name: Clone Hugging Face Repo
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          git config --global user.email "bot@maia.api"
          git config --global user.name "Maia Bot"
          git clone --depth 1 https://user:$HF_TOKEN@huggingface.co/datasets/toquereflexo/maia-deep-search hf_repo

      - name: Download Files & Structure (Smart Rename)
        id: download
        env:
          SLUG: ${{ github.event.client_payload.slug || inputs.slug }}
          PDF_URL: ${{ github.event.client_payload.pdf_url || inputs.pdf_url }}
          GAB_URL: ${{ github.event.client_payload.gabarito_url || inputs.gabarito_url }}
          # Use provided filename or fallback, but allow renaming
          PDF_FILENAME: ${{ github.event.client_payload.metadata.pdf_filename || 'prova.pdf' }}
          GAB_FILENAME: ${{ github.event.client_payload.metadata.gabarito_filename || 'gabarito.pdf' }}
        run: |
          echo "DEBUG: PDF_FILENAME for Download is: '${PDF_FILENAME}'"
          cd hf_repo
          mkdir -p "output/$SLUG/files"

          # Function to check, rename if needed, and download
          # Exports variable: final_pdf_name, final_gab_name
          download_smart() {
            local url="$1"
            local fname="$2"
            local type="$3" # "pdf" or "gab"
            
            # Notify start of download
            python3 ../logger.py "Baixando arquivo processado: $fname"
            
            local final_name="$fname"
            
            if [ -n "$url" ] && [ "$url" != "null" ]; then
                # SMART RENAMING LOOP
                # If file exists, try file-1.pdf, file-2.pdf, etc.
                local base="${fname%.*}"
                local ext="${fname##*.}"
                local counter=1
                
                while [ -f "output/$SLUG/files/$final_name" ]; do
                    # Collision detected!
                    # Check if it's visually identical? (Optional optimization, but user asked to keep it if different)
                    # For now, simplistic approach: Collision = Rename. 
                    # We assume visual hash check happened BEFORE in frontend/worker if we wanted to avoid upload.
                    
                    counter=$((counter + 1))
                    final_name="${base}-${counter}.${ext}"
                done
                
                if [ "$fname" != "$final_name" ]; then
                     echo "::notice::Collision resolved: Renaming '$fname' -> '$final_name'"
                fi

                echo "Downloading $final_name from $url..."
                
                if ! curl -sL --fail --retry 3 --user-agent "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36" -o "output/$SLUG/files/$final_name" "$url"; then
                     echo "::error::Download failed (HTTP Error) for $url"
                     exit 1
                fi
                
                # MIME CHECK
                mime_type=$(file --mime-type -b "output/$SLUG/files/$final_name")
                if [[ "$mime_type" != "application/pdf" ]]; then
                    echo "::error::Invalid file type! Expected pdf, got $mime_type"
                    rm "output/$SLUG/files/$final_name"
                    exit 1
                fi
            else
                echo "Skipping $type (No URL provided)."
                final_name="" 
            fi
            
            # Export to GitHub Output
            echo "final_${type}_name=$final_name" >> $GITHUB_OUTPUT
            python3 ../logger.py "Download concluído: $final_name"
            echo "::notice::Final $type Name: $final_name"
          }

          download_smart "$PDF_URL" "$PDF_FILENAME" "pdf"
          download_smart "$GAB_URL" "$GAB_FILENAME" "gab"

          ls -R "output/$SLUG"

      - name: Update Manifest (Merge Strategy)
        env:
          # Access nested metadata or fallbacks
          SLUG: ${{ github.event.client_payload.slug || inputs.slug }}
          TITLE: ${{ github.event.client_payload.title || inputs.title }}
          YEAR: ${{ github.event.client_payload.metadata.year || inputs.year }}
          INSTITUTION: ${{ github.event.client_payload.metadata.institution || inputs.institution }}
          PHASE: ${{ github.event.client_payload.metadata.phase || inputs.phase }}
          # CRITICAL: Use the FINAL RENAMED FILENAMES from the download step
          PDF_FILENAME: ${{ steps.download.outputs.final_pdf_name }}
          GAB_FILENAME: ${{ steps.download.outputs.final_gab_name }}
          # NEW: Display names from AI
          PDF_DISPLAY_NAME: ${{ github.event.client_payload.metadata.pdf_display_name }}
          GAB_DISPLAY_NAME: ${{ github.event.client_payload.metadata.gabarito_display_name }}

          VISUAL_HASH: ${{ github.event.client_payload.metadata.visual_hash }}
          VISUAL_HASH_GABARITO: ${{ github.event.client_payload.metadata.visual_hash_gabarito }}

          # Only add entries for what we actually downloaded (if filename is not empty)
          HAS_PDF: ${{ steps.download.outputs.final_pdf_name != '' }}
          HAS_GAB: ${{ steps.download.outputs.final_gab_name != '' }}
        run: |
          # Create script in root (OUTSIDE hf_repo to avoid commit)
          cat << 'EOF' > update_manifest.py
          import json
          import os
          import sys
          from pathlib import Path

          slug = os.environ.get("SLUG")
          title = os.environ.get("TITLE")
          year = os.environ.get("YEAR")
          inst = os.environ.get("INSTITUTION")
          phase = os.environ.get("PHASE")
          phase = os.environ.get("PHASE")
          pdf_fname = os.environ.get("PDF_FILENAME")
          gab_fname = os.environ.get("GAB_FILENAME")
          pdf_display = os.environ.get("PDF_DISPLAY_NAME") or title
          gab_display = os.environ.get("GAB_DISPLAY_NAME") or f"{title} (Gabarito)"
          v_hash = os.environ.get("VISUAL_HASH")
          v_hash_gab = os.environ.get("VISUAL_HASH_GABARITO")

          has_pdf = os.environ.get("HAS_PDF") == 'true'
          has_gab = os.environ.get("HAS_GAB") == 'true'

          # Target manifest inside the repo structure
          manifest_path = Path(f"hf_repo/output/{slug}/manifest.json")
          items = []

          # 1. Load Existing (Fail Safe)
          if manifest_path.exists():
              try:
                  with open(manifest_path, 'r', encoding='utf-8') as f:
                      content = f.read().strip()
                      if content:
                          items = json.loads(content)
                          if isinstance(items, dict) and 'results' in items:
                              items = items['results']
              except Exception as e:
                  print(f"::error::Failed to read existing manifest: {e}")
                  sys.exit(1)

          # 2. Build Dictionary for Upsert (Key = filename)
          items_map = {item.get('filename'): item for item in items if item.get('filename')}

          # 3. Prepare Payloads (Using FINAL filenames)
          try:
              year_int = int(year)
          except:
              year_int = year

          if has_pdf and pdf_fname:
               items_map[pdf_fname] = {
                  "nome": pdf_display,
                  "tipo": "Prova",
                  "ano": year_int,
                  "instituicao": inst,
                  "fase": phase,
                  "link_origem": "manual-upload",
                  "status": "downloaded",
                  "filename": pdf_fname,
                  "visual_hash": v_hash
              }

          if has_gab and gab_fname:
               items_map[gab_fname] = {
                  "nome": gab_display,
                  "tipo": "Gabarito",
                  "ano": year_int,
                  "instituicao": inst,
                  "fase": phase,
                  "link_origem": "manual-upload",
                  "status": "downloaded",
                  "filename": gab_fname,
                  "visual_hash": v_hash_gab
              }

          # 4. Reconstruct List
          final_items = list(items_map.values())

          # 5. Write Back
          with open(manifest_path, "w", encoding="utf-8") as f:
              json.dump(final_items, f, indent=2, ensure_ascii=False)
              
          print(f"Manifest updated at {manifest_path}. Total items: {len(final_items)}")
          EOF

          python3 update_manifest.py
          cat "hf_repo/output/$SLUG/manifest.json"

      - name: Compute Visual Hashes
        env:
          SLUG: ${{ github.event.client_payload.slug || inputs.slug }}
          PDF_FILENAME: ${{ steps.download.outputs.final_pdf_name }}
          GAB_FILENAME: ${{ steps.download.outputs.final_gab_name }}
        uses: ./.github/actions/compute-hash
        with:
          path: "hf_repo/output/${{ env.SLUG }}"

      - name: Generate Thumbnails
        env:
          SLUG: ${{ github.event.client_payload.slug || inputs.slug }}
        run: |
          cat << 'EOF' > generate_thumbnails.py
          import os
          import json
          from pdf2image import convert_from_path
          from pathlib import Path

          slug = os.environ.get("SLUG")
          if not slug:
              print("[Thumb] SLUG env var missing.")
              exit(1)

          base_dir = Path(f"hf_repo/output/{slug}")
          thumbs_dir = base_dir / "thumbnails"
          thumbs_dir.mkdir(exist_ok=True)
          manifest_path = base_dir / "manifest.json"

          if manifest_path.exists():
              try:
                  with open(manifest_path, 'r', encoding='utf-8') as f:
                      data = json.load(f)
                  
                  items = data if isinstance(data, list) else data.get('results', [])

                  updated_count = 0
                  for item in items:
                      fname = item.get('filename')
                      # Skip if thumbnail already exists to preserve legacy/manual edits
                      if 'thumbnail' in item:
                          continue

                      if fname and fname.lower().endswith('.pdf'):
                          pdf_path = base_dir / "files" / fname
                          
                          if pdf_path.exists():
                              try:
                                  # Convert first page
                                  images = convert_from_path(str(pdf_path), first_page=1, last_page=1, dpi=150)
                                  if images:
                                      thumb_name = f"{Path(fname).stem}.jpg"
                                      thumb_path = thumbs_dir / thumb_name
                                      images[0].save(thumb_path, 'JPEG', quality=80)
                                      
                                      # Use relative path (standard)
                                      item['thumbnail'] = f"thumbnails/{thumb_name}"
                                      updated_count += 1
                                      print(f"[Thumb] Generated: {thumb_name}")
                              except Exception as e:
                                  print(f"[Thumb] Failed for {fname}: {e}")
                  
                  if updated_count > 0:
                      with open(manifest_path, 'w', encoding='utf-8') as f:
                          json.dump(items, f, indent=2, ensure_ascii=False)
                      print(f"[Thumb] Success. Generated {updated_count} thumbnails.")
                  else:
                      print("[Thumb] No new thumbnails needed.")

              except Exception as e:
                  print(f"[Thumb] Critical Error: {e}")
          else:
              print(f"[Thumb] Manifest not found at {manifest_path}")
          EOF

          (
            echo "Installing dependencies..."
            sudo apt-get update && sudo apt-get install -y poppler-utils
            pip install pdf2image

            echo "Generating thumbnails..."
            python3 generate_thumbnails.py
          )

      - name: Push to Hugging Face
        env:
          SLUG: ${{ github.event.client_payload.slug || inputs.slug }}
          PUSHER_APP_ID: ${{ secrets.PUSHER_APP_ID }}
          PUSHER_KEY: ${{ secrets.PUSHER_KEY }}
          PUSHER_SECRET: ${{ secrets.PUSHER_SECRET }}
          PUSHER_CLUSTER: ${{ secrets.PUSHER_CLUSTER }}
        run: |
          cd hf_repo

          # LFS Optimization for robustness
          git config --global http.postBuffer 524288000
          git config --global http.maxRequestBuffer 104857600
          git config --global lfs.activitytimeout 300
          git config --global lfs.dialtimeout 300

          git lfs install
          git lfs track "*.pdf" "*.jpg"
          git add .
          python3 ../logger.py "Commitando alterações no dataset..."
          git commit -m "Manual upload (Rich Metadata) for $SLUG" || echo "Nothing to commit"
          git push

          # Notify Frontend
          python3 ../logger.py "✅ Cloud sync complete! Files and Thumbnails are live."

      - name: Update Semantic Cache
        if: success()
        env:
          WORKER_URL: https://maia-api-worker.willian-campos-ismart.workers.dev
          GH_PAT: ${{ secrets.GH_PAT }}
          # Inputs for manual upload
          SLUG: ${{ github.event.client_payload.slug || inputs.slug }}
          TITLE: ${{ github.event.client_payload.title || inputs.title }}
        run: |
          (
            echo "Updating Semantic Cache at $WORKER_URL..."

            MANIFEST_PATH="hf_repo/output/$SLUG/manifest.json"

            # Generate Clean Query from Title/Slug
            CLEAN_QUERY="${SLUG//-/ }"
            echo "Using Clean Query for Embedding: $CLEAN_QUERY"

            if [ -f "$MANIFEST_PATH" ]; then
              
              # Using jq to create a safe JSON payload
              # Note: For manual upload, original_query is the TITLE provided by user
              PAYLOAD=$(jq -n \
                      --arg query "$CLEAN_QUERY" \
                      --arg original_query "$TITLE" \
                      --arg slug "$SLUG" \
                      --slurpfile manifest "$MANIFEST_PATH" \
                      '{
                        query: $query, 
                        slug: $slug, 
                        metadata: {
                          source: "manual-upload",
                          original_query: $original_query,
                          file_count: ($manifest[0] | length),
                          institution: ($manifest[0][0].instituicao // $manifest[0][0].institution // "unknown"),
                          year: ($manifest[0][0].ano // $manifest[0][0].year // "unknown"),
                          type: "manual-upload-result"
                        }
                      }')
              
              
              # Curl Request
              RESPONSE=$(curl -s -w "\nHTTP_CODE:%{http_code}" -X POST "$WORKER_URL/update-deep-search-cache" \
                   -H "Content-Type: application/json" \
                   -H "Authorization: Bearer $GH_PAT" \
                   -d "$PAYLOAD")
              
              HTTP_CODE=$(echo "$RESPONSE" | tail -n1 | cut -d: -f2)
              BODY=$(echo "$RESPONSE" | sed '$d')

              echo "Worker Response Body: $BODY"
              echo "Worker HTTP Code: $HTTP_CODE"

              if [ "$HTTP_CODE" -ne 200 ]; then
                echo "::warning::Failed to update cache. HTTP $HTTP_CODE (Non-blocking)"
              fi
            else
              echo "Manifest not found at $MANIFEST_PATH. Skipping cache update."
            fi
          ) 2>&1 | python3 logger.py
