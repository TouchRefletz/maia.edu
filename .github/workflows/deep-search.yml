name: Deep Search (OpenHands)

on:
  repository_dispatch:
    types: [deep-search]
  workflow_dispatch:
    inputs:
      query:
        description: 'Search Query (e.g. "ita 2022")'
        required: true
      slug:
        description: 'Slug for folder name (e.g. "ita-2022")'
        required: true
      ntfy_topic:
        description: "ntfy.sh topic for log streaming"
        required: false

jobs:
  search:
    runs-on: ubuntu-latest
    timeout-minutes: 25
    permissions:
      contents: write

    # Global Environment Variables (Available to all steps)
    env:
      LLM_API_KEY: ${{ secrets.LLM_API_KEY }}
      TAVILY_API_KEY: ${{ secrets.TAVILY_API_KEY }}
      PUSHER_APP_ID: ${{ secrets.PUSHER_APP_ID }}
      PUSHER_KEY: ${{ secrets.PUSHER_KEY }}
      PUSHER_SECRET: ${{ secrets.PUSHER_SECRET }}
      PUSHER_CLUSTER: ${{ secrets.PUSHER_CLUSTER }}
      NTFY_TOPIC: ${{ github.event.client_payload.ntfy_topic || inputs.ntfy_topic }}
      QUERY: ${{ github.event.client_payload.query || inputs.query }}
      SLUG: ${{ github.event.client_payload.slug || inputs.slug }}

    steps:
      - name: Checkout Repo
        uses: actions/checkout@v4

      - name: Setup Logger
        run: |
          cat << 'EOF' > logger.py
          import sys
          import time
          import json
          import hashlib
          import hmac
          import os
          import threading
          import http.client
          import urllib.parse
          import subprocess
          import re
          from collections import deque

          # Config
          PUSHER_APP_ID = os.environ.get("PUSHER_APP_ID")
          PUSHER_KEY = os.environ.get("PUSHER_KEY")
          PUSHER_SECRET = os.environ.get("PUSHER_SECRET")
          PUSHER_CLUSTER = os.environ.get("PUSHER_CLUSTER")
          CHANNEL = os.environ.get("SLUG")

          if not all([PUSHER_APP_ID, PUSHER_KEY, PUSHER_SECRET, PUSHER_CLUSTER, CHANNEL]):
              print("Missing Pusher secrets. Logging to stdout only.")
              # Fallback: just print everything
              for line in sys.stdin:
                  sys.stdout.write(line)
                  sys.stdout.flush()
              sys.exit(0)

          # Buffer
          log_buffer = deque()
          buffer_lock = threading.Lock()
          running = True

          def send_to_pusher(event_name, data_payload):
              timestamp = str(int(time.time()))
              body_data = json.dumps(data_payload)
              body = json.dumps({
                  "name": event_name,
                  "channels": [CHANNEL],
                  "data": body_data
              })
              
              body_md5 = hashlib.md5(body.encode('utf-8')).hexdigest()
              
              sign_string = f"POST\n/apps/{PUSHER_APP_ID}/events\nauth_key={PUSHER_KEY}&auth_timestamp={timestamp}&auth_version=1.0&body_md5={body_md5}"
              auth_signature = hmac.new(PUSHER_SECRET.encode('utf-8'), sign_string.encode('utf-8'), hashlib.sha256).hexdigest()
              
              params = urllib.parse.urlencode({
                  'auth_key': PUSHER_KEY,
                  'auth_timestamp': timestamp,
                  'auth_version': '1.0',
                  'body_md5': body_md5,
                  'auth_signature': auth_signature
              })
              
              try:
                  conn = http.client.HTTPSConnection(f"api-{PUSHER_CLUSTER}.pusher.com", timeout=3)
                  headers = {'Content-Type': 'application/json'}
                  conn.request("POST", f"/apps/{PUSHER_APP_ID}/events?{params}", body, headers)
                  resp = conn.getresponse()
                  resp.read() # Consume
                  conn.close()
              except Exception as e:
                  print(f"Pusher Error: {e}")

          def send_batch(lines):
              if not lines:
                  return
              # Join lines reliably
              text_chunk = "".join(lines)
              send_to_pusher("log", {"message": text_chunk})

          def consumer_thread():
              while running or log_buffer:
                  with buffer_lock:
                      if not log_buffer:
                          batch = []
                      else:
                          batch = list(log_buffer)
                          log_buffer.clear()
                  
                  if batch:
                      send_batch(batch)
                  
                  # Sleep/Tick
                  time.sleep(1.0)

          # Start Consumer
          t = threading.Thread(target=consumer_thread)
          t.daemon = True
          t.start()

          # Helper: Parse TASKS.md
          def parse_tasks_md(content):
              tasks = []
              # Expected format: - [ ] Task Name or - [x] Task Name or - [/] Task Name
              # id is implicit index or we can try to find <!-- id: X -->
              lines = content.split('\n')
              for i, line in enumerate(lines):
                  line = line.strip()
                  match = re.match(r'- \[(.| )\] (.*)', line)
                  if match:
                      status_char = match.group(1)
                      title_raw = match.group(2)
                      
                      # Clean comments from title if any (e.g. <!-- id: 0 -->)
                      title = re.sub(r'<!--.*-->', '', title_raw).strip()
                      
                      status = "todo"
                      if status_char.lower() == 'x':
                          status = "completed"
                      elif status_char == '/':
                          status = "in_progress"
                      
                      tasks.append({"id": i, "title": title, "status": status})
              return tasks

          # Main Producer Loop
          # We don't pre-compile a complex regex, we do a simple check first

          for line in sys.stdin:
              # Always print to stdout for GitHub logs
              sys.stdout.write(line)
              sys.stdout.flush()
              
              # Intercept Tasks
              # Look for the characteristic string from the user's example
              # "TaskTrackingObservation(content='... Stored in session directory: .../TASKS.md', ...)"
              if "TaskTrackingObservation" in line and "TASKS.md" in line:
                  print(f"[Logger] DEBUG: Potential Task Update found: {line.strip()}")
                  
                  # Try to extract path with a more relaxed regex
                  # Matches "session directory: " followed by path not containing spaces or quotes
                  match = re.search(r"session directory:\s*([^\s']+/TASKS\.md)", line)
                  
                  if match:
                      tasks_path = match.group(1).strip()
                      print(f"[Logger] Detected Task Path: {tasks_path}")
                      try:
                          # 1. Copy from container
                          subprocess.run(
                              ["docker", "cp", f"openhands-app:/workspace/{tasks_path}", "tasks_temp.md"],
                              check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL
                          )
                          
                          # 2. Read and Parse
                          with open("tasks_temp.md", "r") as f:
                              content = f.read()
                          
                          parsed_tasks = parse_tasks_md(content)
                          
                          # 3. Broadcast
                          if parsed_tasks:
                              print(f"[Logger] Broadcasting {len(parsed_tasks)} tasks")
                              send_to_pusher("task_update", parsed_tasks)
                              
                      except Exception as e:
                          print(f"[Logger] Failed to intercept tasks: {e}")
                  else:
                      print("[Logger] DEBUG: Could not extract TASKS.md path from line.")

              with buffer_lock:
                  log_buffer.append(line)

          # Shutdown
          running = False
          t.join(timeout=5)
          EOF

      - name: Free Disk Space
        run: |
          (
            echo "Disk space before cleanup:"
            df -h
            sudo rm -rf /usr/local/lib/android
            sudo rm -rf /usr/share/dotnet
            sudo rm -rf /opt/ghc
            sudo rm -rf /usr/local/.ghcup
            echo "Disk space after cleanup:"
            df -h
          ) 2>&1 | python3 logger.py

      - name: Create Output Directory
        run: |
          (
             mkdir -p output/${{ github.event.client_payload.slug || inputs.slug }}
             echo "Created output directory: output/${{ github.event.client_payload.slug || inputs.slug }}"
          ) 2>&1 | python3 logger.py

      - name: Pre-execution Cleanup (HF)
        if: ${{ github.event.client_payload.cleanup == true }}
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          SLUG: ${{ github.event.client_payload.slug || inputs.slug }}
        run: |
          (
            echo "Cleanup requested. Removing existing Hugging Face data for $SLUG..."
            
            # Configure Git
            git config --global user.email "bot@maia.api"
            git config --global user.name "Maia Bot"

            # Clone Repo
            git clone --depth 1 https://user:$HF_TOKEN@huggingface.co/datasets/toquereflexo/maia-deep-search hf_cleanup_repo
            cd hf_cleanup_repo

            # Check and delete
            if [ -d "output/$SLUG" ]; then
                echo "Removing output/$SLUG..."
                git rm -rf "output/$SLUG"
                git commit -m "Cleanup (Retry) for $SLUG"
                git push
                echo "Cleanup committed."
            else
                echo "No existing data found at output/$SLUG."
            fi
            cd ..
            rm -rf hf_cleanup_repo
          ) 2>&1 | python3 logger.py

      - name: Pre-execution Cleanup (Pinecone)
        if: ${{ github.event.client_payload.cleanup == true }}
        env:
          WORKER_URL: https://maia-api-worker.willian-campos-ismart.workers.dev
          GH_PAT: ${{ secrets.GH_PAT }}
          SLUG: ${{ github.event.client_payload.slug || inputs.slug }}
        run: |
          (
             echo "Requesting Pinecone deletion for $SLUG..."
             curl -X POST "$WORKER_URL/delete-pinecone-record" \
                  -H "Content-Type: application/json" \
                  -H "Authorization: Bearer $GH_PAT" \
                  -d "{\"slug\": \"$SLUG\"}" \
             || echo "Warning: Failed to call delete endpoint"
          ) 2>&1 | python3 logger.py

      - name: Announce Job URL
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          RUN_ID: ${{ github.run_id }}
          REPO: ${{ github.repository }}
        run: |
          (
            # Fetch the Jobs for this Run
            JOBS_JSON=$(curl -s -H "Authorization: Bearer $GH_TOKEN" \
                        "https://api.github.com/repos/$REPO/actions/runs/$RUN_ID/jobs")
            
            # Extract the HTML URL of the current job
            JOB_URL=$(echo "$JOBS_JSON" | jq -r '.jobs[0].html_url')
            
            if [ "$JOB_URL" == "null" ] || [ -z "$JOB_URL" ]; then
                # Fallback to Run URL if API fails
                JOB_URL="${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
            fi

            echo "[SYSTEM_INFO] JOB_URL=$JOB_URL"
          ) 2>&1 | python3 logger.py

      - name: Run OpenHands Deep Search
        id: deep_search
        run: |
          set -euo pipefail

          # 1) Heredoc sem interpretação (não executa backticks)
          PROMPT_TEMPLATE=$(cat <<'EOF'
          Você recebeu uma consulta curta (QUERY) do tipo "nome da prova + ano", por exemplo: "ita 2022".

          Objetivo: encontrar e organizar TODOS os links e arquivos (provas e gabaritos) associados a essa QUERY.

          QUERY: "__QUERY__"

          Regras para evitar pesquisa demais (muito importante):
          1) Não use Google como primeira opção (CAPTCHA é comum). Prefira:
            - Sites oficiais da banca/instituição (domínio oficial).
            - Páginas “Provas anteriores / provas e gabaritos / exames anteriores / vestibular / arquivos”.
          2) Faça no máximo 3 buscas no total (usando o mecanismo de busca configurado no OpenHands). Use buscas bem específicas.
          3) Após achar o site oficial, navegue dentro dele e extraia tudo com o mínimo de novas buscas.
          4) Só inclua links que pareçam realmente do exame/ano correspondente; quando estiver ambíguo, checar rapidamente pelo título/URL/nome do PDF.

          O que coletar:
          - Provas (todas as fases/dias/disciplinas/versões que existirem).
          - Gabaritos correspondentes (e “soluções”, se houver).
          - INDISPENSÁVEL: Classifique cada item no manifesto:
            - status: "downloaded" (se baixou com sucesso e validou) OU "reference" (se é apenas um link externo que não foi possível baixar ou não é PDF).
          - Estrutura do JSON: {nome, tipo, ano, instituicao, fase, link_origem, status, filename (só se status=downloaded)}

          Preferência de links:
          - Priorize links diretos de PDF.
          - Se for Google Drive, capture:
            - link "view"
            - e também um link “download direto” quando possível (uc?export=download&id=...).

          Entrega (obrigatório):
          1) Crie a pasta: /workspace/output/__SLUG__/
          2) Baixe todos os PDFs que conseguir para: /workspace/output/__SLUG__/files/.
            CRITICO:
            - USE 'wget' ou 'curl' com verificação de header.
            - VERIFIQUE SE O HEADER 'Content-Type' É 'application/pdf'.
            - SE O ARQUIVO BAIXADO TIVER MENOS DE 1KB, APAGUE-O IMEDIATAMENTE e mude o status para "reference".
            - SE O ARQUIVO CONTIVER HTML (DOCTYPE, <html>), APAGUE-O e mude o status para "reference".
            - NÃO CRIE ARQUIVOS FAKES OU COM CONTEÚDO GERADO. SÓ MANTENHA O QUE FOI BAIXADO DE VERDADE.
          3) Gere /workspace/output/__SLUG__/index.md
          4) Gere /workspace/output/__SLUG__/manifest.json (ESTE ARQUIVO É CRITICO, DEVE SER JSON VÁLIDO. INCLUA ITENS "reference" E "downloaded")
          5) Gere /workspace/output/__SLUG__/DOWNLOAD_URLS.txt
          6) Gere /workspace/output/__SLUG__/slug.txt
             - IMPORTANTE: Escreva neste arquivo APENAS um nome curto e limpo para a pasta final (kebab-case).
             - Exemplo: se a query for 'provas do enem 2025', escreva 'enem-2025'.
             - Exemplo: se for 'unicamp 2024 segunda fase', escreva 'unicamp-2024-2f'.
             - Esse nome será usado para renomear a pasta e salvar no banco de dados.
          7) Gere /workspace/output/__SLUG__.zip
          8) (Opcional) Gere /workspace/output/__SLUG__/index.pdf

          Ao final, responda apenas com caminhos e contagens.
          EOF
          )

          # Replace Placeholders
          PROMPT="${PROMPT_TEMPLATE/__QUERY__/$QUERY}"
          PROMPT="${PROMPT//__SLUG__/$SLUG}"

          # 3) Setup Docker volumes and permissions
          VOL_NAME="openhands-vol-${GITHUB_RUN_ID}"
          STATE_VOL_NAME="openhands-state-${GITHUB_RUN_ID}"
          docker volume create "$VOL_NAME"
          docker volume create "$STATE_VOL_NAME"

          docker run --rm \
            -v "$VOL_NAME:/workspace" \
            -v "$STATE_VOL_NAME:/workspace/.openhands" \
            -e TAVILY_API_KEY="$TAVILY_API_KEY" \
            alpine sh -c '
              set -eux
              mkdir -p /workspace/.openhands
              printf "[core]\nsearch_api_key = \"%s\"\n" "$TAVILY_API_KEY" > /workspace/config.toml
              rm -f /workspace/.openhands/.jwt_secret || true
              chmod -R 777 /workspace
            '
          docker rm -f openhands-app 2>/dev/null || true

          # 4) Run Docker piped to Python
          docker_args=(
            run --pull=always
            -w /workspace
            -v /var/run/docker.sock:/var/run/docker.sock
            -v "$VOL_NAME:/workspace:rw"
            -v "$STATE_VOL_NAME:/workspace/.openhands"
            --add-host host.docker.internal:host-gateway
            -e "SANDBOX_USER_ID=$(id -u)"
            -e "SANDBOX_VOLUMES=$VOL_NAME:/workspace:rw"
            -e "SANDBOX_RUNTIME_CONTAINER_IMAGE=docker.openhands.dev/openhands/runtime:1.0-nikolaik"
            -e "LOG_ALL_EVENTS=true"
            -e "LLM_API_KEY=$LLM_API_KEY"
            -e "LLM_MODEL=gemini/gemini-3-flash-preview"
            -e "TAVILY_API_KEY=$TAVILY_API_KEY"
            -e "PYTHONUNBUFFERED=1"
            -e "PROMPT=$PROMPT"
            -e "FILE_STORE=local"
            -e "FILE_STORE_PATH=/workspace/.openhands"
            --name openhands-app
            docker.openhands.dev/openhands/openhands:1.0
            python -m openhands.core.main -t "$PROMPT"
          )

          # Pipe to python script. Secrets are now in the step env.
          docker "${docker_args[@]}" 2>&1 | python3 logger.py

          EXIT_CODE=${PIPESTATUS[0]}
          echo "Docker run finished with exit code $EXIT_CODE"

          echo "Copying artifacts from container to host..."
          docker cp openhands-app:/workspace/output/${SLUG}/. "output/${SLUG}/" || echo "Failed to copy artifacts or directory empty"

          # --- DYNAMIC RENAMING LOGIC ---
          FINAL_SLUG="$SLUG"
          if [ -f "output/${SLUG}/slug.txt" ]; then
             CANDIDATE_SLUG=$(cat "output/${SLUG}/slug.txt" | tr -cd '[:alnum:]-')
             if [ -n "$CANDIDATE_SLUG" ] && [ "$CANDIDATE_SLUG" != "$SLUG" ]; then
                 echo "Renaming artifact from $SLUG to $CANDIDATE_SLUG..."
                 mv "output/${SLUG}" "output/${CANDIDATE_SLUG}"
                 FINAL_SLUG="$CANDIDATE_SLUG"
             fi
          fi
          echo "final_slug=$FINAL_SLUG" >> $GITHUB_OUTPUT
          # ------------------------------

          docker rm -f openhands-app || true
          docker volume rm "$VOL_NAME" || true
          docker volume rm "$STATE_VOL_NAME" || true

          exit $EXIT_CODE

      - name: Upload Search Results
        uses: actions/upload-artifact@v4
        with:
          name: deep-search-artifact
          path: output/${{ steps.deep_search.outputs.final_slug }}

      - name: Push to Hugging Face Dataset
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          (
            echo "Starting upload to Hugging Face..."
            # Configure Git
            git config --global user.email "bot@maia.api"
            git config --global user.name "Maia Bot"

            # Clone HF Repo (using depth 1 for speed)
            git clone --depth 1 https://user:$HF_TOKEN@huggingface.co/datasets/toquereflexo/maia-deep-search hf_repo

            # Prepare Directories
            slug="${{ steps.deep_search.outputs.final_slug }}"
            mkdir -p hf_repo/output/$slug

            # Copy Artifacts
            cp -r output/$slug/* hf_repo/output/$slug/

            # Commit and Push
            cd hf_repo

            # Setup LFS for binaries
            git lfs install
            git lfs track "*.pdf" "*.zip" "*.rar" "*.doc" "*.docx"
            git add .gitattributes

            git add .
            git commit -m "Add deep search results for $slug" || echo "No changes to commit"
            git push
            echo "Upload to Hugging Face completed."
          ) 2>&1 | python3 logger.py

      - name: Update Semantic Cache
        if: success()
        env:
          WORKER_URL: https://maia-api-worker.willian-campos-ismart.workers.dev
          GH_PAT: ${{ secrets.GH_PAT }}
          QUERY: ${{ github.event.client_payload.query || inputs.query }}
          SLUG: ${{ steps.deep_search.outputs.final_slug }}
        run: |
          (
            echo "Updating Semantic Cache at $WORKER_URL..."

            MANIFEST_PATH="output/$SLUG/manifest.json"

            # Generate Clean Query from Slug (replace dashes with spaces)
            CLEAN_QUERY="${SLUG//-/ }"
            echo "Using Clean Query for Embedding: $CLEAN_QUERY"

            if [ -f "$MANIFEST_PATH" ]; then
              
              # Using jq to create a safe JSON payload
              PAYLOAD=$(jq -n \
                      --arg query "$CLEAN_QUERY" \
                      --arg original_query "$QUERY" \
                      --arg slug "$SLUG" \
                      --slurpfile manifest "$MANIFEST_PATH" \
                      '{
                        query: $query, 
                        slug: $slug, 
                        metadata: {
                          source: "deep-search",
                          original_query: $original_query,
                          file_count: ($manifest[0] | length),
                          institution: ($manifest[0][0].instituicao // $manifest[0][0].institution // "unknown"),
                          year: ($manifest[0][0].ano // $manifest[0][0].year // "unknown")
                        }
                      }')
              
              
              # Curl Request with -f (fail on error) and capturing output
              RESPONSE=$(curl -s -w "\nHTTP_CODE:%{http_code}" -X POST "$WORKER_URL/update-deep-search-cache" \
                   -H "Content-Type: application/json" \
                   -H "Authorization: Bearer $GH_PAT" \
                   -d "$PAYLOAD")
              
              HTTP_CODE=$(echo "$RESPONSE" | tail -n1 | cut -d: -f2)
              BODY=$(echo "$RESPONSE" | sed '$d')

              echo "Worker Response Body: $BODY"
              echo "Worker HTTP Code: $HTTP_CODE"

              if [ "$HTTP_CODE" -ne 200 ]; then
                echo "::error::Failed to update cache. HTTP $HTTP_CODE"
                exit 1
              fi
            else
              echo "Manifest not found at $MANIFEST_PATH. Skipping cache update."
            fi
          ) 2>&1 | python3 logger.py

      - name: Notify Completion
        if: ${{ always() }}
        env:
          PUSHER_APP_ID: ${{ secrets.PUSHER_APP_ID }}
          PUSHER_KEY: ${{ secrets.PUSHER_KEY }}
          PUSHER_SECRET: ${{ secrets.PUSHER_SECRET }}
          PUSHER_CLUSTER: ${{ secrets.PUSHER_CLUSTER }}
          SLUG: ${{ github.event.client_payload.slug || inputs.slug }}
          FINAL_SLUG: ${{ steps.deep_search.outputs.final_slug }}
          JOB_STATUS: ${{ job.status }}
        run: |
          LOG_URL="${GITHUB_SERVER_URL}/${GITHUB_REPOSITORY}/actions/runs/${GITHUB_RUN_ID}"

          if [ "$JOB_STATUS" == "success" ]; then
             MESSAGE="COMPLETED. Logs: $LOG_URL"
             EVENT_NAME="log"
          elif [ "$JOB_STATUS" == "cancelled" ]; then
             MESSAGE="CANCELLED. Job stopped by user. Logs: $LOG_URL"
             EVENT_NAME="log"
          else
             MESSAGE="FAILED. Something went wrong. Logs: $LOG_URL"
             EVENT_NAME="log"
             # Important: The frontend expects "Job failed" or similar to trigger fail state
             # We can send a specific log line that triggers it
          fi

          # Simple Pusher Trigger
          TS=$(date +%s)

          # Include new_slug in data
          DATA_JSON=$(jq -n \
                  --arg msg "$MESSAGE" \
                  --arg final_slug "$FINAL_SLUG" \
                  --arg status "$JOB_STATUS" \
                  '{message: $msg, new_slug: $final_slug, status: $status}')

          # Send to ORIGINAL slug channel so frontend hears it
          BODY=$(jq -n \
                  --arg name "$EVENT_NAME" \
                  --arg channel "$SLUG" \
                  --arg data "$DATA_JSON" \
                  '{name: $name, channels: [$channel], data: $data}')

          BODY_MD5=$(echo -n "$BODY" | md5sum | awk '{print $1}')
          SIGN_STRING="POST\n/apps/$PUSHER_APP_ID/events\nauth_key=$PUSHER_KEY&auth_timestamp=$TS&auth_version=1.0&body_md5=$BODY_MD5"
          AUTH_SIGNATURE=$(echo -n -e "$SIGN_STRING" | openssl dgst -sha256 -hmac "$PUSHER_SECRET" | sed 's/^.* //')

          curl -s -X POST "https://api-$PUSHER_CLUSTER.pusher.com/apps/$PUSHER_APP_ID/events?auth_key=$PUSHER_KEY&auth_timestamp=$TS&auth_version=1.0&body_md5=$BODY_MD5&auth_signature=$AUTH_SIGNATURE" \
                 -H "Content-Type: application/json" \
                 -d "$BODY"
