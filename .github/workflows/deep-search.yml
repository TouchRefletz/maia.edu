name: Deep Search (OpenHands)

on:
  repository_dispatch:
    types: [deep-search]
  workflow_dispatch:
    inputs:
      query:
        description: 'Search Query (e.g. "ita 2022")'
        required: true
      slug:
        description: 'Slug for folder name (e.g. "ita-2022")'
        required: true
      ntfy_topic:
        description: "ntfy.sh topic for log streaming"
        required: false

jobs:
  search:
    runs-on: ubuntu-latest
    timeout-minutes: 25
    permissions:
      contents: write

    # Global Environment Variables (Available to all steps)
    env:
      LLM_API_KEY: ${{ secrets.LLM_API_KEY }}
      TAVILY_API_KEY: ${{ secrets.TAVILY_API_KEY }}
      PUSHER_APP_ID: ${{ secrets.PUSHER_APP_ID }}
      PUSHER_KEY: ${{ secrets.PUSHER_KEY }}
      PUSHER_SECRET: ${{ secrets.PUSHER_SECRET }}
      PUSHER_CLUSTER: ${{ secrets.PUSHER_CLUSTER }}
      NTFY_TOPIC: ${{ github.event.client_payload.ntfy_topic || inputs.ntfy_topic }}
      QUERY: ${{ github.event.client_payload.query || inputs.query }}
      SLUG: ${{ github.event.client_payload.slug || inputs.slug }}

    steps:
      - name: Checkout Repo
        uses: actions/checkout@v4

      - name: Setup Logger
        run: |
          cat << 'EOF' > logger.py
          import sys
          import time
          import json
          import hashlib
          import hmac
          import os
          import threading
          import http.client
          import urllib.parse
          import subprocess
          import re
          from collections import deque

          # --- CONFIG ---
          PUSHER_APP_ID = os.environ.get("PUSHER_APP_ID")
          PUSHER_KEY = os.environ.get("PUSHER_KEY")
          PUSHER_SECRET = os.environ.get("PUSHER_SECRET")
          PUSHER_CLUSTER = os.environ.get("PUSHER_CLUSTER")
          CHANNEL = os.environ.get("SLUG")

          if not all([PUSHER_APP_ID, PUSHER_KEY, PUSHER_SECRET, PUSHER_CLUSTER, CHANNEL]):
              print("[Logger] Missing Pusher secrets. Logging to stdout only.")
              PUSHER_ENABLED = False
          else:
              PUSHER_ENABLED = True

          # Global State
          log_buffer = deque()
          buffer_lock = threading.Lock()
          running = True

          print("[Logger] Script initialized.", flush=True)

          # --- PUSHER HELPER ---
          def send_to_pusher(event_name, data_payload):
              if not PUSHER_ENABLED:
                  return

              timestamp = str(int(time.time()))
              body_data = json.dumps(data_payload)
              body = json.dumps({
                  "name": event_name,
                  "channels": [CHANNEL],
                  "data": body_data
              })
              
              body_md5 = hashlib.md5(body.encode('utf-8')).hexdigest()
              
              sign_string = f"POST\n/apps/{PUSHER_APP_ID}/events\nauth_key={PUSHER_KEY}&auth_timestamp={timestamp}&auth_version=1.0&body_md5={body_md5}"
              auth_signature = hmac.new(PUSHER_SECRET.encode('utf-8'), sign_string.encode('utf-8'), hashlib.sha256).hexdigest()
              
              params = urllib.parse.urlencode({
                  'auth_key': PUSHER_KEY,
                  'auth_timestamp': timestamp,
                  'auth_version': '1.0',
                  'body_md5': body_md5,
                  'auth_signature': auth_signature
              })
              
              try:
                  conn = http.client.HTTPSConnection(f"api-{PUSHER_CLUSTER}.pusher.com", timeout=3)
                  headers = {'Content-Type': 'application/json'}
                  conn.request("POST", f"/apps/{PUSHER_APP_ID}/events?{params}", body, headers)
                  resp = conn.getresponse()
                  resp.read() # Consume response
                  conn.close()
              except Exception as e:
                  print(f"[Logger] Pusher Error: {e}")

          # --- WORKER: LOG BATCHER ---
          def log_worker():
              while running or log_buffer:
                  with buffer_lock:
                      if not log_buffer:
                          batch = []
                      else:
                          batch = list(log_buffer)
                          log_buffer.clear()
                  
                  if batch:
                      # Join lines and broadcast
                      text_chunk = "".join(batch)
                      send_to_pusher("log", {"message": text_chunk})
                  
                  time.sleep(1.0) # Batch window

          # --- WORKER: TASK ACTIVE POLLER ---
          # This thread actively asks Docker for the TASKS.md file
          # It is much more robust than parsing log lines.
          def task_watcher_worker():
              print("[Logger] Task watcher thread started. Polling for TASKS.md...", flush=True)
              last_known_tasks_hash = ""
              container_name = "openhands-app"
              tasks_path = None # Will find dynamically
              
              # Wait slightly for container, but don't block visibility
              time.sleep(2)

              while running:
                  time.sleep(2.0) # Poll every 2s

                  try:
                      # 1. Locate TASKS.md if we haven't yet
                      if not tasks_path:
                          # "find /workspace -name TASKS.md | head -n 1"
                          try:
                              find_cmd = ["docker", "exec", container_name, "sh", "-c", "find /workspace -name TASKS.md 2>/dev/null | head -n 1"]
                              res = subprocess.run(find_cmd, capture_output=True, text=True)
                              if res.returncode == 0 and res.stdout.strip():
                                  tasks_path = res.stdout.strip()
                                  print(f"[Logger] Found active task file at: {tasks_path}", flush=True)
                          except:
                              pass # Container might not be up yet
                      
                      # 2. Read File if we found it
                      if tasks_path:
                          read_cmd = ["docker", "exec", container_name, "cat", tasks_path]
                          res = subprocess.run(read_cmd, capture_output=True, text=True)
                          
                          if res.returncode == 0:
                              content = res.stdout
                              # Debug: Show we got content
                              # print(f"[Logger] Read {len(content)} bytes from TASKS.md", flush=True)

                              # 3. Hash Check
                              current_hash = hashlib.md5(content.encode()).hexdigest()
                              if current_hash != last_known_tasks_hash:
                                  # Debug: Print RAW content (User Request)
                                  print(f"[DEBUG RAW TASKS.md]\n{content}\n[END RAW]", flush=True)

                                  # It changed! Parse it.
                                  parsed_tasks = parse_tasks_md(content)
                                  
                                  if parsed_tasks:
                                      print(f"[Logger] Detected task update ({len(parsed_tasks)} items). Broadcasting...", flush=True)
                                      
                                      # PRINT TASKS TO CONSOLE (User Request)
                                      print("--- CURRENT TASKS ---", flush=True)
                                      for t in parsed_tasks:
                                          status_icon = "[ ]"
                                          if t['status'] == "completed": status_icon = "[x]"
                                          elif t['status'] == "in_progress": status_icon = "[/]"
                                          
                                          # Format: [ ] Title | Notes: Description...
                                          note_str = f" | Notes: {t['notes']}" if t.get('notes') else ""
                                          print(f"{status_icon} {t['title']}{note_str}", flush=True)
                                      print("---------------------", flush=True)
                                      
                                      send_to_pusher("task_update", parsed_tasks)
                                      last_known_tasks_hash = current_hash
                                  else:
                                      # Debug: why no tasks?
                                      print(f"[Logger] Content changed but parsed 0 tasks. Content sample: {content[:100]!r}", flush=True)
                                      
                  except Exception as e:
                      # Don't crash the logger if docker exec fails randomly
                      print(f"[Logger] Task watcher error: {e}", flush=True)
                      pass

          # Helper: Parse TASKS.md content (Supports Markdown Checklists & OpenHands Emojis & Notes)
          def parse_tasks_md(content):
              tasks = []
              lines = content.split('\n')
              
              current_task = None
              
              for line in lines:
                  line = line.strip()
                  if not line: continue

                  # 1. Try Markdown Checklist: - [ ] Task Name
                  match_md = re.match(r'- \[([ xX/])\] (.*)', line)
                  
                  # 2. Try Numbered List with Emojis: 1. ‚è≥ Task Name
                  match_num = re.match(r'\d+\.\s+(?:([^\w\s]+)\s+)?(.*)', line)
                  
                  if match_md:
                      if current_task: tasks.append(current_task)
                      
                      status_char = match_md.group(1).lower()
                      title = re.sub(r'<!--.*-->', '', match_md.group(2)).strip()
                      
                      status = "todo"
                      if status_char == 'x': status = "completed"
                      elif status_char == '/': status = "in_progress"
                      
                      current_task = {"id": len(tasks), "title": title, "status": status, "notes": ""}
                  
                  elif match_num:
                      if current_task: tasks.append(current_task)
                      
                      icon = match_num.group(1) or ""
                      title = re.sub(r'<!--.*-->', '', match_num.group(2)).strip()
                      
                      status = "todo"
                      if icon:
                          if any(c in icon for c in ['‚úÖ', '‚úî', '‚òë']): status = "completed"
                          elif any(c in icon for c in ['‚è≥', '‚ñ∂', 'üèÉ', 'üöß']): status = "in_progress"
                          # ‚è≥ usually means "planned/waiting", so keeping it as 'todo' unless explicitly active?
                          # User requested: "indicar / como todo".
                          # Let's map ‚è≥ to "todo" if that's the intention, OR "in_progress".
                          # OpenHands standard: ‚è≥ = In Progress / Working on it.
                          # But user asked: "indicar / como todo".
                          # If I output "[ ]" for ‚è≥, frontend shows [ ] (Todo).
                          if any(c in icon for c in ['‚è≥']): status = "todo" # Explicit User Request override?
                          elif any(c in icon for c in ['‚ñ∂', 'üèÉ']): status = "in_progress"
                          elif any(c in icon for c in ['‚ùå', 'üö´']): status = "failed"
                      
                      current_task = {"id": len(tasks), "title": title, "status": status, "notes": ""}
                  
                  else:
                      # It's a note line if we have a current task
                      if current_task:
                           # Append to notes
                           if current_task["notes"]:
                               current_task["notes"] += " " + line
                           else:
                               current_task["notes"] = line
              
              if current_task: tasks.append(current_task)
              return tasks

          # --- MAIN THREADS ---
          t_log = threading.Thread(target=log_worker, daemon=True)
          t_log.start()

          t_task = threading.Thread(target=task_watcher_worker, daemon=True)
          t_task.start()

          # --- PRODUCER (STDIN) ---
          try:
              for line in sys.stdin:
                  # 1. Print to stdout (Critical for GitHub UI)
                  sys.stdout.write(line)
                  sys.stdout.flush()
                  
                  # CRIITCAL: Check for Agent Loop Error
                  if "AgentStuckInLoopError" in line:
                      print("\n[Logger] CRITICAL: Agent stuck in loop detected. Aborting workflow.", flush=True)
                      send_to_pusher("log", {"message": "CRITICAL: Agent stuck in loop. Aborting."})
                      sys.exit(1)
                  
                  # 2. Add to Pusher Buffer
                  with buffer_lock:
                      log_buffer.append(line)
          except KeyboardInterrupt:
              pass
          finally:
              # Shutdown
              running = False
              t_log.join(timeout=3)
              t_task.join(timeout=1)
          EOF

      - name: Free Disk Space
        run: |
          (
            echo "Disk space before cleanup:"
            df -h
            sudo rm -rf /usr/local/lib/android
            sudo rm -rf /usr/share/dotnet
            sudo rm -rf /opt/ghc
            sudo rm -rf /usr/local/.ghcup
            echo "Disk space after cleanup:"
            df -h
          ) 2>&1 | python3 -u logger.py

      - name: Create Output Directory
        run: |
          (
             mkdir -p output/${{ github.event.client_payload.slug || inputs.slug }}
             echo "Created output directory: output/${{ github.event.client_payload.slug || inputs.slug }}"
          ) 2>&1 | python3 -u logger.py

      - name: Pre-execution Cleanup (HF)
        if: ${{ github.event.client_payload.cleanup == true }}
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          SLUG: ${{ github.event.client_payload.slug || inputs.slug }}
        run: |
          (
            echo "Cleanup requested. Removing existing Hugging Face data for $SLUG..."
            
            # Configure Git
            git config --global user.email "bot@maia.api"
            git config --global user.name "Maia Bot"

            # Clone Repo
            git clone --depth 1 https://user:$HF_TOKEN@huggingface.co/datasets/toquereflexo/maia-deep-search hf_cleanup_repo
            cd hf_cleanup_repo

            # Check and delete
            if [ -d "output/$SLUG" ]; then
                echo "Removing output/$SLUG..."
                git rm -rf "output/$SLUG"
                git commit -m "Cleanup (Retry) for $SLUG"
                git push
                echo "Cleanup committed."
            else
                echo "No existing data found at output/$SLUG."
            fi
            cd ..
            rm -rf hf_cleanup_repo
          ) 2>&1 | python3 -u logger.py

      - name: Pre-execution Cleanup (Pinecone)
        if: ${{ github.event.client_payload.cleanup == true }}
        env:
          WORKER_URL: https://maia-api-worker.willian-campos-ismart.workers.dev
          GH_PAT: ${{ secrets.GH_PAT }}
          SLUG: ${{ github.event.client_payload.slug || inputs.slug }}
        run: |
          (
             echo "Requesting Pinecone deletion for $SLUG..."
             curl -X POST "$WORKER_URL/delete-pinecone-record" \
                  -H "Content-Type: application/json" \
                  -H "Authorization: Bearer $GH_PAT" \
                  -d "{\"slug\": \"$SLUG\"}" \
             || echo "Warning: Failed to call delete endpoint"
          ) 2>&1 | python3 -u logger.py

      - name: Announce Job URL
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          RUN_ID: ${{ github.run_id }}
          REPO: ${{ github.repository }}
        run: |
          (
            # Fetch the Jobs for this Run
            JOBS_JSON=$(curl -s -H "Authorization: Bearer $GH_TOKEN" \
                        "https://api.github.com/repos/$REPO/actions/runs/$RUN_ID/jobs")
            
            # Extract the HTML URL of the current job
            JOB_URL=$(echo "$JOBS_JSON" | jq -r '.jobs[0].html_url')
            
            if [ "$JOB_URL" == "null" ] || [ -z "$JOB_URL" ]; then
                # Fallback to Run URL if API fails
                JOB_URL="${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
            fi

            echo "[SYSTEM_INFO] JOB_URL=$JOB_URL"
          ) 2>&1 | python3 -u logger.py

      - name: Run OpenHands Deep Search
        id: deep_search
        run: |
          set -euo pipefail

          # 1) Heredoc sem interpreta√ß√£o (n√£o executa backticks)
          PROMPT_TEMPLATE=$(cat <<'EOF'
          Voc√™ recebeu uma consulta curta (QUERY) do tipo "nome da prova + ano", por exemplo: "ita 2022".

          Objetivo: encontrar e organizar TODOS os links e arquivos (provas e gabaritos) associados a essa QUERY.

          QUERY: "__QUERY__"

          Regras para evitar pesquisa demais (muito importante):
          1) N√£o use Google como primeira op√ß√£o (CAPTCHA √© comum). Prefira:
            - Sites oficiais da banca/institui√ß√£o (dom√≠nio oficial).
            - P√°ginas ‚ÄúProvas anteriores / provas e gabaritos / exames anteriores / vestibular / arquivos‚Äù.
          2) Fa√ßa no m√°ximo 3 buscas no total (usando o mecanismo de busca configurado no OpenHands). Use buscas bem espec√≠ficas.
          3) Ap√≥s achar o site oficial, navegue dentro dele e extraia tudo com o m√≠nimo de novas buscas.
          4) S√≥ inclua links que pare√ßam realmente do exame/ano correspondente; quando estiver amb√≠guo, checar rapidamente pelo t√≠tulo/URL/nome do PDF.

          O que coletar:
          - Provas (todas as fases/dias/disciplinas/vers√µes que existirem).
          - Gabaritos correspondentes (e ‚Äúsolu√ß√µes‚Äù, se houver).
          - INDISPENS√ÅVEL: Classifique cada item no manifesto:
            - status: "downloaded" (se baixou com sucesso e validou) OU "reference" (se √© apenas um link externo que n√£o foi poss√≠vel baixar ou n√£o √© PDF).
          - Estrutura do JSON: {nome, tipo, ano, instituicao, fase, link_origem, status, filename (s√≥ se status=downloaded)}

          Prefer√™ncia de links:
          - Priorize links diretos de PDF.
          - Se for Google Drive, capture:
            - link "view"
            - e tamb√©m um link ‚Äúdownload direto‚Äù quando poss√≠vel (uc?export=download&id=...).

          Entrega (obrigat√≥rio):
          1) Crie a pasta: /workspace/output/__SLUG__/
          2) Baixe todos os PDFs que conseguir para: /workspace/output/__SLUG__/files/.
            CRITICO:
            - USE 'wget' ou 'curl' com verifica√ß√£o de header.
            - VERIFIQUE SE O HEADER 'Content-Type' √â 'application/pdf'.
            - SE O ARQUIVO BAIXADO TIVER MENOS DE 1KB, APAGUE-O IMEDIATAMENTE e mude o status para "reference".
            - SE O ARQUIVO CONTIVER HTML (DOCTYPE, <html>), APAGUE-O e mude o status para "reference".
            - N√ÉO CRIE ARQUIVOS FAKES OU COM CONTE√öDO GERADO. S√ì MANTENHA O QUE FOI BAIXADO DE VERDADE.
          3) Gere /workspace/output/__SLUG__/index.md
          4) Gere /workspace/output/__SLUG__/manifest.json (ESTE ARQUIVO √â CRITICO, DEVE SER JSON V√ÅLIDO. INCLUA ITENS "reference" E "downloaded")
          5) Gere /workspace/output/__SLUG__/DOWNLOAD_URLS.txt
          6) Gere /workspace/output/__SLUG__/slug.txt
             - IMPORTANTE: Escreva neste arquivo APENAS um nome curto e limpo para a pasta final (kebab-case).
             - Exemplo: se a query for 'provas do enem 2025', escreva 'enem-2025'.
             - Exemplo: se for 'unicamp 2024 segunda fase', escreva 'unicamp-2024-2f'.
             - Esse nome ser√° usado para renomear a pasta e salvar no banco de dados.
          7) Gere /workspace/output/__SLUG__.zip
          8) (Opcional) Gere /workspace/output/__SLUG__/index.pdf

          Ao final, responda apenas com caminhos e contagens.
          EOF
          )

          # Replace Placeholders
          PROMPT="${PROMPT_TEMPLATE/__QUERY__/$QUERY}"
          PROMPT="${PROMPT//__SLUG__/$SLUG}"

          # 3) Setup Docker volumes and permissions
          VOL_NAME="openhands-vol-${GITHUB_RUN_ID}"
          STATE_VOL_NAME="openhands-state-${GITHUB_RUN_ID}"
          docker volume create "$VOL_NAME"
          docker volume create "$STATE_VOL_NAME"

          docker run --rm \
            -v "$VOL_NAME:/workspace" \
            -v "$STATE_VOL_NAME:/workspace/.openhands" \
            -e TAVILY_API_KEY="$TAVILY_API_KEY" \
            alpine sh -c '
              set -eux
              mkdir -p /workspace/.openhands
              printf "[core]\nsearch_api_key = \"%s\"\n" "$TAVILY_API_KEY" > /workspace/config.toml
              rm -f /workspace/.openhands/.jwt_secret || true
              chmod -R 777 /workspace
            '
          docker rm -f openhands-app 2>/dev/null || true

          # 4) Run Docker piped to Python
          docker_args=(
            run --pull=always
            -w /workspace
            -v /var/run/docker.sock:/var/run/docker.sock
            -v "$VOL_NAME:/workspace:rw"
            -v "$STATE_VOL_NAME:/workspace/.openhands"
            --add-host host.docker.internal:host-gateway
            -e "SANDBOX_USER_ID=$(id -u)"
            -e "SANDBOX_VOLUMES=$VOL_NAME:/workspace:rw"
            -e "SANDBOX_RUNTIME_CONTAINER_IMAGE=docker.openhands.dev/openhands/runtime:1.0-nikolaik"
            -e "LOG_ALL_EVENTS=true"
            -e "LLM_API_KEY=$LLM_API_KEY"
            -e "LLM_MODEL=gemini/gemini-3-flash-preview"
            -e "TAVILY_API_KEY=$TAVILY_API_KEY"
            -e "PYTHONUNBUFFERED=1"
            -e "PROMPT=$PROMPT"
            -e "FILE_STORE=local"
            -e "FILE_STORE_PATH=/workspace/.openhands"
            --name openhands-app
            docker.openhands.dev/openhands/openhands:1.0
            python -m openhands.core.main -t "$PROMPT"
          )

          # Pipe to python script. Secrets are now in the step env.
          docker "${docker_args[@]}" 2>&1 | python3 logger.py

          EXIT_CODE=${PIPESTATUS[0]}
          echo "Docker run finished with exit code $EXIT_CODE"

          echo "Copying artifacts from container to host..."
          docker cp openhands-app:/workspace/output/${SLUG}/. "output/${SLUG}/" || echo "Failed to copy artifacts or directory empty"

          # --- DYNAMIC RENAMING LOGIC ---
          FINAL_SLUG="$SLUG"
          if [ -f "output/${SLUG}/slug.txt" ]; then
             CANDIDATE_SLUG=$(cat "output/${SLUG}/slug.txt" | tr -cd '[:alnum:]-')
             if [ -n "$CANDIDATE_SLUG" ] && [ "$CANDIDATE_SLUG" != "$SLUG" ]; then
                 echo "Renaming artifact from $SLUG to $CANDIDATE_SLUG..."
                 mv "output/${SLUG}" "output/${CANDIDATE_SLUG}"
                 FINAL_SLUG="$CANDIDATE_SLUG"
             fi
          fi
          echo "final_slug=$FINAL_SLUG" >> $GITHUB_OUTPUT
          # ------------------------------

          docker rm -f openhands-app || true
          docker volume rm "$VOL_NAME" || true
          docker volume rm "$STATE_VOL_NAME" || true

          exit $EXIT_CODE

      - name: Generate Thumbnails
        env:
          SLUG: ${{ steps.deep_search.outputs.final_slug }}
        run: |
          # Create script outside of subshell to avoid heredoc indentation issues
          cat << 'EOF' > generate_thumbnails.py
          import os
          import json
          from pdf2image import convert_from_path
          from pathlib import Path

          slug = os.environ.get("SLUG")
          if not slug:
              print("[Thumb] SLUG env var missing.")
              exit(1)

          base_dir = Path(f"output/{slug}")
          thumbs_dir = base_dir / "thumbnails"
          thumbs_dir.mkdir(exist_ok=True)
          manifest_path = base_dir / "manifest.json"

          if manifest_path.exists():
              try:
                  with open(manifest_path, 'r', encoding='utf-8') as f:
                      data = json.load(f)
                  
                  # Normalize list structure (handle list or dict wrapper)
                  items = []
                  if isinstance(data, list):
                      items = data
                  elif isinstance(data, dict):
                      items = data.get('results', []) or data.get('files', [])

                  updated_count = 0
                  for item in items:
                      # Check "status" == "downloaded" OR existence of file in "files/"
                      fname = item.get('filename')
                      if not fname: 
                          # Fallback: try to guess from path if available
                          path = item.get('path')
                          if path: fname = Path(path).name

                      if fname and fname.lower().endswith('.pdf'):
                          # Try to locate the file
                          pdf_path = base_dir / "files" / fname
                          # If not in files/, maybe it's at root of output? (Unlikely based on logic, but safe to check)
                          if not pdf_path.exists():
                              pdf_path = base_dir / fname
                          
                          if pdf_path.exists():
                              try:
                                  # Convert first page (DPI 150 is good balance)
                                  images = convert_from_path(str(pdf_path), first_page=1, last_page=1, dpi=150)
                                  if images:
                                      thumb_name = f"{Path(fname).stem}.jpg"
                                      thumb_path = thumbs_dir / thumb_name
                                      images[0].save(thumb_path, 'JPEG', quality=80)
                                      
                                      # Update manifest with Relative Path
                                      item['thumbnail'] = f"thumbnails/{thumb_name}"
                                      updated_count += 1
                                      print(f"[Thumb] Generated: {thumb_name}")
                              except Exception as e:
                                  print(f"[Thumb] Failed for {fname}: {e}")
                  
                  # Save updated manifest
                  if updated_count > 0:
                      with open(manifest_path, 'w', encoding='utf-8') as f:
                          json.dump(items, f, indent=2, ensure_ascii=False)
                      print(f"[Thumb] Success. Generated {updated_count} thumbnails.")
                  else:
                      print("[Thumb] No thumbnails generated (no PDFs found or errors).")

              except Exception as e:
                  print(f"[Thumb] Critical Error processing manifest: {e}")
          else:
              print(f"[Thumb] Manifest not found at {manifest_path}")
          EOF

          (
            echo "Installing dependencies for thumbnail generation..."
            sudo apt-get update && sudo apt-get install -y poppler-utils
            pip install pdf2image

            echo "Generating thumbnails..."
            python3 generate_thumbnails.py
          ) 2>&1 | python3 logger.py

      - name: Upload Search Results
        uses: actions/upload-artifact@v4
        with:
          name: deep-search-artifact
          path: output/${{ steps.deep_search.outputs.final_slug }}

      - name: Push to Hugging Face Dataset
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          (
            echo "Starting upload to Hugging Face..."
            git config --global user.email "bot@maia.api"
            git config --global user.name "Maia Bot"
            
            # Optimization for Large Pushes (Fixes 503 RPC failed)
            git config --global http.postBuffer 524288000
            git config --global http.maxRequestBuffer 104857600
            git config --global lfs.activitytimeout 300
            git config --global lfs.dialtimeout 300
            git config --global http.version HTTP/1.1

            # Clone HF Repo
            git clone --depth 1 https://user:$HF_TOKEN@huggingface.co/datasets/toquereflexo/maia-deep-search hf_repo
            
            slug="${{ steps.deep_search.outputs.final_slug }}"
            echo "Processing slug: $slug"
            
            # Ensure target directory exists
            mkdir -p hf_repo/output/$slug

            # Simple Copy: Overwrite/Merge local output into repo output
            # This follows the user instruction: "basicamente s√≥ colocava tudo que a ia gerou em output/slug/"
            cp -R output/$slug/* hf_repo/output/$slug/

            cd hf_repo
            
            # LFS Setup
            git lfs install
            git lfs track "*.pdf" "*.zip" "*.rar" "*.doc" "*.docx" "*.jpg" "*.jpeg"
            git add .gitattributes

            git add .
            git commit -m "Add search results for $slug" || echo "No changes to commit"
            
            # Retry Loop for Push (Fixes transient network failures)
            n=0
            until [ "$n" -ge 5 ]
            do
               git push && break
               n=$((n+1)) 
               echo "Push failed. Retrying in 10s... (Attempt $n/5)"
               sleep 10
            done
            if [ "$n" -ge 5 ]; then
               echo "Failed to push after 5 attempts."
               exit 1
            fi
            echo "Upload to Hugging Face completed."
          ) 2>&1 | python3 logger.py

      - name: Update Semantic Cache
        if: success()
        env:
          WORKER_URL: https://maia-api-worker.willian-campos-ismart.workers.dev
          GH_PAT: ${{ secrets.GH_PAT }}
          QUERY: ${{ github.event.client_payload.query || inputs.query }}
          SLUG: ${{ steps.deep_search.outputs.final_slug }}
        run: |
          (
            echo "Updating Semantic Cache at $WORKER_URL..."

            MANIFEST_PATH="output/$SLUG/manifest.json"

            # Generate Clean Query from Slug (replace dashes with spaces)
            CLEAN_QUERY="${SLUG//-/ }"
            echo "Using Clean Query for Embedding: $CLEAN_QUERY"

            if [ -f "$MANIFEST_PATH" ]; then
              
              # Using jq to create a safe JSON payload
              PAYLOAD=$(jq -n \
                      --arg query "$CLEAN_QUERY" \
                      --arg original_query "$QUERY" \
                      --arg slug "$SLUG" \
                      --slurpfile manifest "$MANIFEST_PATH" \
                      '{
                        query: $query, 
                        slug: $slug, 
                        metadata: {
                          source: "deep-search",
                          original_query: $original_query,
                          file_count: ($manifest[0] | length),
                          institution: ($manifest[0][0].instituicao // $manifest[0][0].institution // "unknown"),
                          year: ($manifest[0][0].ano // $manifest[0][0].year // "unknown")
                        }
                      }')
              
              
              # Curl Request with -f (fail on error) and capturing output
              RESPONSE=$(curl -s -w "\nHTTP_CODE:%{http_code}" -X POST "$WORKER_URL/update-deep-search-cache" \
                   -H "Content-Type: application/json" \
                   -H "Authorization: Bearer $GH_PAT" \
                   -d "$PAYLOAD")
              
              HTTP_CODE=$(echo "$RESPONSE" | tail -n1 | cut -d: -f2)
              BODY=$(echo "$RESPONSE" | sed '$d')

              echo "Worker Response Body: $BODY"
              echo "Worker HTTP Code: $HTTP_CODE"

              if [ "$HTTP_CODE" -ne 200 ]; then
                echo "::error::Failed to update cache. HTTP $HTTP_CODE"
                exit 1
              fi
            else
              echo "Manifest not found at $MANIFEST_PATH. Skipping cache update."
            fi
          ) 2>&1 | python3 logger.py

      - name: Notify Completion
        if: ${{ always() }}
        env:
          PUSHER_APP_ID: ${{ secrets.PUSHER_APP_ID }}
          PUSHER_KEY: ${{ secrets.PUSHER_KEY }}
          PUSHER_SECRET: ${{ secrets.PUSHER_SECRET }}
          PUSHER_CLUSTER: ${{ secrets.PUSHER_CLUSTER }}
          SLUG: ${{ github.event.client_payload.slug || inputs.slug }}
          FINAL_SLUG: ${{ steps.deep_search.outputs.final_slug }}
          JOB_STATUS: ${{ job.status }}
        run: |
          LOG_URL="${GITHUB_SERVER_URL}/${GITHUB_REPOSITORY}/actions/runs/${GITHUB_RUN_ID}"

          if [ "$JOB_STATUS" == "success" ]; then
             MESSAGE="COMPLETED. Logs: $LOG_URL"
             EVENT_NAME="log"
          elif [ "$JOB_STATUS" == "cancelled" ]; then
             MESSAGE="CANCELLED. Job stopped by user. Logs: $LOG_URL"
             EVENT_NAME="log"
          else
             MESSAGE="FAILED. Something went wrong. Logs: $LOG_URL"
             EVENT_NAME="log"
             # Important: The frontend expects "Job failed" or similar to trigger fail state
             # We can send a specific log line that triggers it
          fi

          # Simple Pusher Trigger
          TS=$(date +%s)

          # Include new_slug in data
          DATA_JSON=$(jq -n \
                  --arg msg "$MESSAGE" \
                  --arg final_slug "$FINAL_SLUG" \
                  --arg status "$JOB_STATUS" \
                  '{message: $msg, new_slug: $final_slug, status: $status}')

          # Send to ORIGINAL slug channel so frontend hears it
          BODY=$(jq -n \
                  --arg name "$EVENT_NAME" \
                  --arg channel "$SLUG" \
                  --arg data "$DATA_JSON" \
                  '{name: $name, channels: [$channel], data: $data}')

          BODY_MD5=$(echo -n "$BODY" | md5sum | awk '{print $1}')
          SIGN_STRING="POST\n/apps/$PUSHER_APP_ID/events\nauth_key=$PUSHER_KEY&auth_timestamp=$TS&auth_version=1.0&body_md5=$BODY_MD5"
          AUTH_SIGNATURE=$(echo -n -e "$SIGN_STRING" | openssl dgst -sha256 -hmac "$PUSHER_SECRET" | sed 's/^.* //')

          curl -s -X POST "https://api-$PUSHER_CLUSTER.pusher.com/apps/$PUSHER_APP_ID/events?auth_key=$PUSHER_KEY&auth_timestamp=$TS&auth_version=1.0&body_md5=$BODY_MD5&auth_signature=$AUTH_SIGNATURE" \
                 -H "Content-Type: application/json" \
                 -d "$BODY"
