name: Deep Search (OpenHands)

on:
  repository_dispatch:
    types: [deep-search]
  workflow_dispatch:
    inputs:
      query:
        description: 'Search Query (e.g. "ita 2022")'
        required: true
      slug:
        description: 'Slug for folder name (e.g. "ita-2022")'
        required: true
      ntfy_topic:
        description: "ntfy.sh topic for log streaming"
        required: false

jobs:
  search:
    runs-on: ubuntu-latest
    timeout-minutes: 25
    permissions:
      contents: write

    steps:
      - name: Checkout Repo
        uses: actions/checkout@v4

      - name: Free Disk Space
        run: |
          echo "Disk space before cleanup:"
          df -h
          sudo rm -rf /usr/local/lib/android
          sudo rm -rf /usr/share/dotnet
          sudo rm -rf /opt/ghc
          sudo rm -rf /usr/local/.ghcup
          echo "Disk space after cleanup:"
          df -h

      - name: Create Output Directory
        run: mkdir -p output/${{ github.event.client_payload.slug || inputs.slug }}

      - name: Run OpenHands Deep Search
        env:
          LLM_API_KEY: ${{ secrets.LLM_API_KEY }}
          TAVILY_API_KEY: ${{ secrets.TAVILY_API_KEY }}
          NTFY_TOPIC: ${{ github.event.client_payload.ntfy_topic || inputs.ntfy_topic }}
          QUERY: ${{ github.event.client_payload.query || inputs.query }}
          SLUG: ${{ github.event.client_payload.slug || inputs.slug }}
        run: |
          set -euo pipefail

          # 1) Heredoc sem interpretação (não executa backticks)
          PROMPT_TEMPLATE=$(cat <<'EOF'
          Você recebeu uma consulta curta (QUERY) do tipo "nome da prova + ano", por exemplo: "ita 2022".

          Objetivo: encontrar e organizar TODOS os links e arquivos (provas e gabaritos) associados a essa QUERY.

          QUERY: "__QUERY__"

          Regras para evitar pesquisa demais (muito importante):
          1) Não use Google como primeira opção (CAPTCHA é comum). Prefira:
            - Sites oficiais da banca/instituição (domínio oficial).
            - Páginas “Provas anteriores / provas e gabaritos / exames anteriores / vestibular / arquivos”.
          2) Faça no máximo 3 buscas no total (usando o mecanismo de busca configurado no OpenHands). Use buscas bem específicas.
          3) Após achar o site oficial, navegue dentro dele e extraia tudo com o mínimo de novas buscas.
          4) Só inclua links que pareçam realmente do exame/ano correspondente; quando estiver ambíguo, checar rapidamente pelo título/URL/nome do PDF.

          O que coletar:
          - Provas (todas as fases/dias/disciplinas/versões que existirem).
          - Gabaritos correspondentes (e “soluções”, se houver).
          - INDISPENSÁVEL: Classifique cada item no manifesto:
            - status: "downloaded" (se baixou com sucesso e validou) OU "reference" (se é apenas um link externo que não foi possível baixar ou não é PDF).
          - Estrutura do JSON: {nome, tipo, ano, fase, link_origem, status, filename (só se status=downloaded)}

          Preferência de links:
          - Priorize links diretos de PDF.
          - Se for Google Drive, capture:
            - link "view"
            - e também um link “download direto” quando possível (uc?export=download&id=...).

          Entrega (obrigatório):
          1) Crie a pasta: /workspace/output/${SLUG}/
          2) Baixe todos os PDFs que conseguir para: /workspace/output/${SLUG}/files/.
            CRITICO:
            - USE 'wget' ou 'curl' com verificação de header.
            - VERIFIQUE SE O HEADER 'Content-Type' É 'application/pdf'.
            - SE O ARQUIVO BAIXADO TIVER MENOS DE 1KB, APAGUE-O IMEDIATAMENTE e mude o status para "reference".
            - SE O ARQUIVO CONTIVER HTML (DOCTYPE, <html>), APAGUE-O e mude o status para "reference".
            - NÃO CRIE ARQUIVOS FAKES OU COM CONTEÚDO GERADO. SÓ MANTENHA O QUE FOI BAIXADO DE VERDADE.
          3) Gere /workspace/output/${SLUG}/index.md
          4) Gere /workspace/output/${SLUG}/manifest.json (ESTE ARQUIVO É CRITICO, DEVE SER JSON VÁLIDO. INCLUA ITENS "reference" E "downloaded")
          5) Gere /workspace/output/${SLUG}/DOWNLOAD_URLS.txt
          6) Gere /workspace/output/${SLUG}.zip
          7) (Opcional) Gere /workspace/output/${SLUG}/index.pdf

          Ao final, responda apenas com caminhos e contagens.
          EOF
          )

          PROMPT="${PROMPT_TEMPLATE/__QUERY__/$QUERY}"

          log_to_pusher() {
            local app_id="${{ secrets.PUSHER_APP_ID }}"
            local key="${{ secrets.PUSHER_KEY }}"
            local secret="${{ secrets.PUSHER_SECRET }}"
            local cluster="${{ secrets.PUSHER_CLUSTER }}"
            local channel="${{ github.event.client_payload.slug || inputs.slug }}" # Channel is the SLUG
            local event="log"

            # Check if secrets are present
            if [ -z "$app_id" ] || [ -z "$key" ] || [ -z "$secret" ] || [ -z "$cluster" ]; then
              # If secrets missing, just cat to console
              cat
              return
            fi

            while IFS= read -r line; do
              echo "$line"
              
              # Escape JSON content properly
              # Using python for reliable JSON escaping since jq might not be available or fail on some chars
              # If python is available in alpine:
              ESCAPED_LINE=$(echo "$line" | sed 's/\\/\\\\/g' | sed 's/"/\\"/g' | sed "s/'/\\'/g")
              
              # Timestamp
              TS=$(date +%s)
              
              # Construct body
              BODY="{\"name\":\"$event\",\"channels\":[\"$channel\"],\"data\":\"{\\\"message\\\":\\\"$ESCAPED_LINE\\\"}\"}"
              
              # MD5 of body
              BODY_MD5=$(echo -n "$BODY" | md5sum | awk '{print $1}')
              
              # Auth Signature
              # POST\n/apps/$app_id/events\nauth_key=$key&auth_timestamp=$TS&auth_version=1.0&body_md5=$BODY_MD5
              SIGN_STRING="POST\n/apps/$app_id/events\nauth_key=$key&auth_timestamp=$TS&auth_version=1.0&body_md5=$BODY_MD5"
              AUTH_SIGNATURE=$(echo -n -e "$SIGN_STRING" | openssl dgst -sha256 -hmac "$secret" | sed 's/^.* //')
              
              # Send to Pusher
              curl -s -X POST "https://api-$cluster.pusher.com/apps/$app_id/events?auth_key=$key&auth_timestamp=$TS&auth_version=1.0&body_md5=$BODY_MD5&auth_signature=$AUTH_SIGNATURE" \
                   -H "Content-Type: application/json" \
                   -d "$BODY" > /dev/null || true
            done
          }

          echo "Starting OpenHands Docker Container..." | log_to_pusher
          echo "[GITHUB_LOGS] ${GITHUB_SERVER_URL}/${GITHUB_REPOSITORY}/actions/runs/${GITHUB_RUN_ID}" | log_to_pusher

          VOL_NAME="openhands-vol-${GITHUB_RUN_ID}"
          STATE_VOL_NAME="openhands-state-${GITHUB_RUN_ID}"
          docker volume create "$VOL_NAME"
          docker volume create "$STATE_VOL_NAME"

          docker run --rm \
            -v "$VOL_NAME:/workspace" \
            -v "$STATE_VOL_NAME:/workspace/.openhands" \
            -e TAVILY_API_KEY="$TAVILY_API_KEY" \
            alpine sh -c '
              set -eux
              
              # 1. Garante que os diretórios existem
              mkdir -p /workspace/.openhands

              # 2. Cria o config.toml
              printf "[core]\nsearch_api_key = \"%s\"\n" "$TAVILY_API_KEY" > /workspace/config.toml

              # 3. Limpeza preventiva
              rm -f /workspace/.openhands/.jwt_secret || true

              # 4. A CORREÇÃO: Permissão total recursiva
              # Em vez de chown 1001:1001 (que pode estar errado), liberamos tudo.
              # Isso permite que o container OpenHands leia/escreva independentemente do usuário que ele usa.
              chmod -R 777 /workspace
            '
          docker rm -f openhands-app 2>/dev/null || true

          # 2) Chamada robusta: sem "\" com risco de whitespace no fim
          docker_args=(
            run --pull=always
            -w /workspace
            -v /var/run/docker.sock:/var/run/docker.sock
            -v "$VOL_NAME:/workspace:rw"
            -v "$STATE_VOL_NAME:/workspace/.openhands"
            --add-host host.docker.internal:host-gateway
            -e "SANDBOX_USER_ID=$(id -u)"
            -e "SANDBOX_VOLUMES=$VOL_NAME:/workspace:rw"
            -e "SANDBOX_RUNTIME_CONTAINER_IMAGE=docker.openhands.dev/openhands/runtime:1.0-nikolaik"
            -e "LOG_ALL_EVENTS=true"
            -e "LLM_API_KEY=$LLM_API_KEY"
            -e "LLM_MODEL=gemini/gemini-3-flash-preview"
            -e "TAVILY_API_KEY=$TAVILY_API_KEY"
            -e "PYTHONUNBUFFERED=1"
            -e "PROMPT=$PROMPT"
            -e "FILE_STORE=local"
            -e "FILE_STORE_PATH=/workspace/.openhands"
            --name openhands-app
            docker.openhands.dev/openhands/openhands:1.0
            python -m openhands.core.main -t "$PROMPT"
          )
          docker "${docker_args[@]}" 2>&1 | log_to_pusher

          EXIT_CODE=${PIPESTATUS[0]}
          echo "Docker run finished with exit code $EXIT_CODE"

          echo "Copying artifacts from container to host..."
          docker cp openhands-app:/workspace/output/${SLUG}/. "output/${SLUG}/" || echo "Failed to copy artifacts or directory empty"

          docker rm -f openhands-app || true
          docker volume rm "$VOL_NAME" || true
          docker volume rm "$STATE_VOL_NAME" || true

          exit $EXIT_CODE

      - name: Upload Search Results
        uses: actions/upload-artifact@v4
        with:
          name: deep-search-artifact
          path: output/${{ github.event.client_payload.slug || inputs.slug }}

      - name: Push to Hugging Face Dataset
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          # Configure Git
          git config --global user.email "bot@maia.api"
          git config --global user.name "Maia Bot"

          # Clone HF Repo (using depth 1 for speed)
          git clone --depth 1 https://user:$HF_TOKEN@huggingface.co/datasets/toquereflexo/maia-deep-search hf_repo

          # Prepare Directories
          slug="${{ github.event.client_payload.slug || inputs.slug }}"
          mkdir -p hf_repo/output/$slug

          # Copy Artifacts
          cp -r output/$slug/* hf_repo/output/$slug/

          # Commit and Push
          cd hf_repo

          # Setup LFS for binaries
          git lfs install
          git lfs track "*.pdf" "*.zip" "*.rar" "*.doc" "*.docx"
          git add .gitattributes

          git add .
          git commit -m "Add deep search results for $slug" || echo "No changes to commit"
          git push

      - name: Update Semantic Cache
        if: success()
        env:
          WORKER_URL: https://maia-api-worker.willian-campos-ismart.workers.dev
          GH_PAT: ${{ secrets.GH_PAT }} # Using GH_PAT matching the Worker's expected secret
          QUERY: ${{ github.event.client_payload.query || inputs.query }}
          SLUG: ${{ github.event.client_payload.slug || inputs.slug }}
        run: |
          echo "Updating Semantic Cache at $WORKER_URL..."

          MANIFEST_PATH="output/$SLUG/manifest.json"

          if [ -f "$MANIFEST_PATH" ]; then
            # Extract relevant metadata (institution, year, etc) if possible, 
            # or just send the whole manifest as 'metadata.manifest' or similar.
            # The worker expects 'metadata' object. We can send the manifest content as part of it.
            # Ideally we want flat metadata for filtering: institution, year. 
            # For now, let's send the whole manifest as metadata (or a subset if it's too big).
            # Pinecone metadata limit is 40KB. Manifest might be large.
            # Let's just send basic info + file count.
            
            # Using jq to create a safe JSON payload
            PAYLOAD=$(jq -n \
                    --arg query "$QUERY" \
                    --arg slug "$SLUG" \
                    --slurpfile manifest "$MANIFEST_PATH" \
                    '{
                      query: $query, 
                      slug: $slug, 
                      metadata: {
                        source: "deep-search",
                        file_count: ($manifest[0] | length),
                        institution: ($manifest[0][0].institution // "unknown"),
                        year: ($manifest[0][0].year // "unknown")
                      }
                    }')
            
            # Curl Request
            curl -X POST "$WORKER_URL/update-deep-search-cache" \
                 -H "Content-Type: application/json" \
                 -H "Authorization: Bearer $GH_PAT" \
                 -d "$PAYLOAD"
          else
            echo "Manifest not found at $MANIFEST_PATH. Skipping cache update."
          fi

      - name: Notify Completion
        if: ${{ always() }}
        env:
          PUSHER_APP_ID: ${{ secrets.PUSHER_APP_ID }}
          PUSHER_KEY: ${{ secrets.PUSHER_KEY }}
          PUSHER_SECRET: ${{ secrets.PUSHER_SECRET }}
          PUSHER_CLUSTER: ${{ secrets.PUSHER_CLUSTER }}
          SLUG: ${{ github.event.client_payload.slug || inputs.slug }}
        run: |
            LOG_URL="${GITHUB_SERVER_URL}/${GITHUB_REPOSITORY}/actions/runs/${GITHUB_RUN_ID}"
            MESSAGE="COMPLETED. Logs: $LOG_URL"
            
            # Simple Pusher Trigger for Completion
            TS=$(date +%s)
            BODY="{\"name\":\"log\",\"channels\":[\"$SLUG\"],\"data\":\"{\\\"message\\\":\\\"$MESSAGE\\\"}\"}"
            BODY_MD5=$(echo -n "$BODY" | md5sum | awk '{print $1}')
            SIGN_STRING="POST\n/apps/$PUSHER_APP_ID/events\nauth_key=$PUSHER_KEY&auth_timestamp=$TS&auth_version=1.0&body_md5=$BODY_MD5"
            AUTH_SIGNATURE=$(echo -n -e "$SIGN_STRING" | openssl dgst -sha256 -hmac "$PUSHER_SECRET" | sed 's/^.* //')
            
            curl -s -X POST "https://api-$PUSHER_CLUSTER.pusher.com/apps/$PUSHER_APP_ID/events?auth_key=$PUSHER_KEY&auth_timestamp=$TS&auth_version=1.0&body_md5=$BODY_MD5&auth_signature=$AUTH_SIGNATURE" \
                   -H "Content-Type: application/json" \
                   -d "$BODY"
