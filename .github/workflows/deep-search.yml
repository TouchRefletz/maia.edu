name: Deep Search (OpenHands)

on:
  repository_dispatch:
    types: [deep-search]
  workflow_dispatch:
    inputs:
      query:
        description: 'Search Query (e.g. "ita 2022")'
        required: true
      slug:
        description: 'Slug for folder name (e.g. "ita-2022")'
        required: true
      ntfy_topic:
        description: "ntfy.sh topic for log streaming"
        required: false

jobs:
  search:
    runs-on: ubuntu-latest
    timeout-minutes: 25
    permissions:
      contents: write

    steps:
      - name: Checkout Repo
        uses: actions/checkout@v4

      - name: Free Disk Space
        run: |
          echo "Disk space before cleanup:"
          df -h
          sudo rm -rf /usr/local/lib/android
          sudo rm -rf /usr/share/dotnet
          sudo rm -rf /opt/ghc
          sudo rm -rf /usr/local/.ghcup
          echo "Disk space after cleanup:"
          df -h

      - name: Create Output Directory
        run: mkdir -p output/${{ github.event.client_payload.slug || inputs.slug }}

      - name: Run OpenHands Deep Search
        env:
          LLM_API_KEY: ${{ secrets.LLM_API_KEY }}
          TAVILY_API_KEY: ${{ secrets.TAVILY_API_KEY }}
          NTFY_TOPIC: ${{ github.event.client_payload.ntfy_topic || inputs.ntfy_topic }}
          QUERY: ${{ github.event.client_payload.query || inputs.query }}
          SLUG: ${{ github.event.client_payload.slug || inputs.slug }}
          PUSHER_APP_ID: ${{ secrets.PUSHER_APP_ID }}
          PUSHER_KEY: ${{ secrets.PUSHER_KEY }}
          PUSHER_SECRET: ${{ secrets.PUSHER_SECRET }}
          PUSHER_CLUSTER: ${{ secrets.PUSHER_CLUSTER }}
        run: |
          set -euo pipefail

          # 1) Heredoc sem interpretação (não executa backticks)
          PROMPT_TEMPLATE=$(cat <<'EOF'
          Você recebeu uma consulta curta (QUERY) do tipo "nome da prova + ano", por exemplo: "ita 2022".

          Objetivo: encontrar e organizar TODOS os links e arquivos (provas e gabaritos) associados a essa QUERY.

          QUERY: "__QUERY__"

          Regras para evitar pesquisa demais (muito importante):
          1) Não use Google como primeira opção (CAPTCHA é comum). Prefira:
            - Sites oficiais da banca/instituição (domínio oficial).
            - Páginas “Provas anteriores / provas e gabaritos / exames anteriores / vestibular / arquivos”.
          2) Faça no máximo 3 buscas no total (usando o mecanismo de busca configurado no OpenHands). Use buscas bem específicas.
          3) Após achar o site oficial, navegue dentro dele e extraia tudo com o mínimo de novas buscas.
          4) Só inclua links que pareçam realmente do exame/ano correspondente; quando estiver ambíguo, checar rapidamente pelo título/URL/nome do PDF.

          O que coletar:
          - Provas (todas as fases/dias/disciplinas/versões que existirem).
          - Gabaritos correspondentes (e “soluções”, se houver).
          - INDISPENSÁVEL: Classifique cada item no manifesto:
            - status: "downloaded" (se baixou com sucesso e validou) OU "reference" (se é apenas um link externo que não foi possível baixar ou não é PDF).
          - Estrutura do JSON: {nome, tipo, ano, instituicao, fase, link_origem, status, filename (só se status=downloaded)}

          Preferência de links:
          - Priorize links diretos de PDF.
          - Se for Google Drive, capture:
            - link "view"
            - e também um link “download direto” quando possível (uc?export=download&id=...).

          Entrega (obrigatório):
          1) Crie a pasta: /workspace/output/${SLUG}/
          2) Baixe todos os PDFs que conseguir para: /workspace/output/${SLUG}/files/.
            CRITICO:
            - USE 'wget' ou 'curl' com verificação de header.
            - VERIFIQUE SE O HEADER 'Content-Type' É 'application/pdf'.
            - SE O ARQUIVO BAIXADO TIVER MENOS DE 1KB, APAGUE-O IMEDIATAMENTE e mude o status para "reference".
            - SE O ARQUIVO CONTIVER HTML (DOCTYPE, <html>), APAGUE-O e mude o status para "reference".
            - NÃO CRIE ARQUIVOS FAKES OU COM CONTEÚDO GERADO. SÓ MANTENHA O QUE FOI BAIXADO DE VERDADE.
          3) Gere /workspace/output/${SLUG}/index.md
          4) Gere /workspace/output/${SLUG}/manifest.json (ESTE ARQUIVO É CRITICO, DEVE SER JSON VÁLIDO. INCLUA ITENS "reference" E "downloaded")
          5) Gere /workspace/output/${SLUG}/DOWNLOAD_URLS.txt
          6) Gere /workspace/output/${SLUG}.zip
          7) (Opcional) Gere /workspace/output/${SLUG}/index.pdf

          Ao final, responda apenas com caminhos e contagens.
          EOF
          )

          PROMPT="${PROMPT_TEMPLATE/__QUERY__/$QUERY}"

          # 2) Python Logger (Non-blocking)
          cat << 'EOF' > logger.py
          import sys
          import time
          import json
          import hashlib
          import hmac
          import os
          import threading
          import http.client
          import urllib.parse
          from collections import deque

          # Config
          PUSHER_APP_ID = os.environ.get("PUSHER_APP_ID")
          PUSHER_KEY = os.environ.get("PUSHER_KEY")
          PUSHER_SECRET = os.environ.get("PUSHER_SECRET")
          PUSHER_CLUSTER = os.environ.get("PUSHER_CLUSTER")
          CHANNEL = os.environ.get("SLUG")

          if not all([PUSHER_APP_ID, PUSHER_KEY, PUSHER_SECRET, PUSHER_CLUSTER, CHANNEL]):
              print("Missing Pusher secrets. Logging to stdout only.")
              # Fallback: just print everything
              for line in sys.stdin:
                  sys.stdout.write(line)
                  sys.stdout.flush()
              sys.exit(0)

          # Buffer
          log_buffer = deque()
          buffer_lock = threading.Lock()
          running = True

          def send_batch(lines):
              if not lines:
                  return
              
              timestamp = str(int(time.time()))
              # Join lines reliably
              # Note: Javascript expects a single "message" string usually, but we want to stream lines.
              # To allow the frontend to process line-by-line easily, we can send them joined by newlines 
              # or send individual events. Sending deeply nested JSON in a single event is better for rate limits.
              # Let's send the raw chunk.
              text_chunk = "".join(lines)
              
              # Construct Body
              body_data = json.dumps({"message": text_chunk})
              body = json.dumps({
                  "name": "log",
                  "channels": [CHANNEL],
                  "data": body_data
              })
              
              body_md5 = hashlib.md5(body.encode('utf-8')).hexdigest()
              
              sign_string = f"POST\n/apps/{PUSHER_APP_ID}/events\nauth_key={PUSHER_KEY}&auth_timestamp={timestamp}&auth_version=1.0&body_md5={body_md5}"
              auth_signature = hmac.new(PUSHER_SECRET.encode('utf-8'), sign_string.encode('utf-8'), hashlib.sha256).hexdigest()
              
              params = urllib.parse.urlencode({
                  'auth_key': PUSHER_KEY,
                  'auth_timestamp': timestamp,
                  'auth_version': '1.0',
                  'body_md5': body_md5,
                  'auth_signature': auth_signature
              })
              
              try:
                  conn = http.client.HTTPSConnection(f"api-{PUSHER_CLUSTER}.pusher.com", timeout=3)
                  headers = {'Content-Type': 'application/json'}
                  conn.request("POST", f"/apps/{PUSHER_APP_ID}/events?{params}", body, headers)
                  resp = conn.getresponse()
                  resp.read() # Consume
                  conn.close()
              except Exception as e:
                  print(f"Pusher Error: {e}")

          def consumer_thread():
              while running or log_buffer:
                  with buffer_lock:
                      if not log_buffer:
                          batch = []
                      else:
                          batch = list(log_buffer)
                          log_buffer.clear()
                  
                  if batch:
                      send_batch(batch)
                  
                  # Sleep/Tick
                  # If we just sent data, maybe sleep less. If empty, sleep 1s.
                  # For responsiveness, 1s is fine.
                  time.sleep(1.0)

          # Start Consumer
          t = threading.Thread(target=consumer_thread)
          t.daemon = True
          t.start()

          # Main Producer Loop
          for line in sys.stdin:
              # Always print to stdout for GitHub logs
              sys.stdout.write(line)
              sys.stdout.flush()
              
              with buffer_lock:
                  log_buffer.append(line)

          # Shutdown
          running = False
          t.join(timeout=5)
          EOF

          # 3) Setup Docker volumes and permissions
          VOL_NAME="openhands-vol-${GITHUB_RUN_ID}"
          STATE_VOL_NAME="openhands-state-${GITHUB_RUN_ID}"
          docker volume create "$VOL_NAME"
          docker volume create "$STATE_VOL_NAME"

          docker run --rm \
            -v "$VOL_NAME:/workspace" \
            -v "$STATE_VOL_NAME:/workspace/.openhands" \
            -e TAVILY_API_KEY="$TAVILY_API_KEY" \
            alpine sh -c '
              set -eux
              
              # 1. Garante que os diretórios existem
              mkdir -p /workspace/.openhands

              # 2. Cria o config.toml
              printf "[core]\nsearch_api_key = \"%s\"\n" "$TAVILY_API_KEY" > /workspace/config.toml

              # 3. Limpeza preventiva
              rm -f /workspace/.openhands/.jwt_secret || true

              # 4. A CORREÇÃO: Permissão total recursiva
              # Em vez de chown 1001:1001 (que pode estar errado), liberamos tudo.
              # Isso permite que o container OpenHands leia/escreva independentemente do usuário que ele usa.
              chmod -R 777 /workspace
            '
          docker rm -f openhands-app 2>/dev/null || true

          # 4) Run Docker piped to Python
          docker_args=(
            run --pull=always
            -w /workspace
            -v /var/run/docker.sock:/var/run/docker.sock
            -v "$VOL_NAME:/workspace:rw"
            -v "$STATE_VOL_NAME:/workspace/.openhands"
            --add-host host.docker.internal:host-gateway
            -e "SANDBOX_USER_ID=$(id -u)"
            -e "SANDBOX_VOLUMES=$VOL_NAME:/workspace:rw"
            -e "SANDBOX_RUNTIME_CONTAINER_IMAGE=docker.openhands.dev/openhands/runtime:1.0-nikolaik"
            -e "LOG_ALL_EVENTS=true"
            -e "LLM_API_KEY=$LLM_API_KEY"
            -e "LLM_MODEL=gemini/gemini-3-pro-preview"
            -e "TAVILY_API_KEY=$TAVILY_API_KEY"
            -e "PYTHONUNBUFFERED=1"
            -e "PROMPT=$PROMPT"
            -e "FILE_STORE=local"
            -e "FILE_STORE_PATH=/workspace/.openhands"
            --name openhands-app
            docker.openhands.dev/openhands/openhands:1.0
            python -m openhands.core.main -t "$PROMPT"
          )

          # Pipe to python script. Secrets are now in the step env.
          docker "${docker_args[@]}" 2>&1 | python3 logger.py

          EXIT_CODE=${PIPESTATUS[0]}
          echo "Docker run finished with exit code $EXIT_CODE"

          echo "Copying artifacts from container to host..."
          docker cp openhands-app:/workspace/output/${SLUG}/. "output/${SLUG}/" || echo "Failed to copy artifacts or directory empty"

          docker rm -f openhands-app || true
          docker volume rm "$VOL_NAME" || true
          docker volume rm "$STATE_VOL_NAME" || true

          exit $EXIT_CODE

      - name: Upload Search Results
        uses: actions/upload-artifact@v4
        with:
          name: deep-search-artifact
          path: output/${{ github.event.client_payload.slug || inputs.slug }}

      - name: Push to Hugging Face Dataset
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          # Configure Git
          git config --global user.email "bot@maia.api"
          git config --global user.name "Maia Bot"

          # Clone HF Repo (using depth 1 for speed)
          git clone --depth 1 https://user:$HF_TOKEN@huggingface.co/datasets/toquereflexo/maia-deep-search hf_repo

          # Prepare Directories
          slug="${{ github.event.client_payload.slug || inputs.slug }}"
          mkdir -p hf_repo/output/$slug

          # Copy Artifacts
          cp -r output/$slug/* hf_repo/output/$slug/

          # Commit and Push
          cd hf_repo

          # Setup LFS for binaries
          git lfs install
          git lfs track "*.pdf" "*.zip" "*.rar" "*.doc" "*.docx"
          git add .gitattributes

          git add .
          git commit -m "Add deep search results for $slug" || echo "No changes to commit"
          git push

      - name: Update Semantic Cache
        if: success()
        env:
          WORKER_URL: https://maia-api-worker.willian-campos-ismart.workers.dev
          GH_PAT: ${{ secrets.GH_PAT }} # Using GH_PAT matching the Worker's expected secret
          QUERY: ${{ github.event.client_payload.query || inputs.query }}
          SLUG: ${{ github.event.client_payload.slug || inputs.slug }}
        run: |
          echo "Updating Semantic Cache at $WORKER_URL..."

          MANIFEST_PATH="output/$SLUG/manifest.json"

          if [ -f "$MANIFEST_PATH" ]; then
            # Extract relevant metadata (institution, year, etc) if possible, 
            # or just send the whole manifest as 'metadata.manifest' or similar.
            # The worker expects 'metadata' object. We can send the manifest content as part of it.
            # Ideally we want flat metadata for filtering: institution, year. 
            # For now, let's send the whole manifest as metadata (or a subset if it's too big).
            # Pinecone metadata limit is 40KB. Manifest might be large.
            # Let's just send basic info + file count.
            
            # Using jq to create a safe JSON payload
            PAYLOAD=$(jq -n \
                    --arg query "$QUERY" \
                    --arg slug "$SLUG" \
                    --slurpfile manifest "$MANIFEST_PATH" \
                    '{
                      query: $query, 
                      slug: $slug, 
                      metadata: {
                        source: "deep-search",
                        file_count: ($manifest[0] | length),
                        institution: ($manifest[0][0].instituicao // $manifest[0][0].institution // "unknown"),
                        year: ($manifest[0][0].ano // $manifest[0][0].year // "unknown")
                      }
                    }')
            
            
            # Curl Request with -f (fail on error) and capturing output
            RESPONSE=$(curl -s -w "\nHTTP_CODE:%{http_code}" -X POST "$WORKER_URL/update-deep-search-cache" \
                 -H "Content-Type: application/json" \
                 -H "Authorization: Bearer $GH_PAT" \
                 -d "$PAYLOAD")
            
            HTTP_CODE=$(echo "$RESPONSE" | tail -n1 | cut -d: -f2)
            BODY=$(echo "$RESPONSE" | sed '$d')

            echo "Worker Response Body: $BODY"
            echo "Worker HTTP Code: $HTTP_CODE"

            if [ "$HTTP_CODE" -ne 200 ]; then
              echo "::error::Failed to update cache. HTTP $HTTP_CODE"
              exit 1
            fi
          else
            echo "Manifest not found at $MANIFEST_PATH. Skipping cache update."
          fi

      - name: Notify Completion
        if: ${{ always() }}
        env:
          PUSHER_APP_ID: ${{ secrets.PUSHER_APP_ID }}
          PUSHER_KEY: ${{ secrets.PUSHER_KEY }}
          PUSHER_SECRET: ${{ secrets.PUSHER_SECRET }}
          PUSHER_CLUSTER: ${{ secrets.PUSHER_CLUSTER }}
          SLUG: ${{ github.event.client_payload.slug || inputs.slug }}
        run: |
          LOG_URL="${GITHUB_SERVER_URL}/${GITHUB_REPOSITORY}/actions/runs/${GITHUB_RUN_ID}"
          MESSAGE="COMPLETED. Logs: $LOG_URL"

          # Simple Pusher Trigger for Completion
          TS=$(date +%s)
          BODY="{\"name\":\"log\",\"channels\":[\"$SLUG\"],\"data\":\"{\\\"message\\\":\\\"$MESSAGE\\\"}\"}"
          BODY_MD5=$(echo -n "$BODY" | md5sum | awk '{print $1}')
          SIGN_STRING="POST\n/apps/$PUSHER_APP_ID/events\nauth_key=$PUSHER_KEY&auth_timestamp=$TS&auth_version=1.0&body_md5=$BODY_MD5"
          AUTH_SIGNATURE=$(echo -n -e "$SIGN_STRING" | openssl dgst -sha256 -hmac "$PUSHER_SECRET" | sed 's/^.* //')

          curl -s -X POST "https://api-$PUSHER_CLUSTER.pusher.com/apps/$PUSHER_APP_ID/events?auth_key=$PUSHER_KEY&auth_timestamp=$TS&auth_version=1.0&body_md5=$BODY_MD5&auth_signature=$AUTH_SIGNATURE" \
                 -H "Content-Type: application/json" \
                 -d "$BODY"
