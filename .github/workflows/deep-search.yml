name: Deep Search (OpenHands)

on:
  repository_dispatch:
    types: [deep-search]
  workflow_dispatch:
    inputs:
      query:
        description: 'Search Query (e.g. "ita 2022")'
        required: true
      slug:
        description: 'Slug for folder name (e.g. "ita-2022")'
        required: true
      ntfy_topic:
        description: "ntfy.sh topic for log streaming"
        required: false

jobs:
  search:
    runs-on: ubuntu-latest
    timeout-minutes: 25
    permissions:
      contents: write

    # Global Environment Variables (Available to all steps)
    env:
      LLM_API_KEY: ${{ secrets.LLM_API_KEY }}
      TAVILY_API_KEY: ${{ secrets.TAVILY_API_KEY }}
      PUSHER_APP_ID: ${{ secrets.PUSHER_APP_ID }}
      PUSHER_KEY: ${{ secrets.PUSHER_KEY }}
      PUSHER_SECRET: ${{ secrets.PUSHER_SECRET }}
      PUSHER_CLUSTER: ${{ secrets.PUSHER_CLUSTER }}
      NTFY_TOPIC: ${{ github.event.client_payload.ntfy_topic || inputs.ntfy_topic }}
      QUERY: ${{ github.event.client_payload.query || inputs.query }}
      SLUG: ${{ github.event.client_payload.slug || inputs.slug }}

    steps:
      - name: Checkout Repo
        uses: actions/checkout@v4

      - name: Setup Logger
        run: |
          cat << 'EOF' > logger.py
          import sys
          import time
          import json
          import hashlib
          import hmac
          import os
          import threading
          import http.client
          import urllib.parse
          import subprocess
          import re
          from collections import deque

          # --- CONFIG ---
          PUSHER_APP_ID = os.environ.get("PUSHER_APP_ID")
          PUSHER_KEY = os.environ.get("PUSHER_KEY")
          PUSHER_SECRET = os.environ.get("PUSHER_SECRET")
          PUSHER_CLUSTER = os.environ.get("PUSHER_CLUSTER")
          CHANNEL = os.environ.get("SLUG")

          if not all([PUSHER_APP_ID, PUSHER_KEY, PUSHER_SECRET, PUSHER_CLUSTER, CHANNEL]):
              print("[Logger] Missing Pusher secrets. Logging to stdout only.")
              PUSHER_ENABLED = False
          else:
              PUSHER_ENABLED = True

          # Global State
          log_buffer = deque()
          buffer_lock = threading.Lock()
          running = True

          print("[Logger] Script initialized.", flush=True)

          # --- PUSHER HELPER ---
          def send_to_pusher(event_name, data_payload):
              if not PUSHER_ENABLED:
                  return

              timestamp = str(int(time.time()))
              body_data = json.dumps(data_payload)
              body = json.dumps({
                  "name": event_name,
                  "channels": [CHANNEL],
                  "data": body_data
              })
              
              body_md5 = hashlib.md5(body.encode('utf-8')).hexdigest()
              
              sign_string = f"POST\n/apps/{PUSHER_APP_ID}/events\nauth_key={PUSHER_KEY}&auth_timestamp={timestamp}&auth_version=1.0&body_md5={body_md5}"
              auth_signature = hmac.new(PUSHER_SECRET.encode('utf-8'), sign_string.encode('utf-8'), hashlib.sha256).hexdigest()
              
              params = urllib.parse.urlencode({
                  'auth_key': PUSHER_KEY,
                  'auth_timestamp': timestamp,
                  'auth_version': '1.0',
                  'body_md5': body_md5,
                  'auth_signature': auth_signature
              })
              
              try:
                  conn = http.client.HTTPSConnection(f"api-{PUSHER_CLUSTER}.pusher.com", timeout=3)
                  headers = {'Content-Type': 'application/json'}
                  conn.request("POST", f"/apps/{PUSHER_APP_ID}/events?{params}", body, headers)
                  resp = conn.getresponse()
                  resp.read() # Consume response
                  conn.close()
              except Exception as e:
                  print(f"[Logger] Pusher Error: {e}")

          # --- WORKER: LOG BATCHER ---
          def log_worker():
              while running or log_buffer:
                  with buffer_lock:
                      if not log_buffer:
                          batch = []
                      else:
                          batch = list(log_buffer)
                          log_buffer.clear()
                  
                  if batch:
                      # Join lines and broadcast
                      text_chunk = "".join(batch)
                      send_to_pusher("log", {"message": text_chunk})
                  
                  time.sleep(1.0) # Batch window

          # --- WORKER: TASK ACTIVE POLLER ---
          # This thread actively asks Docker for the TASKS.md file
          # It is much more robust than parsing log lines.
          def task_watcher_worker():
              print("[Logger] Task watcher thread started. Polling for TASKS.md...", flush=True)
              last_known_tasks_hash = ""
              container_name = "openhands-app"
              tasks_path = None # Will find dynamically
              
              # Wait slightly for container, but don't block visibility
              time.sleep(2)

              while running:
                  time.sleep(2.0) # Poll every 2s

                  try:
                      # 1. Locate TASKS.md if we haven't yet
                      if not tasks_path:
                          # "find /workspace -name TASKS.md | head -n 1"
                          try:
                              find_cmd = ["docker", "exec", container_name, "sh", "-c", "find /workspace -name TASKS.md 2>/dev/null | head -n 1"]
                              res = subprocess.run(find_cmd, capture_output=True, text=True)
                              if res.returncode == 0 and res.stdout.strip():
                                  tasks_path = res.stdout.strip()
                                  print(f"[Logger] Found active task file at: {tasks_path}", flush=True)
                          except:
                              pass # Container might not be up yet
                      
                      # 2. Read File if we found it
                      if tasks_path:
                          read_cmd = ["docker", "exec", container_name, "cat", tasks_path]
                          res = subprocess.run(read_cmd, capture_output=True, text=True)
                          
                          if res.returncode == 0:
                              content = res.stdout
                              # Debug: Show we got content
                              # print(f"[Logger] Read {len(content)} bytes from TASKS.md", flush=True)

                              # 3. Hash Check
                              current_hash = hashlib.md5(content.encode()).hexdigest()
                              if current_hash != last_known_tasks_hash:
                                  # Debug: Print RAW content (User Request)
                                  print(f"[DEBUG RAW TASKS.md]\n{content}\n[END RAW]", flush=True)

                                  # It changed! Parse it.
                                  parsed_tasks = parse_tasks_md(content)
                                  
                                  if parsed_tasks:
                                      print(f"[Logger] Detected task update ({len(parsed_tasks)} items). Broadcasting...", flush=True)
                                      
                                      # PRINT TASKS TO CONSOLE (User Request)
                                      print("--- CURRENT TASKS ---", flush=True)
                                      for t in parsed_tasks:
                                          status_icon = "[ ]"
                                          if t['status'] == "completed": status_icon = "[x]"
                                          elif t['status'] == "in_progress": status_icon = "[/]"
                                          
                                          # Format: [ ] Title | Notes: Description...
                                          note_str = f" | Notes: {t['notes']}" if t.get('notes') else ""
                                          print(f"{status_icon} {t['title']}{note_str}", flush=True)
                                      print("---------------------", flush=True)
                                      
                                      send_to_pusher("task_update", parsed_tasks)
                                      last_known_tasks_hash = current_hash
                                  else:
                                      # Debug: why no tasks?
                                      print(f"[Logger] Content changed but parsed 0 tasks. Content sample: {content[:100]!r}", flush=True)
                                      
                  except Exception as e:
                      # Don't crash the logger if docker exec fails randomly
                      print(f"[Logger] Task watcher error: {e}", flush=True)
                      pass

          # Helper: Parse TASKS.md content (Supports Markdown Checklists & OpenHands Emojis & Notes)
          def parse_tasks_md(content):
              tasks = []
              lines = content.split('\n')
              
              current_task = None
              
              for line in lines:
                  line = line.strip()
                  if not line: continue

                  # 1. Try Markdown Checklist: - [ ] Task Name
                  match_md = re.match(r'- \[([ xX/])\] (.*)', line)
                  
                  # 2. Try Numbered List with Emojis: 1. ‚è≥ Task Name
                  match_num = re.match(r'\d+\.\s+(?:([^\w\s]+)\s+)?(.*)', line)
                  
                  if match_md:
                      if current_task: tasks.append(current_task)
                      
                      status_char = match_md.group(1).lower()
                      title = re.sub(r'<!--.*-->', '', match_md.group(2)).strip()
                      
                      status = "todo"
                      if status_char == 'x': status = "completed"
                      elif status_char == '/': status = "in_progress"
                      
                      current_task = {"id": len(tasks), "title": title, "status": status, "notes": ""}
                  
                  elif match_num:
                      if current_task: tasks.append(current_task)
                      
                      icon = match_num.group(1) or ""
                      title = re.sub(r'<!--.*-->', '', match_num.group(2)).strip()
                      
                      status = "todo"
                      if icon:
                          if any(c in icon for c in ['‚úÖ', '‚úî', '‚òë']): status = "completed"
                          elif any(c in icon for c in ['‚è≥', '‚ñ∂', 'üèÉ', 'üöß']): status = "in_progress"
                          # ‚è≥ usually means "planned/waiting", so keeping it as 'todo' unless explicitly active?
                          # User requested: "indicar / como todo".
                          # Let's map ‚è≥ to "todo" if that's the intention, OR "in_progress".
                          # OpenHands standard: ‚è≥ = In Progress / Working on it.
                          # But user asked: "indicar / como todo".
                          # If I output "[ ]" for ‚è≥, frontend shows [ ] (Todo).
                          if any(c in icon for c in ['‚è≥']): status = "todo" # Explicit User Request override?
                          elif any(c in icon for c in ['‚ñ∂', 'üèÉ']): status = "in_progress"
                          elif any(c in icon for c in ['‚ùå', 'üö´']): status = "failed"
                      
                      current_task = {"id": len(tasks), "title": title, "status": status, "notes": ""}
                  
                  else:
                      # It's a note line if we have a current task
                      if current_task:
                           # Append to notes
                           if current_task["notes"]:
                               current_task["notes"] += " " + line
                           else:
                               current_task["notes"] = line
              
              if current_task: tasks.append(current_task)
              return tasks

          # --- MAIN THREADS ---
          t_log = threading.Thread(target=log_worker, daemon=True)
          t_log.start()

          t_task = threading.Thread(target=task_watcher_worker, daemon=True)
          t_task.start()

          # --- PRODUCER (STDIN) ---
          try:
              for line in sys.stdin:
                  # 1. Print to stdout (Critical for GitHub UI)
                  sys.stdout.write(line)
                  sys.stdout.flush()
                  
                  # 2. Add to Pusher Buffer
                  with buffer_lock:
                      log_buffer.append(line)
          except KeyboardInterrupt:
              pass
          finally:
              # Shutdown
              running = False
              t_log.join(timeout=3)
              t_task.join(timeout=1)
          EOF

      - name: Free Disk Space
        run: |
          (
            echo "Disk space before cleanup:"
            df -h
            sudo rm -rf /usr/local/lib/android
            sudo rm -rf /usr/share/dotnet
            sudo rm -rf /opt/ghc
            sudo rm -rf /usr/local/.ghcup
            echo "Disk space after cleanup:"
            df -h
          ) 2>&1 | python3 -u logger.py

      - name: Create Output Directory
        run: |
          (
             mkdir -p output/${{ github.event.client_payload.slug || inputs.slug }}
             echo "Created output directory: output/${{ github.event.client_payload.slug || inputs.slug }}"
          ) 2>&1 | python3 -u logger.py

      - name: Pre-execution Cleanup (HF)
        if: ${{ github.event.client_payload.cleanup == true }}
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          SLUG: ${{ github.event.client_payload.slug || inputs.slug }}
        run: |
          (
            echo "Cleanup requested. Removing existing Hugging Face data for $SLUG..."
            
            # Configure Git
            git config --global user.email "bot@maia.api"
            git config --global user.name "Maia Bot"

            # Clone Repo
            git clone --depth 1 https://user:$HF_TOKEN@huggingface.co/datasets/toquereflexo/maia-deep-search hf_cleanup_repo
            cd hf_cleanup_repo

            # Check and delete
            if [ -d "output/$SLUG" ]; then
                echo "Removing output/$SLUG..."
                git rm -rf "output/$SLUG"
                git commit -m "Cleanup (Retry) for $SLUG"
                git push
                echo "Cleanup committed."
            else
                echo "No existing data found at output/$SLUG."
            fi
            cd ..
            rm -rf hf_cleanup_repo
          ) 2>&1 | python3 -u logger.py

      - name: Pre-execution Cleanup (Pinecone)
        if: ${{ github.event.client_payload.cleanup == true }}
        env:
          WORKER_URL: https://maia-api-worker.willian-campos-ismart.workers.dev
          GH_PAT: ${{ secrets.GH_PAT }}
          SLUG: ${{ github.event.client_payload.slug || inputs.slug }}
        run: |
          (
             echo "Requesting Pinecone deletion for $SLUG..."
             curl -X POST "$WORKER_URL/delete-pinecone-record" \
                  -H "Content-Type: application/json" \
                  -H "Authorization: Bearer $GH_PAT" \
                  -d "{\"slug\": \"$SLUG\"}" \
             || echo "Warning: Failed to call delete endpoint"
          ) 2>&1 | python3 -u logger.py

      - name: Announce Job URL
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          RUN_ID: ${{ github.run_id }}
          REPO: ${{ github.repository }}
        run: |
          (
            # Fetch the Jobs for this Run
            JOBS_JSON=$(curl -s -H "Authorization: Bearer $GH_TOKEN" \
                        "https://api.github.com/repos/$REPO/actions/runs/$RUN_ID/jobs")
            
            # Extract the HTML URL of the current job
            JOB_URL=$(echo "$JOBS_JSON" | jq -r '.jobs[0].html_url')
            
            if [ "$JOB_URL" == "null" ] || [ -z "$JOB_URL" ]; then
                # Fallback to Run URL if API fails
                JOB_URL="${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
            fi

            echo "[SYSTEM_INFO] JOB_URL=$JOB_URL"
          ) 2>&1 | python3 -u logger.py

      - name: Run OpenHands Deep Search
        id: deep_search
        run: |
          set -euo pipefail

          # 1) Heredoc sem interpreta√ß√£o (n√£o executa backticks)
          PROMPT_TEMPLATE=$(cat <<'EOF'
          Voc√™ recebeu uma consulta curta (QUERY) do tipo "nome da prova + ano", por exemplo: "ita 2022".

          Objetivo: encontrar e organizar TODOS os links e arquivos (provas e gabaritos) associados a essa QUERY.

          QUERY: "__QUERY__"

          Regras para evitar pesquisa demais (muito importante):
          1) N√£o use Google como primeira op√ß√£o (CAPTCHA √© comum). Prefira:
            - Sites oficiais da banca/institui√ß√£o (dom√≠nio oficial).
            - P√°ginas ‚ÄúProvas anteriores / provas e gabaritos / exames anteriores / vestibular / arquivos‚Äù.
          2) Fa√ßa no m√°ximo 3 buscas no total (usando o mecanismo de busca configurado no OpenHands). Use buscas bem espec√≠ficas.
          3) Ap√≥s achar o site oficial, navegue dentro dele e extraia tudo com o m√≠nimo de novas buscas.
          4) S√≥ inclua links que pare√ßam realmente do exame/ano correspondente; quando estiver amb√≠guo, checar rapidamente pelo t√≠tulo/URL/nome do PDF.

          O que coletar:
          - Provas (todas as fases/dias/disciplinas/vers√µes que existirem).
          - Gabaritos correspondentes (e ‚Äúsolu√ß√µes‚Äù, se houver).
          - INDISPENS√ÅVEL: Classifique cada item no manifesto:
            - status: "downloaded" (se baixou com sucesso e validou) OU "reference" (se √© apenas um link externo que n√£o foi poss√≠vel baixar ou n√£o √© PDF).
          - Estrutura do JSON: {nome, tipo, ano, instituicao, fase, link_origem, status, filename (s√≥ se status=downloaded)}

          Prefer√™ncia de links:
          - Priorize links diretos de PDF.
          - Se for Google Drive, capture:
            - link "view"
            - e tamb√©m um link ‚Äúdownload direto‚Äù quando poss√≠vel (uc?export=download&id=...).

          Entrega (obrigat√≥rio):
          1) Crie a pasta: /workspace/output/__SLUG__/
          2) Baixe todos os PDFs que conseguir para: /workspace/output/__SLUG__/files/.
            CRITICO:
            - USE 'wget' ou 'curl' com verifica√ß√£o de header.
            - VERIFIQUE SE O HEADER 'Content-Type' √â 'application/pdf'.
            - SE O ARQUIVO BAIXADO TIVER MENOS DE 1KB, APAGUE-O IMEDIATAMENTE e mude o status para "reference".
            - SE O ARQUIVO CONTIVER HTML (DOCTYPE, <html>), APAGUE-O e mude o status para "reference".
            - N√ÉO CRIE ARQUIVOS FAKES OU COM CONTE√öDO GERADO. S√ì MANTENHA O QUE FOI BAIXADO DE VERDADE.
          3) Gere /workspace/output/__SLUG__/index.md
          4) Gere /workspace/output/__SLUG__/manifest.json (ESTE ARQUIVO √â CRITICO, DEVE SER JSON V√ÅLIDO. INCLUA ITENS "reference" E "downloaded")
          5) Gere /workspace/output/__SLUG__/DOWNLOAD_URLS.txt
          6) Gere /workspace/output/__SLUG__/slug.txt
             - IMPORTANTE: Escreva neste arquivo APENAS um nome curto e limpo para a pasta final (kebab-case).
             - Exemplo: se a query for 'provas do enem 2025', escreva 'enem-2025'.
             - Exemplo: se for 'unicamp 2024 segunda fase', escreva 'unicamp-2024-2f'.
             - Esse nome ser√° usado para renomear a pasta e salvar no banco de dados.
          7) Gere /workspace/output/__SLUG__.zip
          8) (Opcional) Gere /workspace/output/__SLUG__/index.pdf

          Ao final, responda apenas com caminhos e contagens.
          EOF
          )

          # Replace Placeholders
          PROMPT="${PROMPT_TEMPLATE/__QUERY__/$QUERY}"
          PROMPT="${PROMPT//__SLUG__/$SLUG}"

          # 3) Setup Docker volumes and permissions
          VOL_NAME="openhands-vol-${GITHUB_RUN_ID}"
          STATE_VOL_NAME="openhands-state-${GITHUB_RUN_ID}"
          docker volume create "$VOL_NAME"
          docker volume create "$STATE_VOL_NAME"

          docker run --rm \
            -v "$VOL_NAME:/workspace" \
            -v "$STATE_VOL_NAME:/workspace/.openhands" \
            -e TAVILY_API_KEY="$TAVILY_API_KEY" \
            alpine sh -c '
              set -eux
              mkdir -p /workspace/.openhands
              printf "[core]\nsearch_api_key = \"%s\"\n" "$TAVILY_API_KEY" > /workspace/config.toml
              rm -f /workspace/.openhands/.jwt_secret || true
              chmod -R 777 /workspace
            '
          docker rm -f openhands-app 2>/dev/null || true

          # 4) Run Docker piped to Python
          docker_args=(
            run --pull=always
            -w /workspace
            -v /var/run/docker.sock:/var/run/docker.sock
            -v "$VOL_NAME:/workspace:rw"
            -v "$STATE_VOL_NAME:/workspace/.openhands"
            --add-host host.docker.internal:host-gateway
            -e "SANDBOX_USER_ID=$(id -u)"
            -e "SANDBOX_VOLUMES=$VOL_NAME:/workspace:rw"
            -e "SANDBOX_RUNTIME_CONTAINER_IMAGE=docker.openhands.dev/openhands/runtime:1.0-nikolaik"
            -e "LOG_ALL_EVENTS=true"
            -e "LLM_API_KEY=$LLM_API_KEY"
            -e "LLM_MODEL=gemini/gemini-3-flash-preview"
            -e "TAVILY_API_KEY=$TAVILY_API_KEY"
            -e "PYTHONUNBUFFERED=1"
            -e "PROMPT=$PROMPT"
            -e "FILE_STORE=local"
            -e "FILE_STORE_PATH=/workspace/.openhands"
            --name openhands-app
            docker.openhands.dev/openhands/openhands:1.0
            python -m openhands.core.main -t "$PROMPT"
          )

          # Pipe to python script. Secrets are now in the step env.
          docker "${docker_args[@]}" 2>&1 | python3 logger.py

          EXIT_CODE=${PIPESTATUS[0]}
          echo "Docker run finished with exit code $EXIT_CODE"

          echo "Copying artifacts from container to host..."
          docker cp openhands-app:/workspace/output/${SLUG}/. "output/${SLUG}/" || echo "Failed to copy artifacts or directory empty"

          # --- DYNAMIC RENAMING LOGIC ---
          FINAL_SLUG="$SLUG"
          if [ -f "output/${SLUG}/slug.txt" ]; then
             CANDIDATE_SLUG=$(cat "output/${SLUG}/slug.txt" | tr -cd '[:alnum:]-')
             if [ -n "$CANDIDATE_SLUG" ] && [ "$CANDIDATE_SLUG" != "$SLUG" ]; then
                 echo "Renaming artifact from $SLUG to $CANDIDATE_SLUG..."
                 mv "output/${SLUG}" "output/${CANDIDATE_SLUG}"
                 FINAL_SLUG="$CANDIDATE_SLUG"
             fi
          fi
          echo "final_slug=$FINAL_SLUG" >> $GITHUB_OUTPUT
          # ------------------------------

          docker rm -f openhands-app || true
          docker volume rm "$VOL_NAME" || true
          docker volume rm "$STATE_VOL_NAME" || true

          exit $EXIT_CODE

      - name: Upload Search Results
        uses: actions/upload-artifact@v4
        with:
          name: deep-search-artifact
          path: output/${{ steps.deep_search.outputs.final_slug }}

      - name: Push to Hugging Face Dataset (Smart Merge)
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          MODE: ${{ github.event.client_payload.mode }}
        run: |
          (
            echo "Starting upload to Hugging Face (Mode: $MODE)..."
            # Configure Git
            git config --global user.email "bot@maia.api"
            git config --global user.name "Maia Bot"

            # Clone HF Repo (using depth 1 for speed)
            git clone --depth 1 https://user:$HF_TOKEN@huggingface.co/datasets/toquereflexo/maia-deep-search hf_repo
            
            slug="${{ steps.deep_search.outputs.final_slug }}"
            echo "Processing slug: $slug"
            
            # --- MANIFEST MERGING LOGIC ---
            if [ "$MODE" == "update" ]; then
                 echo "Update Mode: Attempting to merge manifest.json..."
                 
                 EXISTING_MANIFEST="hf_repo/output/$slug/manifest.json"
                 NEW_MANIFEST="output/$slug/manifest.json"
                 MERGED_MANIFEST="output/$slug/manifest_merged.json"
                 
                 # Debug: Check existence
                 ls -l "hf_repo/output/$slug/" || echo "Directory not found in HF repo"

                 if [ -f "$EXISTING_MANIFEST" ] && [ -f "$NEW_MANIFEST" ]; then
                      echo "Found existing manifest. merging..."
                      
                      # Robust Merge Strategy:
                      # 1. Normalize both inputs to Arrays (handle Arrays, Single Objects, or Objects with 'results'/'files')
                      # 2. Add them together
                      # 3. Unique by filename or url
                      
                      jq -s 'map(
                        if type=="array" then . 
                        elif type=="object" and has("results") then .results
                        elif type=="object" and has("files") then .files
                        else [.] 
                        end
                      ) | add | unique_by(.filename // .url)' "$EXISTING_MANIFEST" "$NEW_MANIFEST" > "$MERGED_MANIFEST"
                      
                      if [ -s "$MERGED_MANIFEST" ]; then
                          echo "Merge successful. Overwriting new manifest with merged version."
                          mv "$MERGED_MANIFEST" "output/$slug/manifest.json"
                          
                          # Copy Logic: We want to ensure we don't DELETE old files in the repo 
                          # git add will add new ones, but we should probably copy old ones locally first?
                          # Actually, we are copying output/$slug -> hf_repo. 
                          # If we only copy new files, and the merged manifest lists old files, 
                          # we need to make sure the old files are still there in hf_repo.
                          # Since we cloned hf_repo, they ARE there. We just copy the new stuff ON TOP.
                      else
                          echo "‚ö†Ô∏è Merge produced empty file! Keeping NEW manifest only (Safety fallback)."
                          # Optionally: cp "$EXISTING_MANIFEST" "output/$slug/manifest_backup.json"
                      fi
                 else
                      echo "Existing manifest not found or new manifest missing. Skipping merge."
                 fi
            fi
            # ------------------------------

            # Prepare Directories
            mkdir -p hf_repo/output/$slug

            # Copy Artifacts
            # We copy FROM generated output TO the cloned repo
            # This adds new files and updates            # Copy Artifacts
            cp -r "output/$slug"/* "hf_repo/output/$slug/"

            # --- DEDUPLICATION LOGIC ---
            echo "Running Deduplication on hf_repo/output/$slug..."
            cat << 'EOF' | sed 's/^  //' > deduplicate.py
            import os
            import sys
            import hashlib
            import json
            import re
            import shutil
            import subprocess
            from datetime import datetime

            def get_file_info(filepath):
                try:
                    with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
                        header = f.read(200)
                        match = re.search(r'oid sha256:([a-f0-9]{64})', header)
                        if match:
                            return (match.group(1), True)
                except Exception:
                    pass
                
                h = hashlib.sha256()
                try:
                    with open(filepath, 'rb') as f:
                        while chunk := f.read(8192):
                            h.update(chunk)
                    return (h.hexdigest(), False)
                except Exception as e:
                    print(f"Error hashing {filepath}: {e}")
                    return (None, False)

            def is_git_tracked(filepath, cwd):
                """Checks if a file is already tracked by git (committed)."""
                try:
                    dirname = os.path.dirname(filepath)
                    basename = os.path.basename(filepath)
                    res = subprocess.run(
                        ['git', 'ls-files', basename], 
                        cwd=dirname, 
                        capture_output=True, 
                        text=True,
                        check=False
                    )
                    return bool(res.stdout.strip())
                except Exception as e:
                    print(f"Git check failed for {filepath}: {e}")
                    return False

            def load_manifest(manifest_path):
                if not os.path.exists(manifest_path):
                    return []
                try:
                    with open(manifest_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    if isinstance(data, list): return data
                    if isinstance(data, dict):
                        if 'results' in data: return data['results']
                        if 'files' in data: return data['files']
                    return []
                except Exception as e:
                    print(f"Error loading manifest: {e}")
                    return []

            def save_manifest(manifest_path, entries):
                try:
                    with open(manifest_path, 'w', encoding='utf-8') as f:
                        json.dump(entries, f, indent=2, ensure_ascii=False)
                    print("Manifest updated.")
                except Exception as e:
                    print(f"Error saving manifest: {e}")

            def deduplicate_and_fix_manifest(target_dir):
                print(f"Scanning {target_dir}...")
                
                manifest_path = os.path.join(target_dir, "manifest.json")
                manifest_entries = load_manifest(manifest_path)
                entries_by_filename = {e.get('filename'): e for e in manifest_entries if e.get('filename')}

                files_by_hash = {}
                
                # Scan files and gather info (Hash, LFS, Git Tracked)
                for root, _, files in os.walk(target_dir):
                    for filename in files:
                        if filename in ["manifest.json", "index.md", "slug.txt", "DOWNLOAD_URLS.txt"]: continue
                        if filename.endswith(".json") or filename.endswith(".txt") or filename.endswith(".md"): continue 

                        filepath = os.path.join(root, filename)
                        file_hash, is_lfs = get_file_info(filepath)
                        
                        if file_hash:
                            if file_hash not in files_by_hash:
                                files_by_hash[file_hash] = []
                            files_by_hash[file_hash].append({
                                'path': filepath,
                                'filename': filename,
                                'is_lfs': is_lfs,
                                'in_manifest': filename in entries_by_filename,
                                'is_tracked': is_git_tracked(filepath, root)
                            })

                manifest_changed = False
                files_removed_count = 0
                
                # --- DEDUPLICATION ---
                for fhash, items in files_by_hash.items():
                    if len(items) > 1:
                        # Priority Rules:
                        # 1. Git Tracked (Old/Existing files) - HIGHEST PRIORITY
                        # 2. In Manifest (Valid Metadata)
                        # 3. Real Binary (Not LFS)
                        # 4. Shortest Name
                        
                        items.sort(key=lambda x: (
                            not x['is_tracked'],  # False (Tracked) < True
                            not x['in_manifest'], # False (In Manifest) < True
                            x['is_lfs'],          # False (Binary) < True
                            len(x['filename']),   # Shortest
                            x['filename']         # Alphabetical tie-breaker
                        ))
                        
                        keep_item = items[0]
                        remove_items = items[1:]
                        
                        print(f"Duplicate group ({fhash[:8]})...")
                        print(f"  Keeping: {keep_item['filename']} [Tracked={keep_item['is_tracked']}, Manifest={keep_item['in_manifest']}, LFS={keep_item['is_lfs']}]")
                        
                        # LFS Repair: If keeping a pointer but deleting a binary, overwrite logic.
                        # Only overwrite if the keeper is an LFS pointer and we have a binary source.
                        binary_candidate = next((item for item in [keep_item] + remove_items if not item['is_lfs']), None)
                        
                        if keep_item['is_lfs'] and binary_candidate and binary_candidate != keep_item:
                             print(f"  [LFS Repair] Overwriting pointer '{keep_item['filename']}' with binary content from '{binary_candidate['filename']}'")
                             try:
                                shutil.copy2(binary_candidate['path'], keep_item['path'])
                             except Exception as e:
                                print(f"  Error overwriting LFS: {e}")
                        
                        # Delete others
                        for rm in remove_items:
                            print(f"  Deleting: {rm['filename']}")
                            try:
                                if os.path.exists(rm['path']):
                                    os.remove(rm['path'])
                                    files_removed_count += 1
                            except Exception as e:
                                print(f"  Failed to delete {rm['filename']}: {e}")
                            
                            # Clean Manifest
                            if rm['filename'] in entries_by_filename:
                                print(f"  Removing manifest entry for deleted file: {rm['filename']}")
                                del entries_by_filename[rm['filename']]
                                manifest_changed = True

                # --- ORPHAN DETECTION (New Files) ---
                # Check for files on disk that are NOT in the manifest
                # Re-scan dir to ensure we capture current state
                final_files = []
                for root, _, files in os.walk(target_dir):
                     for filename in files:
                        if filename in ["manifest.json", "index.md", "slug.txt", "DOWNLOAD_URLS.txt"]: continue
                        if filename.endswith(".json") or filename.endswith(".txt") or filename.endswith(".md"): continue
                        final_files.append(filename)
                
                orphans = [f for f in final_files if f not in entries_by_filename]
                
                if orphans:
                    print(f"Found {len(orphans)} orphan files (not in manifest). Adding them...")
                    for orphan in orphans:
                        # Infer simple metadata for orphans
                        new_entry = {
                            "filename": orphan,
                            "name": os.path.splitext(orphan)[0].replace('_', ' ').title(),
                            "type": "recovered",
                            "status": "downloaded",
                            "added_at": datetime.now().isoformat()
                        }
                        entries_by_filename[orphan] = new_entry
                        manifest_changed = True

                if manifest_changed or files_removed_count > 0:
                    final_entries = list(entries_by_filename.values())
                    save_manifest(manifest_path, final_entries)
                else:
                    print("No manifest changes needed.")

            if __name__ == "__main__":
                if len(sys.argv) < 2:
                    print("Usage: deduplicate.py <directory>")
                    sys.exit(1)
                
                target_dir = sys.argv[1]
                if os.path.exists(target_dir):
                    deduplicate_and_fix_manifest(target_dir)
                else:
                    print(f"Target directory {target_dir} not found.")
          EOF
            # ---------------------------

            # Commit and Push
            cd hf_repo

            # Setup LFS for binaries
            git lfs install
            git lfs track "*.pdf" "*.zip" "*.rar" "*.doc" "*.docx"
            git add .gitattributes

            git add .
            git commit -m "Add/Update deep search results for $slug (Mode: $MODE)" || echo "No changes to commit"
            git push
            echo "Upload to Hugging Face completed."
          ) 2>&1 | python3 logger.py

      - name: Update Semantic Cache
        if: success()
        env:
          WORKER_URL: https://maia-api-worker.willian-campos-ismart.workers.dev
          GH_PAT: ${{ secrets.GH_PAT }}
          QUERY: ${{ github.event.client_payload.query || inputs.query }}
          SLUG: ${{ steps.deep_search.outputs.final_slug }}
        run: |
          (
            echo "Updating Semantic Cache at $WORKER_URL..."

            MANIFEST_PATH="output/$SLUG/manifest.json"

            # Generate Clean Query from Slug (replace dashes with spaces)
            CLEAN_QUERY="${SLUG//-/ }"
            echo "Using Clean Query for Embedding: $CLEAN_QUERY"

            if [ -f "$MANIFEST_PATH" ]; then
              
              # Using jq to create a safe JSON payload
              PAYLOAD=$(jq -n \
                      --arg query "$CLEAN_QUERY" \
                      --arg original_query "$QUERY" \
                      --arg slug "$SLUG" \
                      --slurpfile manifest "$MANIFEST_PATH" \
                      '{
                        query: $query, 
                        slug: $slug, 
                        metadata: {
                          source: "deep-search",
                          original_query: $original_query,
                          file_count: ($manifest[0] | length),
                          institution: ($manifest[0][0].instituicao // $manifest[0][0].institution // "unknown"),
                          year: ($manifest[0][0].ano // $manifest[0][0].year // "unknown")
                        }
                      }')
              
              
              # Curl Request with -f (fail on error) and capturing output
              RESPONSE=$(curl -s -w "\nHTTP_CODE:%{http_code}" -X POST "$WORKER_URL/update-deep-search-cache" \
                   -H "Content-Type: application/json" \
                   -H "Authorization: Bearer $GH_PAT" \
                   -d "$PAYLOAD")
              
              HTTP_CODE=$(echo "$RESPONSE" | tail -n1 | cut -d: -f2)
              BODY=$(echo "$RESPONSE" | sed '$d')

              echo "Worker Response Body: $BODY"
              echo "Worker HTTP Code: $HTTP_CODE"

              if [ "$HTTP_CODE" -ne 200 ]; then
                echo "::error::Failed to update cache. HTTP $HTTP_CODE"
                exit 1
              fi
            else
              echo "Manifest not found at $MANIFEST_PATH. Skipping cache update."
            fi
          ) 2>&1 | python3 logger.py

      - name: Notify Completion
        if: ${{ always() }}
        env:
          PUSHER_APP_ID: ${{ secrets.PUSHER_APP_ID }}
          PUSHER_KEY: ${{ secrets.PUSHER_KEY }}
          PUSHER_SECRET: ${{ secrets.PUSHER_SECRET }}
          PUSHER_CLUSTER: ${{ secrets.PUSHER_CLUSTER }}
          SLUG: ${{ github.event.client_payload.slug || inputs.slug }}
          FINAL_SLUG: ${{ steps.deep_search.outputs.final_slug }}
          JOB_STATUS: ${{ job.status }}
        run: |
          LOG_URL="${GITHUB_SERVER_URL}/${GITHUB_REPOSITORY}/actions/runs/${GITHUB_RUN_ID}"

          if [ "$JOB_STATUS" == "success" ]; then
             MESSAGE="COMPLETED. Logs: $LOG_URL"
             EVENT_NAME="log"
          elif [ "$JOB_STATUS" == "cancelled" ]; then
             MESSAGE="CANCELLED. Job stopped by user. Logs: $LOG_URL"
             EVENT_NAME="log"
          else
             MESSAGE="FAILED. Something went wrong. Logs: $LOG_URL"
             EVENT_NAME="log"
             # Important: The frontend expects "Job failed" or similar to trigger fail state
             # We can send a specific log line that triggers it
          fi

          # Simple Pusher Trigger
          TS=$(date +%s)

          # Include new_slug in data
          DATA_JSON=$(jq -n \
                  --arg msg "$MESSAGE" \
                  --arg final_slug "$FINAL_SLUG" \
                  --arg status "$JOB_STATUS" \
                  '{message: $msg, new_slug: $final_slug, status: $status}')

          # Send to ORIGINAL slug channel so frontend hears it
          BODY=$(jq -n \
                  --arg name "$EVENT_NAME" \
                  --arg channel "$SLUG" \
                  --arg data "$DATA_JSON" \
                  '{name: $name, channels: [$channel], data: $data}')

          BODY_MD5=$(echo -n "$BODY" | md5sum | awk '{print $1}')
          SIGN_STRING="POST\n/apps/$PUSHER_APP_ID/events\nauth_key=$PUSHER_KEY&auth_timestamp=$TS&auth_version=1.0&body_md5=$BODY_MD5"
          AUTH_SIGNATURE=$(echo -n -e "$SIGN_STRING" | openssl dgst -sha256 -hmac "$PUSHER_SECRET" | sed 's/^.* //')

          curl -s -X POST "https://api-$PUSHER_CLUSTER.pusher.com/apps/$PUSHER_APP_ID/events?auth_key=$PUSHER_KEY&auth_timestamp=$TS&auth_version=1.0&body_md5=$BODY_MD5&auth_signature=$AUTH_SIGNATURE" \
                 -H "Content-Type: application/json" \
                 -d "$BODY"
