name: Deep Search (OpenHands)

on:
  repository_dispatch:
    types: [deep-search]
  workflow_dispatch:
    inputs:
      query:
        description: 'Search Query (e.g. "ita 2022")'
        required: true
      slug:
        description: 'Slug for folder name (e.g. "ita-2022")'
        required: true
      ntfy_topic:
        description: "ntfy.sh topic for log streaming"
        default: "false"
        # ATEN√á√ÉO: Cleanup for√ßado para FALSE para evitar dele√ß√£o acidental.
        # Mesmo se o client_payload enviar true, vamos ignorar nos jobs cr√≠ticos.

jobs:
  search:
    runs-on: ubuntu-latest
    timeout-minutes: 25
    permissions:
      contents: write

    # Global Environment Variables (Available to all steps)
    env:
      LLM_API_KEY: ${{ secrets.LLM_API_KEY }}
      TAVILY_API_KEY: ${{ secrets.TAVILY_API_KEY }}
      PUSHER_APP_ID: ${{ secrets.PUSHER_APP_ID }}
      PUSHER_KEY: ${{ secrets.PUSHER_KEY }}
      PUSHER_SECRET: ${{ secrets.PUSHER_SECRET }}
      PUSHER_CLUSTER: ${{ secrets.PUSHER_CLUSTER }}
      NTFY_TOPIC: ${{ github.event.client_payload.ntfy_topic || inputs.ntfy_topic }}
      QUERY: ${{ github.event.client_payload.query || inputs.query }}
      SLUG: ${{ github.event.client_payload.slug || inputs.slug }}

    steps:
      - name: Checkout Repo
        uses: actions/checkout@v4

      - name: Setup Logger
        run: |
          cat << 'EOF' > logger.py
          import sys
          import time
          import json
          import hashlib
          import hmac
          import os
          import threading
          import http.client
          import urllib.parse
          import subprocess
          import re
          from collections import deque

          # --- CONFIG ---
          PUSHER_APP_ID = os.environ.get("PUSHER_APP_ID")
          PUSHER_KEY = os.environ.get("PUSHER_KEY")
          PUSHER_SECRET = os.environ.get("PUSHER_SECRET")
          PUSHER_CLUSTER = os.environ.get("PUSHER_CLUSTER")
          CHANNEL = os.environ.get("SLUG")

          if not all([PUSHER_APP_ID, PUSHER_KEY, PUSHER_SECRET, PUSHER_CLUSTER, CHANNEL]):
              print("[Logger] Missing Pusher secrets. Logging to stdout only.")
              PUSHER_ENABLED = False
          else:
              PUSHER_ENABLED = True

          # Global State
          log_buffer = deque()
          buffer_lock = threading.Lock()
          running = True

          print("[Logger] Script initialized.", flush=True)

          # --- PUSHER HELPER ---
          def send_to_pusher(event_name, data_payload):
              if not PUSHER_ENABLED:
                  return

              timestamp = str(int(time.time()))
              body_data = json.dumps(data_payload)
              body = json.dumps({
                  "name": event_name,
                  "channels": [CHANNEL],
                  "data": body_data
              })
              
              body_md5 = hashlib.md5(body.encode('utf-8')).hexdigest()
              
              sign_string = f"POST\n/apps/{PUSHER_APP_ID}/events\nauth_key={PUSHER_KEY}&auth_timestamp={timestamp}&auth_version=1.0&body_md5={body_md5}"
              auth_signature = hmac.new(PUSHER_SECRET.encode('utf-8'), sign_string.encode('utf-8'), hashlib.sha256).hexdigest()
              
              params = urllib.parse.urlencode({
                  'auth_key': PUSHER_KEY,
                  'auth_timestamp': timestamp,
                  'auth_version': '1.0',
                  'body_md5': body_md5,
                  'auth_signature': auth_signature
              })
              
              try:
                  conn = http.client.HTTPSConnection(f"api-{PUSHER_CLUSTER}.pusher.com", timeout=3)
                  headers = {'Content-Type': 'application/json'}
                  conn.request("POST", f"/apps/{PUSHER_APP_ID}/events?{params}", body, headers)
                  resp = conn.getresponse()
                  resp.read() # Consume response
                  conn.close()
              except Exception as e:
                  print(f"[Logger] Pusher Error: {e}")

          # --- WORKER: LOG BATCHER ---
          def log_worker():
              while running or log_buffer:
                  with buffer_lock:
                      if not log_buffer:
                          batch = []
                      else:
                          batch = list(log_buffer)
                          log_buffer.clear()
                  
                  if batch:
                      # Join lines and broadcast
                      text_chunk = "".join(batch)
                      send_to_pusher("log", {"message": text_chunk})
                  
                  time.sleep(1.0) # Batch window

          # --- WORKER: TASK ACTIVE POLLER ---
          # This thread actively asks Docker for the TASKS.md file
          # It is much more robust than parsing log lines.
          def task_watcher_worker():
              print("[Logger] Task watcher thread started. Polling for TASKS.md...", flush=True)
              last_known_tasks_hash = ""
              container_name = "openhands-app"
              tasks_path = None # Will find dynamically
              
              # Wait slightly for container, but don't block visibility
              time.sleep(2)

              while running:
                  time.sleep(2.0) # Poll every 2s

                  try:
                      # 1. Locate TASKS.md if we haven't yet
                      if not tasks_path:
                          # "find /workspace -name TASKS.md | head -n 1"
                          try:
                              find_cmd = ["docker", "exec", container_name, "sh", "-c", "find /workspace -name TASKS.md 2>/dev/null | head -n 1"]
                              res = subprocess.run(find_cmd, capture_output=True, text=True)
                              if res.returncode == 0 and res.stdout.strip():
                                  tasks_path = res.stdout.strip()
                                  print(f"[Logger] Found active task file at: {tasks_path}", flush=True)
                          except:
                              pass # Container might not be up yet
                      
                      # 2. Read File if we found it
                      if tasks_path:
                          read_cmd = ["docker", "exec", container_name, "cat", tasks_path]
                          res = subprocess.run(read_cmd, capture_output=True, text=True)
                          
                          if res.returncode == 0:
                              content = res.stdout
                              # Debug: Show we got content
                              # print(f"[Logger] Read {len(content)} bytes from TASKS.md", flush=True)

                              # 3. Hash Check
                              current_hash = hashlib.md5(content.encode()).hexdigest()
                              if current_hash != last_known_tasks_hash:
                                  # Debug: Print RAW content (User Request)
                                  print(f"[DEBUG RAW TASKS.md]\n{content}\n[END RAW]", flush=True)

                                  # It changed! Parse it.
                                  parsed_tasks = parse_tasks_md(content)
                                  
                                  if parsed_tasks:
                                      print(f"[Logger] Detected task update ({len(parsed_tasks)} items). Broadcasting...", flush=True)
                                      
                                      # PRINT TASKS TO CONSOLE (User Request)
                                      print("--- CURRENT TASKS ---", flush=True)
                                      for t in parsed_tasks:
                                          status_icon = "[ ]"
                                          if t['status'] == "completed": status_icon = "[x]"
                                          elif t['status'] == "in_progress": status_icon = "[/]"
                                          
                                          # Format: [ ] Title | Notes: Description...
                                          note_str = f" | Notes: {t['notes']}" if t.get('notes') else ""
                                          print(f"{status_icon} {t['title']}{note_str}", flush=True)
                                      print("---------------------", flush=True)
                                      
                                      send_to_pusher("task_update", parsed_tasks)
                                      last_known_tasks_hash = current_hash
                                  else:
                                      # Debug: why no tasks?
                                      print(f"[Logger] Content changed but parsed 0 tasks. Content sample: {content[:100]!r}", flush=True)
                                      
                  except Exception as e:
                      # Don't crash the logger if docker exec fails randomly
                      print(f"[Logger] Task watcher error: {e}", flush=True)
                      pass

          # Helper: Parse TASKS.md content (Supports Markdown Checklists & OpenHands Emojis & Notes)
          def parse_tasks_md(content):
              tasks = []
              lines = content.split('\n')
              
              current_task = None
              
              for line in lines:
                  line = line.strip()
                  if not line: continue

                  # 1. Try Markdown Checklist: - [ ] Task Name
                  match_md = re.match(r'- \[([ xX/])\] (.*)', line)
                  
                  # 2. Try Numbered List with Emojis: 1. ‚è≥ Task Name
                  match_num = re.match(r'\d+\.\s+(?:([^\w\s]+)\s+)?(.*)', line)
                  
                  if match_md:
                      if current_task: tasks.append(current_task)
                      
                      status_char = match_md.group(1).lower()
                      title = re.sub(r'<!--.*-->', '', match_md.group(2)).strip()
                      
                      status = "todo"
                      if status_char == 'x': status = "completed"
                      elif status_char == '/': status = "in_progress"
                      
                      current_task = {"id": len(tasks), "title": title, "status": status, "notes": ""}
                  
                  elif match_num:
                      if current_task: tasks.append(current_task)
                      
                      icon = match_num.group(1) or ""
                      title = re.sub(r'<!--.*-->', '', match_num.group(2)).strip()
                      
                      status = "todo"
                      if icon:
                          if any(c in icon for c in ['‚úÖ', '‚úî', '‚òë']): status = "completed"
                          elif any(c in icon for c in ['‚è≥', '‚ñ∂', 'üèÉ', 'üöß']): status = "in_progress"
                          # ‚è≥ usually means "planned/waiting", so keeping it as 'todo' unless explicitly active?
                          # User requested: "indicar / como todo".
                          # Let's map ‚è≥ to "todo" if that's the intention, OR "in_progress".
                          # OpenHands standard: ‚è≥ = In Progress / Working on it.
                          # But user asked: "indicar / como todo".
                          # If I output "[ ]" for ‚è≥, frontend shows [ ] (Todo).
                          if any(c in icon for c in ['‚è≥']): status = "todo" # Explicit User Request override?
                          elif any(c in icon for c in ['‚ñ∂', 'üèÉ']): status = "in_progress"
                          elif any(c in icon for c in ['‚ùå', 'üö´']): status = "failed"
                      
                      current_task = {"id": len(tasks), "title": title, "status": status, "notes": ""}
                  
                  else:
                      # It's a note line if we have a current task
                      if current_task:
                           # Append to notes
                           if current_task["notes"]:
                               current_task["notes"] += " " + line
                           else:
                               current_task["notes"] = line
              
              if current_task: tasks.append(current_task)
              return tasks

          # --- MAIN THREADS ---
          t_log = threading.Thread(target=log_worker, daemon=True)
          t_log.start()

          t_task = threading.Thread(target=task_watcher_worker, daemon=True)
          t_task.start()

          # --- PRODUCER (STDIN) ---
          try:
              for line in sys.stdin:
                  # 1. Print to stdout (Critical for GitHub UI)
                  sys.stdout.write(line)
                  sys.stdout.flush()
                  
                  # CRIITCAL: Check for Agent Loop Error
                  if "AgentStuckInLoopError" in line:
                      print("\n[Logger] CRITICAL: Agent stuck in loop detected. Aborting workflow.", flush=True)
                      send_to_pusher("log", {"message": "CRITICAL: Agent stuck in loop. Aborting."})
                      sys.exit(1)
                  
                  # 2. Add to Pusher Buffer
                  with buffer_lock:
                      log_buffer.append(line)
          except KeyboardInterrupt:
              pass
          finally:
              # Shutdown
              running = False
              t_log.join(timeout=3)
              t_task.join(timeout=1)
          EOF

      - name: Free Disk Space
        run: |
          (
            echo "Disk space before cleanup:"
            df -h
            sudo rm -rf /usr/local/lib/android
            sudo rm -rf /usr/share/dotnet
            sudo rm -rf /opt/ghc
            sudo rm -rf /usr/local/.ghcup
            echo "Disk space after cleanup:"
            df -h
          ) 2>&1 | python3 -u logger.py

      - name: Create Output Directory
        run: |
          (
             mkdir -p output/${{ github.event.client_payload.slug || inputs.slug }}
             echo "Created output directory: output/${{ github.event.client_payload.slug || inputs.slug }}"
          ) 2>&1 | python3 -u logger.py

      # - name: Pre-execution Cleanup (HF) - DISABLED FOR NON-DESTRUCTIVE WORKFLOW
      #   if: ${{ github.event.client_payload.cleanup == 'NEVER_RUN' }} # Disabled
      #   env:
      #     HF_TOKEN: ${{ secrets.HF_TOKEN }}
      #     SLUG: ${{ github.event.client_payload.slug || inputs.slug }}
      #   run: |
      #     echo "Cleanup is disabled to preserve existing data."

      # - name: Pre-execution Cleanup (Pinecone) - DISABLED
      #   if: ${{ github.event.client_payload.cleanup == 'NEVER_RUN' }}
      #   env:
      #     WORKER_URL: https://maia-api-worker.willian-campos-ismart.workers.dev
      #     GH_PAT: ${{ secrets.GH_PAT }}
      #     SLUG: ${{ github.event.client_payload.slug || inputs.slug }}
      #   run: |
      #     (
      #        echo "Requesting Pinecone deletion for $SLUG..."
      #        curl -X POST "$WORKER_URL/delete-pinecone-record" \
      #             -H "Content-Type: application/json" \
      #             -H "Authorization: Bearer $GH_PAT" \
      #             -d "{\"slug\": \"$SLUG\"}" \
      #        || echo "Warning: Failed to call delete endpoint"
      #     ) 2>&1 | python3 -u logger.py

      - name: Announce Job URL
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          RUN_ID: ${{ github.run_id }}
          REPO: ${{ github.repository }}
        run: |
          (
            # Fetch the Jobs for this Run
            JOBS_JSON=$(curl -s -H "Authorization: Bearer $GH_TOKEN" \
                        "https://api.github.com/repos/$REPO/actions/runs/$RUN_ID/jobs")
            
            # Extract the HTML URL of the current job
            JOB_URL=$(echo "$JOBS_JSON" | jq -r '.jobs[0].html_url')
            
            if [ "$JOB_URL" == "null" ] || [ -z "$JOB_URL" ]; then
                # Fallback to Run URL if API fails
                JOB_URL="${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
            fi

            echo "[SYSTEM_INFO] JOB_URL=$JOB_URL"
          ) 2>&1 | python3 -u logger.py

      - name: Fetch Existing Manifest
        id: fetch_manifest
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          SLUG: ${{ github.event.client_payload.slug || inputs.slug }}
        run: |
          (
            echo "Checking for existing manifest in Hugging Face..."
            # Try to fetch manifest.json specifically using curl (faster than git clone)
            # URL format: https://huggingface.co/datasets/OWNER/REPO/resolve/main/PATH
            
            MANIFEST_URL="https://huggingface.co/datasets/toquereflexo/maia-deep-search/resolve/main/output/$SLUG/manifest.json"
            
            # Using a temporary file
            curl -s -L -o existing_manifest.json -H "Authorization: Bearer $HF_TOKEN" "$MANIFEST_URL" || true
            
            if [ -s existing_manifest.json ] && grep -q "{" existing_manifest.json; then
               echo "Found existing manifest."
               
               # Extract FULL JSON CONTENT using python for reliability (minify it)
               cat << 'EOF' > parse_existing.py
          import json
          try:
              with open('existing_manifest.json', 'r') as f:
                  data = json.load(f)
              # Normalize to list
              items = data if isinstance(data, list) else data.get('results', [])
              # Dump as compact JSON string
              print(json.dumps(items, separators=(',', ':')))
          except:
              print("[]")
          EOF
               EXISTING_MANIFEST=$(python3 parse_existing.py)
               rm parse_existing.py
               
               # Write to GITHUB_OUTPUT (SAFE way for multiline/JSON content)
               # Although separators=(',', ':') makes it one line, strictly speaking.
               echo "existing_manifest=$EXISTING_MANIFEST" >> $GITHUB_OUTPUT
               echo "Existing manifest content loaded."
            else
               echo "No existing manifest found or invalid."
               echo "existing_manifest=" >> $GITHUB_OUTPUT
            fi
            
            rm -f existing_manifest.json
          ) 2>&1 | python3 logger.py

      - name: Run OpenHands Deep Search
        id: deep_search
        run: |
          set -euo pipefail

          EXISTING_MANIFEST='${{ steps.fetch_manifest.outputs.existing_manifest }}'
          # 1) Heredoc sem interpreta√ß√£o (n√£o executa backticks)
          PROMPT_TEMPLATE=$(cat <<'EOF'
          Voc√™ recebeu uma consulta curta (QUERY) do tipo "nome da prova + ano", por exemplo: "ita 2022".

          Objetivo: encontrar e organizar TODOS os links e arquivos (provas e gabaritos) associados a essa QUERY.

          QUERY: "__QUERY__"

          __EXISTING_MANIFEST_INSTRUCTION__

          Regras para evitar pesquisa demais (muito importante):
          1) N√£o use Google como primeira op√ß√£o (CAPTCHA √© comum). Prefira:
            - Sites oficiais da banca/institui√ß√£o (dom√≠nio oficial).
            - P√°ginas ‚ÄúProvas anteriores / provas e gabaritos / exames anteriores / vestibular / arquivos‚Äù.
          2) Fa√ßa no m√°ximo 3 buscas no total (usando o mecanismo de busca configurado no OpenHands). Use buscas bem espec√≠ficas.
          3) Ap√≥s achar o site oficial, navegue dentro dele e extraia tudo com o m√≠nimo de novas buscas.
          4) S√≥ inclua links que pare√ßam realmente do exame/ano correspondente; quando estiver amb√≠guo, checar rapidamente pelo t√≠tulo/URL/nome do PDF.

          O que coletar:
          - Provas (todas as fases/dias/disciplinas/vers√µes que existirem).
          - Gabaritos correspondentes (e ‚Äúsolu√ß√µes‚Äù, se houver).
          - INDISPENS√ÅVEL: Classifique cada item no manifesto:
            - status: "downloaded" (se baixou com sucesso e validou) OU "reference" (se √© apenas um link externo que n√£o foi poss√≠vel baixar ou n√£o √© PDF).
          - Estrutura do JSON: {nome, tipo, ano, instituicao, fase, link_origem, status, filename (s√≥ se status=downloaded)}

          Prefer√™ncia de links:
          - Priorize links diretos de PDF.
          - Se for Google Drive, capture:
            - link "view"
            - e tamb√©m um link ‚Äúdownload direto‚Äù quando poss√≠vel (uc?export=download&id=...).

          Entrega (obrigat√≥rio):
          1) Crie a pasta: /workspace/output/__SLUG__/
          2) Baixe todos os PDFs que conseguir para: /workspace/output/__SLUG__/files/.
            CRITICO:
            - USE 'curl -I' PRIMEIRO para verificar o header.
            - SE O HEADER 'Content-Type' N√ÉO FOR 'application/pdf' (ex: 'text/html'), PARE IMEDIATAMENTE. N√ÉO BAIXE. MARQUE COMO "reference".
            - JAMAIS TENTE BAIXAR O MESMO LINK DUAS VEZES SE FALHAR NA PRIMEIRA.
            - SE VOC√ä SE PEGAR FAZENDO A MESMA A√á√ÉO 2 VEZES SEGUIDAS (ex: checar o mesmo link), DESISTA DESSE LINK E V√Å PARA O PR√ìXIMO.
            - SE O ARQUIVO BAIXADO TIVER MENOS DE 1KB, APAGUE-O IMEDIATAMENTE e mude o status para "reference".
            - SE O ARQUIVO CONTIVER HTML (DOCTYPE, <html>), APAGUE-O e mude o status para "reference".
            - N√ÉO CRIE ARQUIVOS FAKES OU COM CONTE√öDO GERADO. S√ì MANTENHA O QUE FOI BAIXADO DE VERDADE.
          3) Gere /workspace/output/__SLUG__/manifest.json (ESTE ARQUIVO √â CRITICO, DEVE SER JSON V√ÅLIDO. INCLUA ITENS "reference" E "downloaded")

          Ao final, responda apenas com caminhos e contagens.
          EOF
          )

          # Logic to inject existing manifest only if available
          if [ -n "$EXISTING_MANIFEST" ] && [ "$EXISTING_MANIFEST" != "[]" ]; then
             INSTRUCTION="‚ö†Ô∏è ATEN√á√ÉO: Os seguintes itens J√Å EXISTEM no banco de dados (Manifesto JSON Atual): $EXISTING_MANIFEST\n   - ANALISE este JSON para saber o que j√° temos.\n   - N√ÉO baixe novamente arquivos que j√° est√£o listados como 'downloaded'.\n   - EVITE duplicatas de arquivos que j√° possuem nomes similares.\n   - FOCAR em encontrar o que FALTA (ex: se tem a prova 1, procure o gabarito 1; se tem dia 1, procure dia 2)."
             PROMPT_TEMPLATE="${PROMPT_TEMPLATE/__EXISTING_MANIFEST_INSTRUCTION__/$INSTRUCTION}"
          else
             PROMPT_TEMPLATE="${PROMPT_TEMPLATE/__EXISTING_MANIFEST_INSTRUCTION__/}"
          fi

          # Replace Placeholders
          PROMPT="${PROMPT_TEMPLATE/__QUERY__/$QUERY}"
          PROMPT="${PROMPT//__SLUG__/$SLUG}"

          # 3) Setup Docker volumes and permissions
          VOL_NAME="openhands-vol-${GITHUB_RUN_ID}"
          STATE_VOL_NAME="openhands-state-${GITHUB_RUN_ID}"
          docker volume create "$VOL_NAME"
          docker volume create "$STATE_VOL_NAME"

          docker run --rm \
            -v "$VOL_NAME:/workspace" \
            -v "$STATE_VOL_NAME:/workspace/.openhands" \
            -e TAVILY_API_KEY="$TAVILY_API_KEY" \
            alpine sh -c '
              set -eux
              mkdir -p /workspace/.openhands
              printf "[core]\nsearch_api_key = \"%s\"\n" "$TAVILY_API_KEY" > /workspace/config.toml
              rm -f /workspace/.openhands/.jwt_secret || true
              chmod -R 777 /workspace
            '
          docker rm -f openhands-app 2>/dev/null || true

          # 4) Run Docker piped to Python
          docker_args=(
            run --pull=always
            -w /workspace
            -v /var/run/docker.sock:/var/run/docker.sock
            -v "$VOL_NAME:/workspace:rw"
            -v "$STATE_VOL_NAME:/workspace/.openhands"
            --add-host host.docker.internal:host-gateway
            -e "SANDBOX_USER_ID=$(id -u)"
            -e "SANDBOX_VOLUMES=$VOL_NAME:/workspace:rw"
            -e "SANDBOX_RUNTIME_CONTAINER_IMAGE=docker.openhands.dev/openhands/runtime:1.0-nikolaik"
            -e "LOG_ALL_EVENTS=true"
            -e "LLM_API_KEY=$LLM_API_KEY"
            -e "LLM_MODEL=gemini/gemini-3-flash-preview"
            -e "TAVILY_API_KEY=$TAVILY_API_KEY"
            -e "PYTHONUNBUFFERED=1"
            -e "PROMPT=$PROMPT"
            -e "FILE_STORE=local"
            -e "FILE_STORE_PATH=/workspace/.openhands"
            --name openhands-app
            docker.openhands.dev/openhands/openhands:1.0
            python -m openhands.core.main -t "$PROMPT"
          )

          # Pipe to python script. Secrets are now in the step env.
          docker "${docker_args[@]}" 2>&1 | python3 logger.py

          EXIT_CODE=${PIPESTATUS[0]}
          echo "Docker run finished with exit code $EXIT_CODE"

          echo "Copying artifacts from container to host..."
          docker cp openhands-app:/workspace/output/${SLUG}/. "output/${SLUG}/" || echo "Failed to copy artifacts or directory empty"

          # --- NO DYNAMIC RENAMING (Handled by Worker) ---
          FINAL_SLUG="$SLUG"
          echo "final_slug=$FINAL_SLUG" >> $GITHUB_OUTPUT
          # ------------------------------

          docker rm -f openhands-app || true
          docker volume rm "$VOL_NAME" || true
          docker volume rm "$STATE_VOL_NAME" || true

          exit $EXIT_CODE

      - name: Compute Visual Hashes
        env:
          SLUG: ${{ steps.deep_search.outputs.final_slug }}
        uses: ./.github/actions/compute-hash
        with:
          path: "output/${{ env.SLUG }}"

      - name: Integrity Verification & Cleanup
        env:
          SLUG: ${{ steps.deep_search.outputs.final_slug }}
        run: |
          # Create script
          cat << 'EOF' > verify_integrity.py
          import os
          import json
          import shutil
          import hashlib
          import time
          from pathlib import Path
          from pdf2image import convert_from_path
          import http.client
          from urllib.parse import urlparse

          slug = os.environ.get("SLUG")
          if not slug:
              print("[Verify] SLUG missing.")
              exit(1)

          base_dir = Path(f"output/{slug}")
          files_dir = base_dir / "files"
          manifest_path = base_dir / "manifest.json"
          existing_manifest = os.environ.get("EXISTING_MANIFEST_JSON", "[]") # We need to pass this or read it

          # Helper to load JSON safely
          def load_json(path):
              if os.path.exists(path):
                  try:
                      with open(path, 'r', encoding='utf-8') as f:
                          data = json.load(f)
                          # Normalize to list
                          if isinstance(data, dict): return data.get('results', []) or data.get('files', [])
                          return data if isinstance(data, list) else []
                  except: return []
              return []

          current_items = load_json(manifest_path)

          # Load Existing Manifest (from previous step or repo)
          # Ideally we should have it available. For now, let's assume we rely on 
          # checks within the current batch + what was preserved. 
          # NOTE: The 'Fetch Existing Manifest' step saved it to GITHUB_OUTPUT, 
          # but passing complex JSON env var can be tricky.
          # Alternative: We read 'hf_repo/output/slug/manifest.json' if it exists (since we cloned it later? No, clone is next).
          # Actually, 'Fetch Existing Manifest' step ran before. 
          # Let's try to trust the 'visual_hash' collision within the batch and relies on 'merge_manifests.py' later?
          # NO, the user explicitly asked to "DESCARTAR O NOVO" if hash exists.
          # To do that, we need to know the EXISTING hashes.
          # Let's use the 'existing_manifest.json' if it was kept? 
          # 'Fetch Existing Manifest' deletes it.
          # Let's try to read 'existing_manifest' from the environment VARIABLE passed from the step output.

          existing_items = []
          try:
              ex_str = os.environ.get("EXISTING_MANIFEST_CONTENT", "[]")
              existing_items = json.loads(ex_str) if ex_str else []
          except:
              pass
              
          existing_hashes = set()
          for i in existing_items:
              if i.get('visual_hash'): existing_hashes.add(i.get('visual_hash'))

          print(f"[Verify] Loaded {len(current_items)} current items and {len(existing_hashes)} existing hashes.")

          valid_items = []
          corrupted_count = 0
          deduped_count = 0

          # --- HELPER: VERIFY REFERENCE LINK ---
          def check_link(url):
              if not url: return False
              try:
                  parsed = urlparse(url)
                  conn = http.client.HTTPSConnection(parsed.netloc, timeout=5) if parsed.scheme == 'https' else http.client.HTTPConnection(parsed.netloc, timeout=5)
                  conn.request("HEAD", parsed.path)
                  res = conn.getresponse()
                  conn.close()
                  return res.status < 400 # Accept redirects and success
              except:
                  return False

          for item in current_items:
              is_valid = False
              reason = ""
              
              # 1. REFERENCE CHECK
              if item.get('status') == 'reference':
                  url = item.get('link_origem') or item.get('url')
                  if check_link(url):
                      is_valid = True
                  else:
                      reason = "Dead Link"
              
              # 2. FILE VERIFICATION (Downloaded)
              else:
                  fname = item.get('filename')
                  if fname:
                      fpath = files_dir / fname
                      if fpath.exists():
                          # Layer 1: Size
                          size = fpath.stat().st_size
                          if size < 500:
                              reason = f"Small File ({size}b)"
                          else:
                              # Layer 2: Signature
                              try:
                                  with open(fpath, 'rb') as f_bin:
                                      sig = f_bin.read(10)
                                      if b'%PDF-' not in sig:
                                          reason = "Invalid PDF Signature"
                                      else:
                                          # Layer 3: Renderability
                                          try:
                                              convert_from_path(str(fpath), first_page=1, last_page=1)
                                              
                                              # Layer 4: Deduplication
                                              vhash = item.get('visual_hash')
                                              if vhash and vhash in existing_hashes:
                                                  reason = "Duplicate (Hash Collision)"
                                                  deduped_count += 1
                                              else:
                                                  is_valid = True
                                                  if vhash: existing_hashes.add(vhash) # Add to set to catch intra-batch dupes
                                          except Exception as e:
                                              reason = f"Render Fail: {str(e)}"
                              except Exception as e:
                                  reason = f"Read Error: {str(e)}"
                      else:
                          reason = "File Not Found"
                  else:
                      reason = "No Filename"

              if is_valid:
                  # Keep item, ensure status verified
                  item['status'] = 'verified' if item.get('status') != 'reference' else 'reference'
                  valid_items.append(item)
              else:
                  print(f"[Verify] Discarding {item.get('name')}: {reason}")
                  corrupted_count += 1

          # Save Valid Manifest
          with open(manifest_path, 'w', encoding='utf-8') as f:
              json.dump(valid_items, f, indent=2, ensure_ascii=False)

          print(f"[Verify] Done. Valid: {len(valid_items)}, Removed: {corrupted_count}, Deduped: {deduped_count}")

          # CLEANUP: Remove files folder COMPLETELY
          if files_dir.exists():
              print("[Verify] Deleting files directory...")
              shutil.rmtree(files_dir)
          EOF

          (
            echo "Installing verification dependencies..."
            sudo apt-get update && sudo apt-get install -y poppler-utils
            pip install pdf2image

            echo "Running Verification & Cleanup..."
            # Pass existing manifest content from step output
            export EXISTING_MANIFEST_CONTENT='${{ steps.fetch_manifest.outputs.existing_manifest }}'
            python3 verify_integrity.py
          ) 2>&1 | python3 logger.py

      - name: Upload Search Results
        uses: actions/upload-artifact@v4
        with:
          name: deep-search-artifact
          path: output/${{ steps.deep_search.outputs.final_slug }}

      - name: Push to Hugging Face Dataset
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          (
            echo "Starting upload to Hugging Face..."
            git config --global user.email "bot@maia.api"
            git config --global user.name "Maia Bot"
            
            # Optimization for Large Pushes (Fixes 503 RPC failed)
            git config --global http.postBuffer 524288000
            git config --global http.maxRequestBuffer 104857600
            git config --global lfs.activitytimeout 300
            git config --global lfs.dialtimeout 300
            git config --global http.version HTTP/1.1

            # Clone HF Repo
            git clone --depth 1 https://user:$HF_TOKEN@huggingface.co/datasets/toquereflexo/maia-deep-search hf_repo
            
            slug="${{ steps.deep_search.outputs.final_slug }}"
            echo "Processing slug: $slug"
            
            # Ensure target directory exists
            mkdir -p hf_repo/output/$slug

            # --- MANIFEST MERGE LOGIC START ---
            # Create merge script to avoid indentation issues
          cat << 'EOF' > merge_manifests.py
          import json
          import os
          import sys

          slug = os.environ.get('SLUG')
          local_manifest_path = f'output/{slug}/manifest.json'
          remote_manifest_path = f'hf_repo/output/{slug}/manifest.json'

          if os.path.exists(local_manifest_path) and os.path.exists(remote_manifest_path):
              print(f'[Merge] Found both local and remote manifests for {slug}. Merging...')
              try:
                  with open(local_manifest_path, 'r', encoding='utf-8') as f:
                      local_data = json.load(f)
                  with open(remote_manifest_path, 'r', encoding='utf-8') as f:
                      remote_data = json.load(f)

                  # Normalize to lists
                  local_items = local_data if isinstance(local_data, list) else local_data.get('results', [])
                  remote_items = remote_data if isinstance(remote_data, list) else remote_data.get('results', [])

                  merged_items = remote_items[:] # Start with old
                  
                  # Create a map of existing filenames to avoid naive duplicates
                  for item in local_items:
                      fname = item.get('filename') or item.get('path', '').split('/')[-1]
                      
                      # Remove existing entry with same filename then append new
                      merged_items = [i for i in merged_items if (i.get('filename') or i.get('path', '').split('/')[-1]) != fname]
                      merged_items.append(item)

                  # Save back to LOCAL manifest (which will be cp'd over)
                  with open(local_manifest_path, 'w', encoding='utf-8') as f:
                      json.dump(merged_items, f, indent=2, ensure_ascii=False)
                  
                  print(f'[Merge] Success. Total items: {len(merged_items)}')
              except Exception as e:
                  print(f'[Merge] Error: {e}')
          else:
               print('[Merge] Manifests not found in both locations. Skipping merge.')
          EOF
            
            # Run the merge script
            export SLUG="$slug"
            python3 merge_manifests.py
            rm merge_manifests.py
            # --- MANIFEST MERGE LOGIC END ---

            # Simple Copy: Overwrite/Merge local output into repo output
            # This follows the user instruction: "basicamente s√≥ colocava tudo que a ia gerou em output/slug/"
            cp -R output/$slug/* hf_repo/output/$slug/

            cd hf_repo
            
            # LFS Setup
            git lfs install
            git lfs track "*.pdf" "*.zip" "*.rar" "*.doc" "*.docx" "*.jpg" "*.jpeg"
            git add .gitattributes

            git add .
            # Warning: 'files' dir might be gone, so 'git add .' is safe but we should ensure we don't fail if it's missing in previous commits (git tracks removals automatically with add .)
            
            git commit -m "Add search results for $slug" || echo "No changes to commit"
            
            # Retry Loop for Push (Fixes transient network failures)
            n=0
            until [ "$n" -ge 5 ]
            do
               git push && break
               n=$((n+1)) 
               echo "Push failed. Retrying in 10s... (Attempt $n/5)"
               sleep 10
            done
            if [ "$n" -ge 5 ]; then
               echo "Failed to push after 5 attempts."
               exit 1
            fi
            echo "Upload to Hugging Face completed."
          ) 2>&1 | python3 logger.py

      - name: Update Semantic Cache
        if: success()
        env:
          WORKER_URL: https://maia-api-worker.willian-campos-ismart.workers.dev
          GH_PAT: ${{ secrets.GH_PAT }}
          QUERY: ${{ github.event.client_payload.query || inputs.query }}
          SLUG: ${{ steps.deep_search.outputs.final_slug }}
        run: |
          (
            echo "Updating Semantic Cache at $WORKER_URL..."

            MANIFEST_PATH="output/$SLUG/manifest.json"

            # Generate Clean Query from Slug (replace dashes with spaces)
            CLEAN_QUERY="${SLUG//-/ }"
            echo "Using Clean Query for Embedding: $CLEAN_QUERY"

            if [ -f "$MANIFEST_PATH" ]; then
              
              # Using jq to create a safe JSON payload
              PAYLOAD=$(jq -n \
                      --arg query "$CLEAN_QUERY" \
                      --arg original_query "$QUERY" \
                      --arg slug "$SLUG" \
                      --slurpfile manifest "$MANIFEST_PATH" \
                      '{
                        query: $query, 
                        slug: $slug, 
                        metadata: {
                          source: "deep-search",
                          original_query: $original_query,
                          file_count: ($manifest[0] | length),
                          institution: ($manifest[0][0].instituicao // $manifest[0][0].institution // "unknown"),
                          year: ($manifest[0][0].ano // $manifest[0][0].year // "unknown")
                        }
                      }')
              
              
              # Curl Request with -f (fail on error) and capturing output
              RESPONSE=$(curl -s -w "\nHTTP_CODE:%{http_code}" -X POST "$WORKER_URL/update-deep-search-cache" \
                   -H "Content-Type: application/json" \
                   -H "Authorization: Bearer $GH_PAT" \
                   -d "$PAYLOAD")
              
              HTTP_CODE=$(echo "$RESPONSE" | tail -n1 | cut -d: -f2)
              BODY=$(echo "$RESPONSE" | sed '$d')

              echo "Worker Response Body: $BODY"
              echo "Worker HTTP Code: $HTTP_CODE"

              if [ "$HTTP_CODE" -ne 200 ]; then
                echo "::error::Failed to update cache. HTTP $HTTP_CODE"
                exit 1
              fi
            else
              echo "Manifest not found at $MANIFEST_PATH. Skipping cache update."
            fi
          ) 2>&1 | python3 logger.py

      - name: Notify Completion
        if: ${{ always() }}
        env:
          PUSHER_APP_ID: ${{ secrets.PUSHER_APP_ID }}
          PUSHER_KEY: ${{ secrets.PUSHER_KEY }}
          PUSHER_SECRET: ${{ secrets.PUSHER_SECRET }}
          PUSHER_CLUSTER: ${{ secrets.PUSHER_CLUSTER }}
          SLUG: ${{ github.event.client_payload.slug || inputs.slug }}
          FINAL_SLUG: ${{ steps.deep_search.outputs.final_slug }}
          JOB_STATUS: ${{ job.status }}
        run: |
          LOG_URL="${GITHUB_SERVER_URL}/${GITHUB_REPOSITORY}/actions/runs/${GITHUB_RUN_ID}"

          if [ "$JOB_STATUS" == "success" ]; then
             MESSAGE="COMPLETED. Logs: $LOG_URL"
             EVENT_NAME="log"
          elif [ "$JOB_STATUS" == "cancelled" ]; then
             MESSAGE="CANCELLED. Job stopped by user. Logs: $LOG_URL"
             EVENT_NAME="log"
          else
             MESSAGE="FAILED. Something went wrong. Logs: $LOG_URL"
             EVENT_NAME="log"
             # Important: The frontend expects "Job failed" or similar to trigger fail state
             # We can send a specific log line that triggers it
          fi

          # Simple Pusher Trigger
          TS=$(date +%s)

          # Include new_slug in data
          DATA_JSON=$(jq -n \
                  --arg msg "$MESSAGE" \
                  --arg final_slug "$FINAL_SLUG" \
                  --arg status "$JOB_STATUS" \
                  '{message: $msg, new_slug: $final_slug, status: $status}')

          # Send to ORIGINAL slug channel so frontend hears it
          BODY=$(jq -n \
                  --arg name "$EVENT_NAME" \
                  --arg channel "$SLUG" \
                  --arg data "$DATA_JSON" \
                  '{name: $name, channels: [$channel], data: $data}')

          BODY_MD5=$(echo -n "$BODY" | md5sum | awk '{print $1}')
          SIGN_STRING="POST\n/apps/$PUSHER_APP_ID/events\nauth_key=$PUSHER_KEY&auth_timestamp=$TS&auth_version=1.0&body_md5=$BODY_MD5"
          AUTH_SIGNATURE=$(echo -n -e "$SIGN_STRING" | openssl dgst -sha256 -hmac "$PUSHER_SECRET" | sed 's/^.* //')

          curl -s -X POST "https://api-$PUSHER_CLUSTER.pusher.com/apps/$PUSHER_APP_ID/events?auth_key=$PUSHER_KEY&auth_timestamp=$TS&auth_version=1.0&body_md5=$BODY_MD5&auth_signature=$AUTH_SIGNATURE" \
                 -H "Content-Type: application/json" \
                 -d "$BODY"
